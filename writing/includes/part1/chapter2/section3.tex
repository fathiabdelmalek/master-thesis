\section{Methods of \ac{slr}}
\paragraph{}
\ac{slr} involves the development and application of various methods and techniques to accurately interpret and understand \ac{sl} gestures. Over the years, several approaches have been explored in \ac{slr} research, employing different algorithms and models to recognize and classify the gestures. This section provides an overview of some commonly employed methods in \ac{slr}.

\subsection{\ac{hmm}}
\paragraph{}
\ac{hmm} have been extensively used in \ac{slr}, especially for temporal analysis of the gestures. They are probabilistic models that represent gestures as sequences of hidden states and observable outputs. They capture the temporal dynamics and dependencies between consecutive frames or observations in videos. \ac{hmm}-based \ac{slr} systems use training data to estimate the model parameters and then apply the Viterbi algorithm or other decoding techniques to recognize and classify the gestures.

\subsection{\ac{nn}s}
\paragraph{}
\ac{nn}s have gained significant attention due to their ability to learn complex patterns and relationships in data. Different types of \ac{nn}s, such as \ac{mlp}, \ac{rnn}, and \ac{cnn}, have been employed. \ac{nn}-based \ac{slr} systems often require large amounts of annotated training data to effectively learn the mapping between input gestures and their corresponding classes. With appropriate training, \ac{nn}s can achieve high recognition accuracy and handle variations in gestures.

\subsection{\ac{svm} }
\paragraph{}
\ac{svm}s are widely used in \ac{slr} due to their ability to handle high-dimensional data and their effectiveness in classification tasks. They aim to find an optimal hyperplane that separates different gestures in a high-dimensional feature space. By mapping input gestures to this feature space, they can accurately classify new gestures based on their position relative to the hyperplane. They have been successfully applied to both image-based and video-based \ac{slr} tasks, achieving good recognition accuracy.

\subsection{Fuzzy Sets}
\paragraph{}
Fuzzy sets theory has been employed to handle the inherent ambiguity and uncertainty present in \ac{slr}. They allow for gradual membership of gestures in different classes, providing a more flexible and robust representation. Fuzzy logic-based systems often involve defining membership functions and fuzzy rules to capture the variability and imprecision of the gestures. These systems can handle variations in hand shape, movement, and position, improving the recognition accuracy.

\paragraph{}
Other methods and techniques, such as decision trees, rule-based systems, and \ac{dtw}, \ac{tcn}, have also been explored in this context. Each method has its own strengths and limitations, and their suitability depends on the specific requirements of the task and available data.
\paragraph{}
It is worth noting that the choice of \ac{slr} method often depends on the characteristics of the dataset, the complexity of the gestures, and the available computational resources. Researchers continue to explore and develop novel methods and hybrid approaches to improve the accuracy and efficiency of this systems.
\paragraph{}
The table \ref{tab:previous-works} presents a glimpse into the latest works conducted in the field, showcasing different approaches employed and their corresponding results. These studies highlight the diverse range of methodologies utilized. The results demonstrate the progress made in the field, with notable accuracies, attained on various datasets. It is important to note that the studies mentioned here are representative examples, and there are numerous other significant contributions in the field.
\begin{table}[h]
	\centering
	\caption{Previous Works in \ac{slr} and Results}
	\label{tab:previous-works}
	\begin{tabular}{|p{0.24\textwidth}|p{0.2\textwidth}|p{0.3\textwidth}|p{0.24\textwidth}|M{0.2\textwidth}|}
		\hline
		\textbf{Study} & \textbf{Sign lang} & \textbf{Approach} & \textbf{Results} \\
		\hline
		Mapari et al\cite{mapari2016american} & \ac{asl} & \ac{mlp} & 90\% accuracy \\
		\hline
		Khelil et al. (2016) \cite{khelil2016hand} & ArSL & \ac{svm} & 91.3\% accuracy \\
		\hline
		Chong et al. (2018) \cite{chong2018american} & Custom dataset & \ac{dnn} & 88.79\% accuracy \\
		\hline
		Pu, J., et al. (2017) \cite{fang2017deepasl} & Custom dataset & HB-RNN & more than 92\% accuracy	\\
		\hline
		Guo Dan, et al. (2019) \cite{guo2019dense} & Various &  \ac{tcn} & 44.7\% WER \\
		\hline
		Yu et al (2019) \cite{yu2019exploration} & CSL & \ac{cnn} and \ac{lstm} & 95.1\% and 88.2\% accuracy (user-dependent) \\
		\hline
		Renz et al. (2021) \cite{renz2021sign} & Various & \ac{tcn} & - \\
		\hline
		Faisal et al (2021) \cite{faisal2021sensor} & \ac{asl} & \ac{knn} & 99.53\% accuracy for static gestures and 98.64\% for dynamic \\
		\hline
		Wen et al (2021) \cite{wen2021ai} & \ac{asl} &  \ac{cnn} & 88.67\% accuracy \\
		\hline
		Faisal et al (2022) \cite{ref-article} & \ac{asl} & \ac{cnn} & 97.35\% accuracy for static and 84.42\% for dynamic \\
		\hline
	\end{tabular}
\end{table}
