\begin{abstract}
	\paragraph{}
	\ac{slr} plays a crucial role in facilitating communication between individuals with hearing impairments and non-signers. However, the lack of an \ac{asp} dataset and limited time constraints pose challenges in developing a customized dataset. In this study, an existing \ac{asl} dataset is utilized, and a \ac{lstm} model is employed for sign classification. The \ac{lstm} model achieves an impressive accuracy of 95.03\% for character-level classification and 94.63\% for word-level classification. Remarkably, the model exhibits minimal errors in both character and word classifications, primarily due to the selection of the most frequently predicted sign from a pool of 150 predictions per gesture. While these results demonstrate the effectiveness of the model.
	
	\vspace{0.5cm}
	
	\providecommand{\keywords}[1]
	{
		\small	
		\textbf{\textit{Keywords---}} #1
	}
	
	\keywords{ Sign Language Recognition, Deep Learning, LSTM}
\end{abstract}
