{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3027,"status":"ok","timestamp":1686605218783,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"QxvpAQB0Aeqi","outputId":"e1b26b62-4851-4d2f-eb55-9e3f6fe46ec2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4210,"status":"ok","timestamp":1686605222990,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"x1kEfMoNAOeu"},"outputs":[],"source":["from scipy.stats import mode\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","from tensorflow import keras\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","from keras.utils import to_categorical\n","from keras.optimizers import Adam\n","from keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4313,"status":"ok","timestamp":1686605227299,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"qfVi9KSqf9yG","outputId":"4d79b90d-922a-498c-8727-2954bf8f4054"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'bad': 0, 'deaf': 1, 'fine': 2, 'good': 3, 'goodbye': 4, 'hello': 5, 'hungry': 6, 'me': 7, 'no': 8, 'please': 9, 'sorry': 10, 'thankyou': 11, 'yes': 12, 'you': 13}\n","(525000, 11) (525000, 14)\n","(3500, 150, 11) (3500, 150, 14)\n"]}],"source":["df = pd.read_csv('/content/drive/MyDrive/University/MemoireAbdelmalek/data/all/words.csv')\n","X = df[['flex_1', 'flex_2', 'flex_3', 'flex_4', 'flex_5', 'GYRx', 'GYRy', 'GYRz', 'ACCx', 'ACCy', 'ACCz']].values\n","labels = df.iloc[:, -1]\n","label_dict = {label: i for i, label in enumerate(sorted(set(labels)))}\n","y = np.array([label_dict[label] for label in labels])\n","y = to_categorical(y, num_classes=len(label_dict))\n","\n","print(label_dict)\n","\n","print(X.shape, y.shape)\n","X = X.reshape((-1, 150, X.shape[1]))\n","y = y.reshape((-1, 150, y.shape[1]))\n","print(X.shape, y.shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1686605227300,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"K2UQcb2nP3c-","outputId":"4514fe32-1361-4460-f24c-aee1e36ef040"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2800, 150, 11) (700, 150, 11)\n"]}],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n","print(X_train.shape, X_test.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1686605227301,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"LlHDdYGz1lIf"},"outputs":[],"source":["checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","    filepath='best_model.h5',\n","    monitor='val_accuracy',\n","    save_best_only=True,\n","    save_weights_only=False,\n","    mode='max',\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2859,"status":"ok","timestamp":1686605230142,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"JFdL2GVKmHUD"},"outputs":[],"source":["input_layer = Input(shape=(150, 11), name='input_layer')\n","lstm_layer = LSTM(units=38, return_sequences=True, name='lstm_layer')(input_layer)\n","output_layer = Dense(units=len(label_dict), activation='softmax', name='output_layer')(lstm_layer)\n","model = Model(inputs=input_layer, outputs=output_layer)\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss=CategoricalCrossentropy(), metrics=['accuracy'])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":210458,"status":"ok","timestamp":1686605440589,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"YmuTowI-pNBe","outputId":"a65b2178-c090-42b2-ab86-a2d242b0f064"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fold 1\n","Epoch 1/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 2.6240 - accuracy: 0.1076\n","Epoch 1: val_accuracy improved from -inf to 0.10001, saving model to best_model.h5\n","70/70 [==============================] - 8s 53ms/step - loss: 2.6209 - accuracy: 0.1093 - val_loss: 2.5848 - val_accuracy: 0.1000\n","Epoch 2/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 2.5342 - accuracy: 0.1439\n","Epoch 2: val_accuracy improved from 0.10001 to 0.14308, saving model to best_model.h5\n","70/70 [==============================] - 2s 22ms/step - loss: 2.5332 - accuracy: 0.1446 - val_loss: 2.5054 - val_accuracy: 0.1431\n","Epoch 3/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 2.4545 - accuracy: 0.1828\n","Epoch 3: val_accuracy improved from 0.14308 to 0.19292, saving model to best_model.h5\n","70/70 [==============================] - 1s 16ms/step - loss: 2.4537 - accuracy: 0.1832 - val_loss: 2.4287 - val_accuracy: 0.1929\n","Epoch 4/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 2.3748 - accuracy: 0.2251\n","Epoch 4: val_accuracy improved from 0.19292 to 0.22806, saving model to best_model.h5\n","70/70 [==============================] - 1s 21ms/step - loss: 2.3756 - accuracy: 0.2246 - val_loss: 2.3547 - val_accuracy: 0.2281\n","Epoch 5/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 2.3063 - accuracy: 0.2585\n","Epoch 5: val_accuracy improved from 0.22806 to 0.25468, saving model to best_model.h5\n","70/70 [==============================] - 1s 21ms/step - loss: 2.3015 - accuracy: 0.2615 - val_loss: 2.2939 - val_accuracy: 0.2547\n","Epoch 6/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 2.2419 - accuracy: 0.2916\n","Epoch 6: val_accuracy improved from 0.25468 to 0.27901, saving model to best_model.h5\n","70/70 [==============================] - 2s 22ms/step - loss: 2.2421 - accuracy: 0.2922 - val_loss: 2.2415 - val_accuracy: 0.2790\n","Epoch 7/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 2.1915 - accuracy: 0.3187\n","Epoch 7: val_accuracy improved from 0.27901 to 0.30479, saving model to best_model.h5\n","70/70 [==============================] - 1s 18ms/step - loss: 2.1905 - accuracy: 0.3194 - val_loss: 2.1938 - val_accuracy: 0.3048\n","Epoch 8/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 2.1442 - accuracy: 0.3412\n","Epoch 8: val_accuracy improved from 0.30479 to 0.32767, saving model to best_model.h5\n","70/70 [==============================] - 1s 15ms/step - loss: 2.1425 - accuracy: 0.3423 - val_loss: 2.1490 - val_accuracy: 0.3277\n","Epoch 9/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 2.0975 - accuracy: 0.3607\n","Epoch 9: val_accuracy improved from 0.32767 to 0.34506, saving model to best_model.h5\n","70/70 [==============================] - 1s 14ms/step - loss: 2.0960 - accuracy: 0.3623 - val_loss: 2.1069 - val_accuracy: 0.3451\n","Epoch 10/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 2.0527 - accuracy: 0.3795\n","Epoch 10: val_accuracy improved from 0.34506 to 0.36329, saving model to best_model.h5\n","70/70 [==============================] - 1s 15ms/step - loss: 2.0512 - accuracy: 0.3790 - val_loss: 2.0643 - val_accuracy: 0.3633\n","Epoch 11/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 2.0070 - accuracy: 0.3968\n","Epoch 11: val_accuracy improved from 0.36329 to 0.38029, saving model to best_model.h5\n","70/70 [==============================] - 1s 21ms/step - loss: 2.0078 - accuracy: 0.3964 - val_loss: 2.0251 - val_accuracy: 0.3803\n","Epoch 12/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 1.9670 - accuracy: 0.4138\n","Epoch 12: val_accuracy improved from 0.38029 to 0.39864, saving model to best_model.h5\n","70/70 [==============================] - 2s 24ms/step - loss: 1.9669 - accuracy: 0.4136 - val_loss: 1.9874 - val_accuracy: 0.3986\n","Epoch 13/50\n","70/70 [==============================] - ETA: 0s - loss: 1.9244 - accuracy: 0.4363\n","Epoch 13: val_accuracy improved from 0.39864 to 0.41546, saving model to best_model.h5\n","70/70 [==============================] - 2s 25ms/step - loss: 1.9244 - accuracy: 0.4363 - val_loss: 1.9495 - val_accuracy: 0.4155\n","Epoch 14/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 1.8843 - accuracy: 0.4540\n","Epoch 14: val_accuracy improved from 0.41546 to 0.43007, saving model to best_model.h5\n","70/70 [==============================] - 1s 14ms/step - loss: 1.8849 - accuracy: 0.4531 - val_loss: 1.9121 - val_accuracy: 0.4301\n","Epoch 15/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 1.8450 - accuracy: 0.4726\n","Epoch 15: val_accuracy improved from 0.43007 to 0.44423, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 1.8461 - accuracy: 0.4719 - val_loss: 1.8752 - val_accuracy: 0.4442\n","Epoch 16/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 1.8112 - accuracy: 0.4860\n","Epoch 16: val_accuracy improved from 0.44423 to 0.45343, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 1.8080 - accuracy: 0.4885 - val_loss: 1.8384 - val_accuracy: 0.4534\n","Epoch 17/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 1.7699 - accuracy: 0.5007\n","Epoch 17: val_accuracy improved from 0.45343 to 0.46483, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 1.7704 - accuracy: 0.5003 - val_loss: 1.8012 - val_accuracy: 0.4648\n","Epoch 18/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 1.7366 - accuracy: 0.5134\n","Epoch 18: val_accuracy improved from 0.46483 to 0.47414, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 1.7341 - accuracy: 0.5143 - val_loss: 1.7663 - val_accuracy: 0.4741\n","Epoch 19/50\n","70/70 [==============================] - ETA: 0s - loss: 1.7005 - accuracy: 0.5313\n","Epoch 19: val_accuracy improved from 0.47414 to 0.49193, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.7005 - accuracy: 0.5313 - val_loss: 1.7314 - val_accuracy: 0.4919\n","Epoch 20/50\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 1.6676 - accuracy: 0.5454\n","Epoch 20: val_accuracy improved from 0.49193 to 0.50692, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.6682 - accuracy: 0.5448 - val_loss: 1.7005 - val_accuracy: 0.5069\n","Epoch 21/50\n","70/70 [==============================] - ETA: 0s - loss: 1.6370 - accuracy: 0.5561\n","Epoch 21: val_accuracy improved from 0.50692 to 0.51514, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 1.6370 - accuracy: 0.5561 - val_loss: 1.6706 - val_accuracy: 0.5151\n","Epoch 22/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 1.6072 - accuracy: 0.5656\n","Epoch 22: val_accuracy improved from 0.51514 to 0.52121, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.6073 - accuracy: 0.5651 - val_loss: 1.6416 - val_accuracy: 0.5212\n","Epoch 23/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 1.5776 - accuracy: 0.5709\n","Epoch 23: val_accuracy improved from 0.52121 to 0.53146, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 1.5786 - accuracy: 0.5698 - val_loss: 1.6120 - val_accuracy: 0.5315\n","Epoch 24/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 1.5457 - accuracy: 0.5811\n","Epoch 24: val_accuracy improved from 0.53146 to 0.54140, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 1.5494 - accuracy: 0.5786 - val_loss: 1.5823 - val_accuracy: 0.5414\n","Epoch 25/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 1.5200 - accuracy: 0.5881\n","Epoch 25: val_accuracy improved from 0.54140 to 0.54930, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.5202 - accuracy: 0.5879 - val_loss: 1.5521 - val_accuracy: 0.5493\n","Epoch 26/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 1.4926 - accuracy: 0.5964\n","Epoch 26: val_accuracy improved from 0.54930 to 0.55969, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 1.4907 - accuracy: 0.5971 - val_loss: 1.5231 - val_accuracy: 0.5597\n","Epoch 27/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 1.4613 - accuracy: 0.6036\n","Epoch 27: val_accuracy improved from 0.55969 to 0.56485, saving model to best_model.h5\n","70/70 [==============================] - 1s 11ms/step - loss: 1.4616 - accuracy: 0.6040 - val_loss: 1.4954 - val_accuracy: 0.5648\n","Epoch 28/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 1.4328 - accuracy: 0.6120\n","Epoch 28: val_accuracy improved from 0.56485 to 0.57643, saving model to best_model.h5\n","70/70 [==============================] - 1s 16ms/step - loss: 1.4342 - accuracy: 0.6099 - val_loss: 1.4679 - val_accuracy: 0.5764\n","Epoch 29/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 1.4065 - accuracy: 0.6221\n","Epoch 29: val_accuracy improved from 0.57643 to 0.58168, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 1.4089 - accuracy: 0.6204 - val_loss: 1.4445 - val_accuracy: 0.5817\n","Epoch 30/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 1.3847 - accuracy: 0.6268\n","Epoch 30: val_accuracy improved from 0.58168 to 0.59277, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 1.3842 - accuracy: 0.6275 - val_loss: 1.4209 - val_accuracy: 0.5928\n","Epoch 31/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 1.3628 - accuracy: 0.6320\n","Epoch 31: val_accuracy improved from 0.59277 to 0.60037, saving model to best_model.h5\n","70/70 [==============================] - 1s 12ms/step - loss: 1.3614 - accuracy: 0.6340 - val_loss: 1.3986 - val_accuracy: 0.6004\n","Epoch 32/50\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 1.3384 - accuracy: 0.6431\n","Epoch 32: val_accuracy improved from 0.60037 to 0.60695, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.3394 - accuracy: 0.6414 - val_loss: 1.3769 - val_accuracy: 0.6070\n","Epoch 33/50\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 1.3160 - accuracy: 0.6505\n","Epoch 33: val_accuracy improved from 0.60695 to 0.61475, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.3189 - accuracy: 0.6467 - val_loss: 1.3557 - val_accuracy: 0.6148\n","Epoch 34/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 1.2975 - accuracy: 0.6544\n","Epoch 34: val_accuracy improved from 0.61475 to 0.62001, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.2966 - accuracy: 0.6557 - val_loss: 1.3352 - val_accuracy: 0.6200\n","Epoch 35/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 1.2782 - accuracy: 0.6599\n","Epoch 35: val_accuracy improved from 0.62001 to 0.63073, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 1.2756 - accuracy: 0.6624 - val_loss: 1.3144 - val_accuracy: 0.6307\n","Epoch 36/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 1.2534 - accuracy: 0.6722\n","Epoch 36: val_accuracy improved from 0.63073 to 0.63763, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.2550 - accuracy: 0.6702 - val_loss: 1.2940 - val_accuracy: 0.6376\n","Epoch 37/50\n","70/70 [==============================] - ETA: 0s - loss: 1.2345 - accuracy: 0.6762\n","Epoch 37: val_accuracy improved from 0.63763 to 0.64677, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 1.2345 - accuracy: 0.6762 - val_loss: 1.2729 - val_accuracy: 0.6468\n","Epoch 38/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 1.2169 - accuracy: 0.6807\n","Epoch 38: val_accuracy improved from 0.64677 to 0.65824, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 1.2132 - accuracy: 0.6832 - val_loss: 1.2522 - val_accuracy: 0.6582\n","Epoch 39/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 1.1911 - accuracy: 0.6914\n","Epoch 39: val_accuracy improved from 0.65824 to 0.66774, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.1920 - accuracy: 0.6910 - val_loss: 1.2322 - val_accuracy: 0.6677\n","Epoch 40/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 1.1707 - accuracy: 0.6985\n","Epoch 40: val_accuracy improved from 0.66774 to 0.67431, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.1726 - accuracy: 0.6980 - val_loss: 1.2132 - val_accuracy: 0.6743\n","Epoch 41/50\n","70/70 [==============================] - ETA: 0s - loss: 1.1552 - accuracy: 0.7029\n","Epoch 41: val_accuracy improved from 0.67431 to 0.67993, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.1552 - accuracy: 0.7029 - val_loss: 1.1960 - val_accuracy: 0.6799\n","Epoch 42/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 1.1403 - accuracy: 0.7038\n","Epoch 42: val_accuracy improved from 0.67993 to 0.68586, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.1373 - accuracy: 0.7061 - val_loss: 1.1784 - val_accuracy: 0.6859\n","Epoch 43/50\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 1.1178 - accuracy: 0.7150\n","Epoch 43: val_accuracy improved from 0.68586 to 0.69221, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 1.1196 - accuracy: 0.7122 - val_loss: 1.1606 - val_accuracy: 0.6922\n","Epoch 44/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 1.1043 - accuracy: 0.7148\n","Epoch 44: val_accuracy improved from 0.69221 to 0.69794, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.1030 - accuracy: 0.7161 - val_loss: 1.1410 - val_accuracy: 0.6979\n","Epoch 45/50\n","70/70 [==============================] - ETA: 0s - loss: 1.0853 - accuracy: 0.7193\n","Epoch 45: val_accuracy improved from 0.69794 to 0.70255, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.0853 - accuracy: 0.7193 - val_loss: 1.1221 - val_accuracy: 0.7025\n","Epoch 46/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 1.0672 - accuracy: 0.7242\n","Epoch 46: val_accuracy improved from 0.70255 to 0.70862, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 1.0667 - accuracy: 0.7250 - val_loss: 1.1004 - val_accuracy: 0.7086\n","Epoch 47/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 1.0468 - accuracy: 0.7291\n","Epoch 47: val_accuracy improved from 0.70862 to 0.71313, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 1.0457 - accuracy: 0.7292 - val_loss: 1.0787 - val_accuracy: 0.7131\n","Epoch 48/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 1.0255 - accuracy: 0.7352\n","Epoch 48: val_accuracy improved from 0.71313 to 0.71823, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 1.0246 - accuracy: 0.7354 - val_loss: 1.0614 - val_accuracy: 0.7182\n","Epoch 49/50\n","70/70 [==============================] - ETA: 0s - loss: 1.0067 - accuracy: 0.7412\n","Epoch 49: val_accuracy improved from 0.71823 to 0.72399, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 1.0067 - accuracy: 0.7412 - val_loss: 1.0445 - val_accuracy: 0.7240\n","Epoch 50/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.9840 - accuracy: 0.7488\n","Epoch 50: val_accuracy improved from 0.72399 to 0.73308, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.9894 - accuracy: 0.7464 - val_loss: 1.0253 - val_accuracy: 0.7331\n","Fold 2\n","Epoch 1/50\n","70/70 [==============================] - ETA: 0s - loss: 0.9749 - accuracy: 0.7519\n","Epoch 1: val_accuracy did not improve from 0.73308\n","70/70 [==============================] - 1s 10ms/step - loss: 0.9749 - accuracy: 0.7519 - val_loss: 1.0164 - val_accuracy: 0.7329\n","Epoch 2/50\n","70/70 [==============================] - ETA: 0s - loss: 0.9590 - accuracy: 0.7567\n","Epoch 2: val_accuracy improved from 0.73308 to 0.74183, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.9590 - accuracy: 0.7567 - val_loss: 0.9968 - val_accuracy: 0.7418\n","Epoch 3/50\n","70/70 [==============================] - ETA: 0s - loss: 0.9447 - accuracy: 0.7601\n","Epoch 3: val_accuracy improved from 0.74183 to 0.74471, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.9447 - accuracy: 0.7601 - val_loss: 0.9827 - val_accuracy: 0.7447\n","Epoch 4/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.9295 - accuracy: 0.7635\n","Epoch 4: val_accuracy improved from 0.74471 to 0.74929, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.9306 - accuracy: 0.7629 - val_loss: 0.9697 - val_accuracy: 0.7493\n","Epoch 5/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.9174 - accuracy: 0.7681\n","Epoch 5: val_accuracy improved from 0.74929 to 0.75002, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.9179 - accuracy: 0.7664 - val_loss: 0.9589 - val_accuracy: 0.7500\n","Epoch 6/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.9077 - accuracy: 0.7699\n","Epoch 6: val_accuracy improved from 0.75002 to 0.75531, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.9056 - accuracy: 0.7715 - val_loss: 0.9470 - val_accuracy: 0.7553\n","Epoch 7/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.8904 - accuracy: 0.7777\n","Epoch 7: val_accuracy improved from 0.75531 to 0.75867, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.8928 - accuracy: 0.7755 - val_loss: 0.9338 - val_accuracy: 0.7587\n","Epoch 8/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.8791 - accuracy: 0.7770\n","Epoch 8: val_accuracy improved from 0.75867 to 0.76173, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.8789 - accuracy: 0.7773 - val_loss: 0.9206 - val_accuracy: 0.7617\n","Epoch 9/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.8656 - accuracy: 0.7806\n","Epoch 9: val_accuracy improved from 0.76173 to 0.76314, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.8678 - accuracy: 0.7800 - val_loss: 0.9087 - val_accuracy: 0.7631\n","Epoch 10/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.8512 - accuracy: 0.7855\n","Epoch 10: val_accuracy improved from 0.76314 to 0.76931, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.8559 - accuracy: 0.7830 - val_loss: 0.8942 - val_accuracy: 0.7693\n","Epoch 11/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.8437 - accuracy: 0.7860\n","Epoch 11: val_accuracy improved from 0.76931 to 0.77037, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.8446 - accuracy: 0.7859 - val_loss: 0.8858 - val_accuracy: 0.7704\n","Epoch 12/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.8321 - accuracy: 0.7899\n","Epoch 12: val_accuracy improved from 0.77037 to 0.77464, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.8320 - accuracy: 0.7895 - val_loss: 0.8726 - val_accuracy: 0.7746\n","Epoch 13/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.8229 - accuracy: 0.7899\n","Epoch 13: val_accuracy did not improve from 0.77464\n","70/70 [==============================] - 1s 10ms/step - loss: 0.8202 - accuracy: 0.7915 - val_loss: 0.8652 - val_accuracy: 0.7739\n","Epoch 14/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.8120 - accuracy: 0.7930\n","Epoch 14: val_accuracy improved from 0.77464 to 0.77757, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.8107 - accuracy: 0.7930 - val_loss: 0.8508 - val_accuracy: 0.7776\n","Epoch 15/50\n","70/70 [==============================] - ETA: 0s - loss: 0.7994 - accuracy: 0.7959\n","Epoch 15: val_accuracy improved from 0.77757 to 0.78064, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.7994 - accuracy: 0.7959 - val_loss: 0.8438 - val_accuracy: 0.7806\n","Epoch 16/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.7918 - accuracy: 0.7973\n","Epoch 16: val_accuracy did not improve from 0.78064\n","70/70 [==============================] - 1s 11ms/step - loss: 0.7895 - accuracy: 0.7979 - val_loss: 0.8317 - val_accuracy: 0.7805\n","Epoch 17/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.7769 - accuracy: 0.8004\n","Epoch 17: val_accuracy improved from 0.78064 to 0.78275, saving model to best_model.h5\n","70/70 [==============================] - 1s 14ms/step - loss: 0.7788 - accuracy: 0.8000 - val_loss: 0.8226 - val_accuracy: 0.7828\n","Epoch 18/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.7675 - accuracy: 0.8042\n","Epoch 18: val_accuracy improved from 0.78275 to 0.78660, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.7686 - accuracy: 0.8031 - val_loss: 0.8117 - val_accuracy: 0.7866\n","Epoch 19/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.7593 - accuracy: 0.8063\n","Epoch 19: val_accuracy improved from 0.78660 to 0.78883, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.7590 - accuracy: 0.8060 - val_loss: 0.8013 - val_accuracy: 0.7888\n","Epoch 20/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.7521 - accuracy: 0.8053\n","Epoch 20: val_accuracy improved from 0.78883 to 0.79025, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.7497 - accuracy: 0.8072 - val_loss: 0.7924 - val_accuracy: 0.7903\n","Epoch 21/50\n","70/70 [==============================] - ETA: 0s - loss: 0.7404 - accuracy: 0.8097\n","Epoch 21: val_accuracy improved from 0.79025 to 0.79321, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.7404 - accuracy: 0.8097 - val_loss: 0.7832 - val_accuracy: 0.7932\n","Epoch 22/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.7340 - accuracy: 0.8101\n","Epoch 22: val_accuracy improved from 0.79321 to 0.79517, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.7311 - accuracy: 0.8116 - val_loss: 0.7734 - val_accuracy: 0.7952\n","Epoch 23/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.7212 - accuracy: 0.8154\n","Epoch 23: val_accuracy improved from 0.79517 to 0.79558, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.7227 - accuracy: 0.8133 - val_loss: 0.7655 - val_accuracy: 0.7956\n","Epoch 24/50\n","70/70 [==============================] - ETA: 0s - loss: 0.7140 - accuracy: 0.8160\n","Epoch 24: val_accuracy improved from 0.79558 to 0.79862, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.7140 - accuracy: 0.8160 - val_loss: 0.7555 - val_accuracy: 0.7986\n","Epoch 25/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.7057 - accuracy: 0.8173\n","Epoch 25: val_accuracy improved from 0.79862 to 0.80069, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.7055 - accuracy: 0.8176 - val_loss: 0.7464 - val_accuracy: 0.8007\n","Epoch 26/50\n","70/70 [==============================] - ETA: 0s - loss: 0.6967 - accuracy: 0.8190\n","Epoch 26: val_accuracy improved from 0.80069 to 0.80231, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.6967 - accuracy: 0.8190 - val_loss: 0.7375 - val_accuracy: 0.8023\n","Epoch 27/50\n","70/70 [==============================] - ETA: 0s - loss: 0.6881 - accuracy: 0.8214\n","Epoch 27: val_accuracy improved from 0.80231 to 0.80414, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.6881 - accuracy: 0.8214 - val_loss: 0.7282 - val_accuracy: 0.8041\n","Epoch 28/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.6805 - accuracy: 0.8231\n","Epoch 28: val_accuracy did not improve from 0.80414\n","70/70 [==============================] - 1s 10ms/step - loss: 0.6803 - accuracy: 0.8224 - val_loss: 0.7411 - val_accuracy: 0.7950\n","Epoch 29/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.6815 - accuracy: 0.8194\n","Epoch 29: val_accuracy improved from 0.80414 to 0.80754, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.6819 - accuracy: 0.8200 - val_loss: 0.7180 - val_accuracy: 0.8075\n","Epoch 30/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.6648 - accuracy: 0.8261\n","Epoch 30: val_accuracy improved from 0.80754 to 0.81098, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.6676 - accuracy: 0.8251 - val_loss: 0.7054 - val_accuracy: 0.8110\n","Epoch 31/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.6523 - accuracy: 0.8303\n","Epoch 31: val_accuracy improved from 0.81098 to 0.81183, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.6557 - accuracy: 0.8275 - val_loss: 0.6968 - val_accuracy: 0.8118\n","Epoch 32/50\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.6506 - accuracy: 0.8266\n","Epoch 32: val_accuracy improved from 0.81183 to 0.81212, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.6469 - accuracy: 0.8291 - val_loss: 0.6873 - val_accuracy: 0.8121\n","Epoch 33/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.6363 - accuracy: 0.8320\n","Epoch 33: val_accuracy improved from 0.81212 to 0.81232, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.6380 - accuracy: 0.8306 - val_loss: 0.6790 - val_accuracy: 0.8123\n","Epoch 34/50\n","70/70 [==============================] - ETA: 0s - loss: 0.6299 - accuracy: 0.8315\n","Epoch 34: val_accuracy improved from 0.81232 to 0.81252, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.6299 - accuracy: 0.8315 - val_loss: 0.6701 - val_accuracy: 0.8125\n","Epoch 35/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.6220 - accuracy: 0.8339\n","Epoch 35: val_accuracy improved from 0.81252 to 0.81329, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.6224 - accuracy: 0.8329 - val_loss: 0.6645 - val_accuracy: 0.8133\n","Epoch 36/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.6140 - accuracy: 0.8353\n","Epoch 36: val_accuracy improved from 0.81329 to 0.81560, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.6145 - accuracy: 0.8348 - val_loss: 0.6567 - val_accuracy: 0.8156\n","Epoch 37/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.6071 - accuracy: 0.8371\n","Epoch 37: val_accuracy improved from 0.81560 to 0.81661, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.6074 - accuracy: 0.8368 - val_loss: 0.6500 - val_accuracy: 0.8166\n","Epoch 38/50\n","70/70 [==============================] - ETA: 0s - loss: 0.5996 - accuracy: 0.8382\n","Epoch 38: val_accuracy did not improve from 0.81661\n","70/70 [==============================] - 1s 12ms/step - loss: 0.5996 - accuracy: 0.8382 - val_loss: 0.6430 - val_accuracy: 0.8161\n","Epoch 39/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.5930 - accuracy: 0.8393\n","Epoch 39: val_accuracy improved from 0.81661 to 0.81904, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.5920 - accuracy: 0.8390 - val_loss: 0.6368 - val_accuracy: 0.8190\n","Epoch 40/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.5878 - accuracy: 0.8400\n","Epoch 40: val_accuracy did not improve from 0.81904\n","70/70 [==============================] - 1s 11ms/step - loss: 0.5854 - accuracy: 0.8413 - val_loss: 0.6314 - val_accuracy: 0.8184\n","Epoch 41/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.5749 - accuracy: 0.8440\n","Epoch 41: val_accuracy improved from 0.81904 to 0.82099, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.5781 - accuracy: 0.8422 - val_loss: 0.6234 - val_accuracy: 0.8210\n","Epoch 42/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.5738 - accuracy: 0.8418\n","Epoch 42: val_accuracy did not improve from 0.82099\n","70/70 [==============================] - 1s 10ms/step - loss: 0.5706 - accuracy: 0.8441 - val_loss: 0.6181 - val_accuracy: 0.8201\n","Epoch 43/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.5669 - accuracy: 0.8431\n","Epoch 43: val_accuracy improved from 0.82099 to 0.82455, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.5653 - accuracy: 0.8444 - val_loss: 0.6093 - val_accuracy: 0.8245\n","Epoch 44/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.5572 - accuracy: 0.8467\n","Epoch 44: val_accuracy did not improve from 0.82455\n","70/70 [==============================] - 1s 10ms/step - loss: 0.5585 - accuracy: 0.8461 - val_loss: 0.6060 - val_accuracy: 0.8235\n","Epoch 45/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.5510 - accuracy: 0.8479\n","Epoch 45: val_accuracy improved from 0.82455 to 0.82520, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.5514 - accuracy: 0.8472 - val_loss: 0.6000 - val_accuracy: 0.8252\n","Epoch 46/50\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.5477 - accuracy: 0.8479\n","Epoch 46: val_accuracy improved from 0.82520 to 0.82536, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.5459 - accuracy: 0.8480 - val_loss: 0.5937 - val_accuracy: 0.8254\n","Epoch 47/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.5371 - accuracy: 0.8511\n","Epoch 47: val_accuracy did not improve from 0.82536\n","70/70 [==============================] - 1s 10ms/step - loss: 0.5391 - accuracy: 0.8495 - val_loss: 0.5885 - val_accuracy: 0.8250\n","Epoch 48/50\n","70/70 [==============================] - ETA: 0s - loss: 0.5329 - accuracy: 0.8504\n","Epoch 48: val_accuracy did not improve from 0.82536\n","70/70 [==============================] - 1s 9ms/step - loss: 0.5329 - accuracy: 0.8504 - val_loss: 0.5836 - val_accuracy: 0.8249\n","Epoch 49/50\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.5286 - accuracy: 0.8513\n","Epoch 49: val_accuracy improved from 0.82536 to 0.82811, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.5276 - accuracy: 0.8514 - val_loss: 0.5759 - val_accuracy: 0.8281\n","Epoch 50/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.5230 - accuracy: 0.8524\n","Epoch 50: val_accuracy did not improve from 0.82811\n","70/70 [==============================] - 1s 10ms/step - loss: 0.5222 - accuracy: 0.8528 - val_loss: 0.5729 - val_accuracy: 0.8276\n","Fold 3\n","Epoch 1/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.5176 - accuracy: 0.8535\n","Epoch 1: val_accuracy improved from 0.82811 to 0.82945, saving model to best_model.h5\n","70/70 [==============================] - 1s 11ms/step - loss: 0.5175 - accuracy: 0.8535 - val_loss: 0.5667 - val_accuracy: 0.8295\n","Epoch 2/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.5145 - accuracy: 0.8530\n","Epoch 2: val_accuracy improved from 0.82945 to 0.82981, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.5124 - accuracy: 0.8535 - val_loss: 0.5627 - val_accuracy: 0.8298\n","Epoch 3/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.5097 - accuracy: 0.8527\n","Epoch 3: val_accuracy did not improve from 0.82981\n","70/70 [==============================] - 1s 9ms/step - loss: 0.5070 - accuracy: 0.8541 - val_loss: 0.5586 - val_accuracy: 0.8285\n","Epoch 4/50\n","70/70 [==============================] - ETA: 0s - loss: 0.5014 - accuracy: 0.8555\n","Epoch 4: val_accuracy did not improve from 0.82981\n","70/70 [==============================] - 1s 9ms/step - loss: 0.5014 - accuracy: 0.8555 - val_loss: 0.5536 - val_accuracy: 0.8294\n","Epoch 5/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.4961 - accuracy: 0.8567\n","Epoch 5: val_accuracy improved from 0.82981 to 0.83051, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4984 - accuracy: 0.8566 - val_loss: 0.5505 - val_accuracy: 0.8305\n","Epoch 6/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.4934 - accuracy: 0.8573\n","Epoch 6: val_accuracy improved from 0.83051 to 0.83063, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4923 - accuracy: 0.8581 - val_loss: 0.5450 - val_accuracy: 0.8306\n","Epoch 7/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.4868 - accuracy: 0.8594\n","Epoch 7: val_accuracy improved from 0.83063 to 0.83112, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4871 - accuracy: 0.8595 - val_loss: 0.5423 - val_accuracy: 0.8311\n","Epoch 8/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.4790 - accuracy: 0.8623\n","Epoch 8: val_accuracy improved from 0.83112 to 0.83282, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4823 - accuracy: 0.8603 - val_loss: 0.5379 - val_accuracy: 0.8328\n","Epoch 9/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.4802 - accuracy: 0.8608\n","Epoch 9: val_accuracy improved from 0.83282 to 0.83377, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4775 - accuracy: 0.8621 - val_loss: 0.5327 - val_accuracy: 0.8338\n","Epoch 10/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.4742 - accuracy: 0.8619\n","Epoch 10: val_accuracy improved from 0.83377 to 0.83387, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4729 - accuracy: 0.8616 - val_loss: 0.5295 - val_accuracy: 0.8339\n","Epoch 11/50\n","70/70 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.8624\n","Epoch 11: val_accuracy improved from 0.83387 to 0.83644, saving model to best_model.h5\n","70/70 [==============================] - 1s 18ms/step - loss: 0.4695 - accuracy: 0.8624 - val_loss: 0.5215 - val_accuracy: 0.8364\n","Epoch 12/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.4630 - accuracy: 0.8627\n","Epoch 12: val_accuracy improved from 0.83644 to 0.83744, saving model to best_model.h5\n","70/70 [==============================] - 1s 18ms/step - loss: 0.4652 - accuracy: 0.8620 - val_loss: 0.5181 - val_accuracy: 0.8374\n","Epoch 13/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.4621 - accuracy: 0.8633\n","Epoch 13: val_accuracy did not improve from 0.83744\n","70/70 [==============================] - 1s 19ms/step - loss: 0.4603 - accuracy: 0.8641 - val_loss: 0.5169 - val_accuracy: 0.8351\n","Epoch 14/50\n","70/70 [==============================] - ETA: 0s - loss: 0.4579 - accuracy: 0.8644\n","Epoch 14: val_accuracy improved from 0.83744 to 0.83763, saving model to best_model.h5\n","70/70 [==============================] - 2s 28ms/step - loss: 0.4579 - accuracy: 0.8644 - val_loss: 0.5104 - val_accuracy: 0.8376\n","Epoch 15/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.4496 - accuracy: 0.8670\n","Epoch 15: val_accuracy improved from 0.83763 to 0.83782, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.4515 - accuracy: 0.8661 - val_loss: 0.5067 - val_accuracy: 0.8378\n","Epoch 16/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.4473 - accuracy: 0.8666\n","Epoch 16: val_accuracy improved from 0.83782 to 0.83944, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.4469 - accuracy: 0.8669 - val_loss: 0.5027 - val_accuracy: 0.8394\n","Epoch 17/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.4432 - accuracy: 0.8680\n","Epoch 17: val_accuracy improved from 0.83944 to 0.84064, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4439 - accuracy: 0.8678 - val_loss: 0.4986 - val_accuracy: 0.8406\n","Epoch 18/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.4401 - accuracy: 0.8674\n","Epoch 18: val_accuracy improved from 0.84064 to 0.84196, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4411 - accuracy: 0.8674 - val_loss: 0.4953 - val_accuracy: 0.8420\n","Epoch 19/50\n","70/70 [==============================] - ETA: 0s - loss: 0.4355 - accuracy: 0.8708\n","Epoch 19: val_accuracy did not improve from 0.84196\n","70/70 [==============================] - 1s 10ms/step - loss: 0.4355 - accuracy: 0.8708 - val_loss: 0.4950 - val_accuracy: 0.8398\n","Epoch 20/50\n","70/70 [==============================] - ETA: 0s - loss: 0.4330 - accuracy: 0.8697\n","Epoch 20: val_accuracy improved from 0.84196 to 0.84514, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.4330 - accuracy: 0.8697 - val_loss: 0.4871 - val_accuracy: 0.8451\n","Epoch 21/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.4286 - accuracy: 0.8720\n","Epoch 21: val_accuracy did not improve from 0.84514\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4280 - accuracy: 0.8718 - val_loss: 0.4838 - val_accuracy: 0.8446\n","Epoch 22/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.4250 - accuracy: 0.8729\n","Epoch 22: val_accuracy improved from 0.84514 to 0.84669, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4245 - accuracy: 0.8724 - val_loss: 0.4805 - val_accuracy: 0.8467\n","Epoch 23/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.4220 - accuracy: 0.8742\n","Epoch 23: val_accuracy did not improve from 0.84669\n","70/70 [==============================] - 1s 10ms/step - loss: 0.4206 - accuracy: 0.8739 - val_loss: 0.4782 - val_accuracy: 0.8463\n","Epoch 24/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.4144 - accuracy: 0.8755\n","Epoch 24: val_accuracy improved from 0.84669 to 0.84738, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.4170 - accuracy: 0.8744 - val_loss: 0.4734 - val_accuracy: 0.8474\n","Epoch 25/50\n","70/70 [==============================] - ETA: 0s - loss: 0.4153 - accuracy: 0.8753\n","Epoch 25: val_accuracy improved from 0.84738 to 0.84742, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4153 - accuracy: 0.8753 - val_loss: 0.4706 - val_accuracy: 0.8474\n","Epoch 26/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.4135 - accuracy: 0.8755\n","Epoch 26: val_accuracy improved from 0.84742 to 0.84867, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.4116 - accuracy: 0.8759 - val_loss: 0.4679 - val_accuracy: 0.8487\n","Epoch 27/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.4056 - accuracy: 0.8781\n","Epoch 27: val_accuracy improved from 0.84867 to 0.85043, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4079 - accuracy: 0.8772 - val_loss: 0.4611 - val_accuracy: 0.8504\n","Epoch 28/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.4045 - accuracy: 0.8770\n","Epoch 28: val_accuracy did not improve from 0.85043\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4040 - accuracy: 0.8773 - val_loss: 0.4608 - val_accuracy: 0.8485\n","Epoch 29/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.3986 - accuracy: 0.8788\n","Epoch 29: val_accuracy improved from 0.85043 to 0.85112, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.4010 - accuracy: 0.8780 - val_loss: 0.4555 - val_accuracy: 0.8511\n","Epoch 30/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.3959 - accuracy: 0.8802\n","Epoch 30: val_accuracy improved from 0.85112 to 0.85157, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3975 - accuracy: 0.8796 - val_loss: 0.4527 - val_accuracy: 0.8516\n","Epoch 31/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.3970 - accuracy: 0.8780\n","Epoch 31: val_accuracy improved from 0.85157 to 0.85336, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3951 - accuracy: 0.8785 - val_loss: 0.4488 - val_accuracy: 0.8534\n","Epoch 32/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.3941 - accuracy: 0.8803\n","Epoch 32: val_accuracy did not improve from 0.85336\n","70/70 [==============================] - 1s 15ms/step - loss: 0.3928 - accuracy: 0.8806 - val_loss: 0.4509 - val_accuracy: 0.8530\n","Epoch 33/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.3909 - accuracy: 0.8806\n","Epoch 33: val_accuracy improved from 0.85336 to 0.85381, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.3905 - accuracy: 0.8809 - val_loss: 0.4464 - val_accuracy: 0.8538\n","Epoch 34/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.3867 - accuracy: 0.8807\n","Epoch 34: val_accuracy improved from 0.85381 to 0.85648, saving model to best_model.h5\n","70/70 [==============================] - 1s 12ms/step - loss: 0.3868 - accuracy: 0.8807 - val_loss: 0.4407 - val_accuracy: 0.8565\n","Epoch 35/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.3827 - accuracy: 0.8829\n","Epoch 35: val_accuracy did not improve from 0.85648\n","70/70 [==============================] - 1s 13ms/step - loss: 0.3840 - accuracy: 0.8821 - val_loss: 0.4381 - val_accuracy: 0.8559\n","Epoch 36/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.3826 - accuracy: 0.8813\n","Epoch 36: val_accuracy improved from 0.85648 to 0.85836, saving model to best_model.h5\n","70/70 [==============================] - 1s 12ms/step - loss: 0.3810 - accuracy: 0.8821 - val_loss: 0.4345 - val_accuracy: 0.8584\n","Epoch 37/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.3775 - accuracy: 0.8837\n","Epoch 37: val_accuracy did not improve from 0.85836\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3787 - accuracy: 0.8832 - val_loss: 0.4313 - val_accuracy: 0.8575\n","Epoch 38/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.3745 - accuracy: 0.8844\n","Epoch 38: val_accuracy did not improve from 0.85836\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - accuracy: 0.8842 - val_loss: 0.4296 - val_accuracy: 0.8573\n","Epoch 39/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.3800 - accuracy: 0.8809\n","Epoch 39: val_accuracy improved from 0.85836 to 0.85935, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3719 - accuracy: 0.8848 - val_loss: 0.4269 - val_accuracy: 0.8593\n","Epoch 40/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.3698 - accuracy: 0.8863\n","Epoch 40: val_accuracy did not improve from 0.85935\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3692 - accuracy: 0.8862 - val_loss: 0.4248 - val_accuracy: 0.8590\n","Epoch 41/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.3651 - accuracy: 0.8872\n","Epoch 41: val_accuracy improved from 0.85935 to 0.86136, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3668 - accuracy: 0.8859 - val_loss: 0.4206 - val_accuracy: 0.8614\n","Epoch 42/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.3587 - accuracy: 0.8879\n","Epoch 42: val_accuracy did not improve from 0.86136\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3630 - accuracy: 0.8871 - val_loss: 0.4197 - val_accuracy: 0.8612\n","Epoch 43/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.3608 - accuracy: 0.8869\n","Epoch 43: val_accuracy improved from 0.86136 to 0.86250, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3609 - accuracy: 0.8874 - val_loss: 0.4169 - val_accuracy: 0.8625\n","Epoch 44/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.3625 - accuracy: 0.8884\n","Epoch 44: val_accuracy did not improve from 0.86250\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3595 - accuracy: 0.8887 - val_loss: 0.4140 - val_accuracy: 0.8619\n","Epoch 45/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.3587 - accuracy: 0.8885\n","Epoch 45: val_accuracy did not improve from 0.86250\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3586 - accuracy: 0.8885 - val_loss: 0.4164 - val_accuracy: 0.8623\n","Epoch 46/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.3567 - accuracy: 0.8903\n","Epoch 46: val_accuracy improved from 0.86250 to 0.86314, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3606 - accuracy: 0.8885 - val_loss: 0.4136 - val_accuracy: 0.8631\n","Epoch 47/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.3515 - accuracy: 0.8894\n","Epoch 47: val_accuracy improved from 0.86314 to 0.86682, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3538 - accuracy: 0.8889 - val_loss: 0.4070 - val_accuracy: 0.8668\n","Epoch 48/50\n","70/70 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.8901\n","Epoch 48: val_accuracy did not improve from 0.86682\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3517 - accuracy: 0.8901 - val_loss: 0.4052 - val_accuracy: 0.8648\n","Epoch 49/50\n","70/70 [==============================] - ETA: 0s - loss: 0.3482 - accuracy: 0.8906\n","Epoch 49: val_accuracy did not improve from 0.86682\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3482 - accuracy: 0.8906 - val_loss: 0.4024 - val_accuracy: 0.8655\n","Epoch 50/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.3474 - accuracy: 0.8902\n","Epoch 50: val_accuracy improved from 0.86682 to 0.86714, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3469 - accuracy: 0.8902 - val_loss: 0.4002 - val_accuracy: 0.8671\n","Fold 4\n","Epoch 1/50\n","70/70 [==============================] - ETA: 0s - loss: 0.3428 - accuracy: 0.8916\n","Epoch 1: val_accuracy improved from 0.86714 to 0.86789, saving model to best_model.h5\n","70/70 [==============================] - 1s 14ms/step - loss: 0.3428 - accuracy: 0.8916 - val_loss: 0.3960 - val_accuracy: 0.8679\n","Epoch 2/50\n","70/70 [==============================] - ETA: 0s - loss: 0.3404 - accuracy: 0.8919\n","Epoch 2: val_accuracy did not improve from 0.86789\n","70/70 [==============================] - 1s 13ms/step - loss: 0.3404 - accuracy: 0.8919 - val_loss: 0.3959 - val_accuracy: 0.8677\n","Epoch 3/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.3381 - accuracy: 0.8930\n","Epoch 3: val_accuracy improved from 0.86789 to 0.87013, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.3383 - accuracy: 0.8928 - val_loss: 0.3905 - val_accuracy: 0.8701\n","Epoch 4/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.3348 - accuracy: 0.8936\n","Epoch 4: val_accuracy improved from 0.87013 to 0.87029, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.3358 - accuracy: 0.8931 - val_loss: 0.3889 - val_accuracy: 0.8703\n","Epoch 5/50\n","70/70 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.8943\n","Epoch 5: val_accuracy did not improve from 0.87029\n","70/70 [==============================] - 1s 13ms/step - loss: 0.3330 - accuracy: 0.8943 - val_loss: 0.3864 - val_accuracy: 0.8702\n","Epoch 6/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.3378 - accuracy: 0.8920\n","Epoch 6: val_accuracy improved from 0.87029 to 0.87054, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3365 - accuracy: 0.8925 - val_loss: 0.3880 - val_accuracy: 0.8705\n","Epoch 7/50\n","70/70 [==============================] - ETA: 0s - loss: 0.3304 - accuracy: 0.8945\n","Epoch 7: val_accuracy improved from 0.87054 to 0.87124, saving model to best_model.h5\n","70/70 [==============================] - 1s 11ms/step - loss: 0.3304 - accuracy: 0.8945 - val_loss: 0.3854 - val_accuracy: 0.8712\n","Epoch 8/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.3277 - accuracy: 0.8955\n","Epoch 8: val_accuracy improved from 0.87124 to 0.87326, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3273 - accuracy: 0.8955 - val_loss: 0.3827 - val_accuracy: 0.8733\n","Epoch 9/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.3277 - accuracy: 0.8944\n","Epoch 9: val_accuracy did not improve from 0.87326\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3246 - accuracy: 0.8959 - val_loss: 0.3791 - val_accuracy: 0.8731\n","Epoch 10/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.3215 - accuracy: 0.8969\n","Epoch 10: val_accuracy improved from 0.87326 to 0.87494, saving model to best_model.h5\n","70/70 [==============================] - 1s 11ms/step - loss: 0.3212 - accuracy: 0.8972 - val_loss: 0.3744 - val_accuracy: 0.8749\n","Epoch 11/50\n","70/70 [==============================] - ETA: 0s - loss: 0.3198 - accuracy: 0.8969\n","Epoch 11: val_accuracy did not improve from 0.87494\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3198 - accuracy: 0.8969 - val_loss: 0.3738 - val_accuracy: 0.8748\n","Epoch 12/50\n","70/70 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.8980\n","Epoch 12: val_accuracy improved from 0.87494 to 0.87593, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3165 - accuracy: 0.8980 - val_loss: 0.3703 - val_accuracy: 0.8759\n","Epoch 13/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.3148 - accuracy: 0.8987\n","Epoch 13: val_accuracy improved from 0.87593 to 0.87636, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3147 - accuracy: 0.8986 - val_loss: 0.3690 - val_accuracy: 0.8764\n","Epoch 14/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.3113 - accuracy: 0.9004\n","Epoch 14: val_accuracy improved from 0.87636 to 0.87730, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.3132 - accuracy: 0.8993 - val_loss: 0.3667 - val_accuracy: 0.8773\n","Epoch 15/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.3122 - accuracy: 0.8987\n","Epoch 15: val_accuracy did not improve from 0.87730\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3108 - accuracy: 0.8993 - val_loss: 0.3648 - val_accuracy: 0.8770\n","Epoch 16/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.3075 - accuracy: 0.9012\n","Epoch 16: val_accuracy improved from 0.87730 to 0.87826, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3085 - accuracy: 0.9000 - val_loss: 0.3635 - val_accuracy: 0.8783\n","Epoch 17/50\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.3054 - accuracy: 0.9007\n","Epoch 17: val_accuracy improved from 0.87826 to 0.87837, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3070 - accuracy: 0.9001 - val_loss: 0.3621 - val_accuracy: 0.8784\n","Epoch 18/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.3049 - accuracy: 0.9012\n","Epoch 18: val_accuracy improved from 0.87837 to 0.87950, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3043 - accuracy: 0.9015 - val_loss: 0.3577 - val_accuracy: 0.8795\n","Epoch 19/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.3000 - accuracy: 0.9027\n","Epoch 19: val_accuracy did not improve from 0.87950\n","70/70 [==============================] - 1s 9ms/step - loss: 0.3024 - accuracy: 0.9016 - val_loss: 0.3583 - val_accuracy: 0.8791\n","Epoch 20/50\n","70/70 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.9024\n","Epoch 20: val_accuracy improved from 0.87950 to 0.87963, saving model to best_model.h5\n","70/70 [==============================] - 1s 11ms/step - loss: 0.3002 - accuracy: 0.9024 - val_loss: 0.3559 - val_accuracy: 0.8796\n","Epoch 21/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2998 - accuracy: 0.9024\n","Epoch 21: val_accuracy did not improve from 0.87963\n","70/70 [==============================] - 1s 12ms/step - loss: 0.2998 - accuracy: 0.9024 - val_loss: 0.3547 - val_accuracy: 0.8790\n","Epoch 22/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.2984 - accuracy: 0.9026\n","Epoch 22: val_accuracy improved from 0.87963 to 0.88033, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.2986 - accuracy: 0.9026 - val_loss: 0.3515 - val_accuracy: 0.8803\n","Epoch 23/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9035\n","Epoch 23: val_accuracy did not improve from 0.88033\n","70/70 [==============================] - 1s 13ms/step - loss: 0.2960 - accuracy: 0.9036 - val_loss: 0.3505 - val_accuracy: 0.8802\n","Epoch 24/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.2943 - accuracy: 0.9034\n","Epoch 24: val_accuracy improved from 0.88033 to 0.88180, saving model to best_model.h5\n","70/70 [==============================] - 1s 14ms/step - loss: 0.2935 - accuracy: 0.9037 - val_loss: 0.3472 - val_accuracy: 0.8818\n","Epoch 25/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.2931 - accuracy: 0.9047\n","Epoch 25: val_accuracy improved from 0.88180 to 0.88285, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.2910 - accuracy: 0.9052 - val_loss: 0.3475 - val_accuracy: 0.8828\n","Epoch 26/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2904 - accuracy: 0.9051\n","Epoch 26: val_accuracy improved from 0.88285 to 0.88317, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2906 - accuracy: 0.9051 - val_loss: 0.3447 - val_accuracy: 0.8832\n","Epoch 27/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.2880 - accuracy: 0.9055\n","Epoch 27: val_accuracy did not improve from 0.88317\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2875 - accuracy: 0.9061 - val_loss: 0.3433 - val_accuracy: 0.8829\n","Epoch 28/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9060\n","Epoch 28: val_accuracy improved from 0.88317 to 0.88331, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2869 - accuracy: 0.9066 - val_loss: 0.3442 - val_accuracy: 0.8833\n","Epoch 29/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9075\n","Epoch 29: val_accuracy improved from 0.88331 to 0.88456, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2863 - accuracy: 0.9067 - val_loss: 0.3387 - val_accuracy: 0.8846\n","Epoch 30/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2809 - accuracy: 0.9091\n","Epoch 30: val_accuracy improved from 0.88456 to 0.88561, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2819 - accuracy: 0.9084 - val_loss: 0.3364 - val_accuracy: 0.8856\n","Epoch 31/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.9077\n","Epoch 31: val_accuracy did not improve from 0.88561\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2815 - accuracy: 0.9077 - val_loss: 0.3371 - val_accuracy: 0.8855\n","Epoch 32/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.2792 - accuracy: 0.9088\n","Epoch 32: val_accuracy improved from 0.88561 to 0.88574, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2810 - accuracy: 0.9079 - val_loss: 0.3345 - val_accuracy: 0.8857\n","Epoch 33/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.9075\n","Epoch 33: val_accuracy did not improve from 0.88574\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2808 - accuracy: 0.9075 - val_loss: 0.3320 - val_accuracy: 0.8857\n","Epoch 34/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2778 - accuracy: 0.9090\n","Epoch 34: val_accuracy did not improve from 0.88574\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2766 - accuracy: 0.9093 - val_loss: 0.3325 - val_accuracy: 0.8852\n","Epoch 35/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.2750 - accuracy: 0.9094\n","Epoch 35: val_accuracy did not improve from 0.88574\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2752 - accuracy: 0.9095 - val_loss: 0.3314 - val_accuracy: 0.8856\n","Epoch 36/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.9105\n","Epoch 36: val_accuracy improved from 0.88574 to 0.88648, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2729 - accuracy: 0.9105 - val_loss: 0.3293 - val_accuracy: 0.8865\n","Epoch 37/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.2752 - accuracy: 0.9096\n","Epoch 37: val_accuracy did not improve from 0.88648\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2722 - accuracy: 0.9106 - val_loss: 0.3293 - val_accuracy: 0.8853\n","Epoch 38/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.9114\n","Epoch 38: val_accuracy improved from 0.88648 to 0.88795, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2702 - accuracy: 0.9114 - val_loss: 0.3252 - val_accuracy: 0.8880\n","Epoch 39/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.2715 - accuracy: 0.9103\n","Epoch 39: val_accuracy did not improve from 0.88795\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2682 - accuracy: 0.9118 - val_loss: 0.3234 - val_accuracy: 0.8879\n","Epoch 40/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.2668 - accuracy: 0.9132\n","Epoch 40: val_accuracy did not improve from 0.88795\n","70/70 [==============================] - 1s 12ms/step - loss: 0.2663 - accuracy: 0.9129 - val_loss: 0.3226 - val_accuracy: 0.8878\n","Epoch 41/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.9135\n","Epoch 41: val_accuracy improved from 0.88795 to 0.88869, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.2642 - accuracy: 0.9135 - val_loss: 0.3200 - val_accuracy: 0.8887\n","Epoch 42/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2634 - accuracy: 0.9136\n","Epoch 42: val_accuracy improved from 0.88869 to 0.88945, saving model to best_model.h5\n","70/70 [==============================] - 1s 14ms/step - loss: 0.2636 - accuracy: 0.9130 - val_loss: 0.3189 - val_accuracy: 0.8895\n","Epoch 43/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9149\n","Epoch 43: val_accuracy improved from 0.88945 to 0.88964, saving model to best_model.h5\n","70/70 [==============================] - 1s 14ms/step - loss: 0.2610 - accuracy: 0.9149 - val_loss: 0.3168 - val_accuracy: 0.8896\n","Epoch 44/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.2604 - accuracy: 0.9142\n","Epoch 44: val_accuracy improved from 0.88964 to 0.89005, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.2595 - accuracy: 0.9146 - val_loss: 0.3146 - val_accuracy: 0.8900\n","Epoch 45/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9153\n","Epoch 45: val_accuracy improved from 0.89005 to 0.89079, saving model to best_model.h5\n","70/70 [==============================] - 1s 12ms/step - loss: 0.2580 - accuracy: 0.9153 - val_loss: 0.3144 - val_accuracy: 0.8908\n","Epoch 46/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.9158\n","Epoch 46: val_accuracy did not improve from 0.89079\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2568 - accuracy: 0.9158 - val_loss: 0.3136 - val_accuracy: 0.8907\n","Epoch 47/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.2566 - accuracy: 0.9164\n","Epoch 47: val_accuracy improved from 0.89079 to 0.89092, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2553 - accuracy: 0.9161 - val_loss: 0.3113 - val_accuracy: 0.8909\n","Epoch 48/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.2532 - accuracy: 0.9175\n","Epoch 48: val_accuracy improved from 0.89092 to 0.89226, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2535 - accuracy: 0.9168 - val_loss: 0.3091 - val_accuracy: 0.8923\n","Epoch 49/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9172\n","Epoch 49: val_accuracy did not improve from 0.89226\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2518 - accuracy: 0.9176 - val_loss: 0.3079 - val_accuracy: 0.8909\n","Epoch 50/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.2466 - accuracy: 0.9189\n","Epoch 50: val_accuracy improved from 0.89226 to 0.89263, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2511 - accuracy: 0.9169 - val_loss: 0.3056 - val_accuracy: 0.8926\n","Fold 5\n","Epoch 1/50\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.2445 - accuracy: 0.9205\n","Epoch 1: val_accuracy improved from 0.89263 to 0.89300, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2494 - accuracy: 0.9185 - val_loss: 0.3055 - val_accuracy: 0.8930\n","Epoch 2/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.9183\n","Epoch 2: val_accuracy did not improve from 0.89300\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2484 - accuracy: 0.9183 - val_loss: 0.3044 - val_accuracy: 0.8927\n","Epoch 3/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9189\n","Epoch 3: val_accuracy improved from 0.89300 to 0.89306, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2482 - accuracy: 0.9189 - val_loss: 0.3036 - val_accuracy: 0.8931\n","Epoch 4/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.2476 - accuracy: 0.9186\n","Epoch 4: val_accuracy improved from 0.89306 to 0.89469, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2481 - accuracy: 0.9191 - val_loss: 0.3003 - val_accuracy: 0.8947\n","Epoch 5/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2431 - accuracy: 0.9211\n","Epoch 5: val_accuracy improved from 0.89469 to 0.89582, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2433 - accuracy: 0.9212 - val_loss: 0.2991 - val_accuracy: 0.8958\n","Epoch 6/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.2434 - accuracy: 0.9212\n","Epoch 6: val_accuracy improved from 0.89582 to 0.89605, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2412 - accuracy: 0.9221 - val_loss: 0.2994 - val_accuracy: 0.8960\n","Epoch 7/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.2425 - accuracy: 0.9210\n","Epoch 7: val_accuracy improved from 0.89605 to 0.89677, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2409 - accuracy: 0.9223 - val_loss: 0.2978 - val_accuracy: 0.8968\n","Epoch 8/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.2365 - accuracy: 0.9229\n","Epoch 8: val_accuracy did not improve from 0.89677\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2387 - accuracy: 0.9226 - val_loss: 0.2987 - val_accuracy: 0.8954\n","Epoch 9/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.2394 - accuracy: 0.9228\n","Epoch 9: val_accuracy did not improve from 0.89677\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2383 - accuracy: 0.9228 - val_loss: 0.2966 - val_accuracy: 0.8967\n","Epoch 10/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.2388 - accuracy: 0.9217\n","Epoch 10: val_accuracy improved from 0.89677 to 0.89764, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.2372 - accuracy: 0.9228 - val_loss: 0.2956 - val_accuracy: 0.8976\n","Epoch 11/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9233\n","Epoch 11: val_accuracy improved from 0.89764 to 0.89783, saving model to best_model.h5\n","70/70 [==============================] - 1s 14ms/step - loss: 0.2363 - accuracy: 0.9233 - val_loss: 0.2958 - val_accuracy: 0.8978\n","Epoch 12/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.2340 - accuracy: 0.9239\n","Epoch 12: val_accuracy did not improve from 0.89783\n","70/70 [==============================] - 1s 14ms/step - loss: 0.2349 - accuracy: 0.9237 - val_loss: 0.2972 - val_accuracy: 0.8959\n","Epoch 13/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2333 - accuracy: 0.9244\n","Epoch 13: val_accuracy improved from 0.89783 to 0.89900, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.2327 - accuracy: 0.9247 - val_loss: 0.2911 - val_accuracy: 0.8990\n","Epoch 14/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2309 - accuracy: 0.9255\n","Epoch 14: val_accuracy improved from 0.89900 to 0.89958, saving model to best_model.h5\n","70/70 [==============================] - 1s 14ms/step - loss: 0.2315 - accuracy: 0.9250 - val_loss: 0.2888 - val_accuracy: 0.8996\n","Epoch 15/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.2330 - accuracy: 0.9239\n","Epoch 15: val_accuracy improved from 0.89958 to 0.89973, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2312 - accuracy: 0.9251 - val_loss: 0.2888 - val_accuracy: 0.8997\n","Epoch 16/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2281 - accuracy: 0.9260\n","Epoch 16: val_accuracy improved from 0.89973 to 0.90076, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2281 - accuracy: 0.9260 - val_loss: 0.2873 - val_accuracy: 0.9008\n","Epoch 17/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2268 - accuracy: 0.9269\n","Epoch 17: val_accuracy did not improve from 0.90076\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2268 - accuracy: 0.9269 - val_loss: 0.2885 - val_accuracy: 0.9000\n","Epoch 18/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.2234 - accuracy: 0.9279\n","Epoch 18: val_accuracy did not improve from 0.90076\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2257 - accuracy: 0.9271 - val_loss: 0.2882 - val_accuracy: 0.8997\n","Epoch 19/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9272\n","Epoch 19: val_accuracy improved from 0.90076 to 0.90085, saving model to best_model.h5\n","70/70 [==============================] - 1s 11ms/step - loss: 0.2254 - accuracy: 0.9272 - val_loss: 0.2864 - val_accuracy: 0.9008\n","Epoch 20/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.2237 - accuracy: 0.9277\n","Epoch 20: val_accuracy improved from 0.90085 to 0.90162, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2247 - accuracy: 0.9268 - val_loss: 0.2851 - val_accuracy: 0.9016\n","Epoch 21/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2229 - accuracy: 0.9274\n","Epoch 21: val_accuracy did not improve from 0.90162\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2229 - accuracy: 0.9274 - val_loss: 0.2842 - val_accuracy: 0.9008\n","Epoch 22/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2196 - accuracy: 0.9282\n","Epoch 22: val_accuracy did not improve from 0.90162\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2221 - accuracy: 0.9278 - val_loss: 0.2831 - val_accuracy: 0.9012\n","Epoch 23/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.2218 - accuracy: 0.9283\n","Epoch 23: val_accuracy improved from 0.90162 to 0.90370, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2207 - accuracy: 0.9289 - val_loss: 0.2799 - val_accuracy: 0.9037\n","Epoch 24/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2191 - accuracy: 0.9291\n","Epoch 24: val_accuracy did not improve from 0.90370\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2191 - accuracy: 0.9291 - val_loss: 0.2807 - val_accuracy: 0.9022\n","Epoch 25/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.2183 - accuracy: 0.9293\n","Epoch 25: val_accuracy did not improve from 0.90370\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2186 - accuracy: 0.9292 - val_loss: 0.2771 - val_accuracy: 0.9036\n","Epoch 26/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2177 - accuracy: 0.9291\n","Epoch 26: val_accuracy improved from 0.90370 to 0.90440, saving model to best_model.h5\n","70/70 [==============================] - 1s 11ms/step - loss: 0.2163 - accuracy: 0.9297 - val_loss: 0.2758 - val_accuracy: 0.9044\n","Epoch 27/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2164 - accuracy: 0.9299\n","Epoch 27: val_accuracy did not improve from 0.90440\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2155 - accuracy: 0.9303 - val_loss: 0.2774 - val_accuracy: 0.9034\n","Epoch 28/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2140 - accuracy: 0.9307\n","Epoch 28: val_accuracy did not improve from 0.90440\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2140 - accuracy: 0.9307 - val_loss: 0.2758 - val_accuracy: 0.9042\n","Epoch 29/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.2139 - accuracy: 0.9304\n","Epoch 29: val_accuracy improved from 0.90440 to 0.90470, saving model to best_model.h5\n","70/70 [==============================] - 1s 14ms/step - loss: 0.2127 - accuracy: 0.9310 - val_loss: 0.2751 - val_accuracy: 0.9047\n","Epoch 30/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.2101 - accuracy: 0.9320\n","Epoch 30: val_accuracy improved from 0.90470 to 0.90499, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.2120 - accuracy: 0.9314 - val_loss: 0.2760 - val_accuracy: 0.9050\n","Epoch 31/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.2111 - accuracy: 0.9324\n","Epoch 31: val_accuracy did not improve from 0.90499\n","70/70 [==============================] - 1s 13ms/step - loss: 0.2114 - accuracy: 0.9314 - val_loss: 0.2761 - val_accuracy: 0.9033\n","Epoch 32/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2101 - accuracy: 0.9318\n","Epoch 32: val_accuracy did not improve from 0.90499\n","70/70 [==============================] - 1s 12ms/step - loss: 0.2101 - accuracy: 0.9318 - val_loss: 0.2744 - val_accuracy: 0.9045\n","Epoch 33/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2095 - accuracy: 0.9321\n","Epoch 33: val_accuracy improved from 0.90499 to 0.90601, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.2095 - accuracy: 0.9321 - val_loss: 0.2717 - val_accuracy: 0.9060\n","Epoch 34/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.2077 - accuracy: 0.9321\n","Epoch 34: val_accuracy did not improve from 0.90601\n","70/70 [==============================] - 1s 12ms/step - loss: 0.2089 - accuracy: 0.9319 - val_loss: 0.2712 - val_accuracy: 0.9058\n","Epoch 35/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.2136 - accuracy: 0.9307\n","Epoch 35: val_accuracy did not improve from 0.90601\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2142 - accuracy: 0.9305 - val_loss: 0.2724 - val_accuracy: 0.9052\n","Epoch 36/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2091 - accuracy: 0.9326\n","Epoch 36: val_accuracy did not improve from 0.90601\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2091 - accuracy: 0.9326 - val_loss: 0.2711 - val_accuracy: 0.9054\n","Epoch 37/50\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.2047 - accuracy: 0.9344\n","Epoch 37: val_accuracy improved from 0.90601 to 0.90668, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2050 - accuracy: 0.9341 - val_loss: 0.2683 - val_accuracy: 0.9067\n","Epoch 38/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.9341\n","Epoch 38: val_accuracy did not improve from 0.90668\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2035 - accuracy: 0.9341 - val_loss: 0.2675 - val_accuracy: 0.9064\n","Epoch 39/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.2004 - accuracy: 0.9353\n","Epoch 39: val_accuracy improved from 0.90668 to 0.90795, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2023 - accuracy: 0.9342 - val_loss: 0.2649 - val_accuracy: 0.9080\n","Epoch 40/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2017 - accuracy: 0.9344\n","Epoch 40: val_accuracy did not improve from 0.90795\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2017 - accuracy: 0.9344 - val_loss: 0.2669 - val_accuracy: 0.9064\n","Epoch 41/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.9342\n","Epoch 41: val_accuracy improved from 0.90795 to 0.90876, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2012 - accuracy: 0.9342 - val_loss: 0.2612 - val_accuracy: 0.9088\n","Epoch 42/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9355\n","Epoch 42: val_accuracy did not improve from 0.90876\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2003 - accuracy: 0.9355 - val_loss: 0.2617 - val_accuracy: 0.9078\n","Epoch 43/50\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.2002 - accuracy: 0.9353\n","Epoch 43: val_accuracy improved from 0.90876 to 0.90954, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1986 - accuracy: 0.9356 - val_loss: 0.2593 - val_accuracy: 0.9095\n","Epoch 44/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1967 - accuracy: 0.9360\n","Epoch 44: val_accuracy did not improve from 0.90954\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1975 - accuracy: 0.9362 - val_loss: 0.2596 - val_accuracy: 0.9085\n","Epoch 45/50\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.2020 - accuracy: 0.9341\n","Epoch 45: val_accuracy did not improve from 0.90954\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2025 - accuracy: 0.9338 - val_loss: 0.2882 - val_accuracy: 0.9006\n","Epoch 46/50\n","70/70 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9309\n","Epoch 46: val_accuracy did not improve from 0.90954\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2122 - accuracy: 0.9309 - val_loss: 0.2741 - val_accuracy: 0.9039\n","Epoch 47/50\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9326\n","Epoch 47: val_accuracy did not improve from 0.90954\n","70/70 [==============================] - 1s 9ms/step - loss: 0.2073 - accuracy: 0.9330 - val_loss: 0.2667 - val_accuracy: 0.9078\n","Epoch 48/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9345\n","Epoch 48: val_accuracy did not improve from 0.90954\n","70/70 [==============================] - 1s 10ms/step - loss: 0.2015 - accuracy: 0.9343 - val_loss: 0.2671 - val_accuracy: 0.9044\n","Epoch 49/50\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.2001 - accuracy: 0.9353\n","Epoch 49: val_accuracy did not improve from 0.90954\n","70/70 [==============================] - 1s 11ms/step - loss: 0.1998 - accuracy: 0.9355 - val_loss: 0.2600 - val_accuracy: 0.9088\n","Epoch 50/50\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9363\n","Epoch 50: val_accuracy did not improve from 0.90954\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1971 - accuracy: 0.9361 - val_loss: 0.2653 - val_accuracy: 0.9079\n"]}],"source":["kfold = KFold(n_splits=5, shuffle=True)\n","\n","for fold, (train_indices, val_indices) in enumerate(kfold.split(X_train)):\n","    print(f\"Fold {fold+1}\")\n","    model.fit(X_train, y_train, validation_split=0.2, epochs=50, callbacks=[checkpoint_callback])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1686605440592,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"H_RM6czepRwZ","outputId":"17093526-7c40-4b26-ca96-07651fb67ae7"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2800, 150, 11) (700, 150, 11)\n"]}],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n","print(X_train.shape, X_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"KN_mKEEQBnHc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1980 - accuracy: 0.9363\n","Epoch 1: val_accuracy improved from 0.90954 to 0.90969, saving model to best_model.h5\n","70/70 [==============================] - 1s 11ms/step - loss: 0.1959 - accuracy: 0.9370 - val_loss: 0.2580 - val_accuracy: 0.9097\n","Epoch 2/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9370\n","Epoch 2: val_accuracy improved from 0.90969 to 0.91076, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1942 - accuracy: 0.9376 - val_loss: 0.2551 - val_accuracy: 0.9108\n","Epoch 3/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9379\n","Epoch 3: val_accuracy did not improve from 0.91076\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1930 - accuracy: 0.9379 - val_loss: 0.2561 - val_accuracy: 0.9098\n","Epoch 4/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9377\n","Epoch 4: val_accuracy did not improve from 0.91076\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1925 - accuracy: 0.9377 - val_loss: 0.2535 - val_accuracy: 0.9106\n","Epoch 5/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1902 - accuracy: 0.9388\n","Epoch 5: val_accuracy improved from 0.91076 to 0.91195, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1904 - accuracy: 0.9387 - val_loss: 0.2519 - val_accuracy: 0.9120\n","Epoch 6/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1854 - accuracy: 0.9420\n","Epoch 6: val_accuracy did not improve from 0.91195\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1881 - accuracy: 0.9401 - val_loss: 0.2521 - val_accuracy: 0.9112\n","Epoch 7/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1858 - accuracy: 0.9400\n","Epoch 7: val_accuracy improved from 0.91195 to 0.91246, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1868 - accuracy: 0.9404 - val_loss: 0.2498 - val_accuracy: 0.9125\n","Epoch 8/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1871 - accuracy: 0.9401\n","Epoch 8: val_accuracy improved from 0.91246 to 0.91406, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1861 - accuracy: 0.9404 - val_loss: 0.2458 - val_accuracy: 0.9141\n","Epoch 9/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1834 - accuracy: 0.9410\n","Epoch 9: val_accuracy improved from 0.91406 to 0.91418, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1846 - accuracy: 0.9406 - val_loss: 0.2458 - val_accuracy: 0.9142\n","Epoch 10/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1826 - accuracy: 0.9423\n","Epoch 10: val_accuracy did not improve from 0.91418\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1836 - accuracy: 0.9417 - val_loss: 0.2497 - val_accuracy: 0.9122\n","Epoch 11/150\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.1818 - accuracy: 0.9421\n","Epoch 11: val_accuracy did not improve from 0.91418\n","70/70 [==============================] - 1s 8ms/step - loss: 0.1825 - accuracy: 0.9417 - val_loss: 0.2488 - val_accuracy: 0.9125\n","Epoch 12/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9415\n","Epoch 12: val_accuracy did not improve from 0.91418\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1823 - accuracy: 0.9415 - val_loss: 0.2467 - val_accuracy: 0.9134\n","Epoch 13/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1798 - accuracy: 0.9429\n","Epoch 13: val_accuracy did not improve from 0.91418\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1811 - accuracy: 0.9419 - val_loss: 0.2472 - val_accuracy: 0.9128\n","Epoch 14/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1815 - accuracy: 0.9416\n","Epoch 14: val_accuracy did not improve from 0.91418\n","70/70 [==============================] - 1s 14ms/step - loss: 0.1814 - accuracy: 0.9414 - val_loss: 0.2456 - val_accuracy: 0.9127\n","Epoch 15/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9425\n","Epoch 15: val_accuracy did not improve from 0.91418\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1802 - accuracy: 0.9421 - val_loss: 0.2452 - val_accuracy: 0.9138\n","Epoch 16/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1787 - accuracy: 0.9426\n","Epoch 16: val_accuracy did not improve from 0.91418\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1790 - accuracy: 0.9424 - val_loss: 0.2471 - val_accuracy: 0.9117\n","Epoch 17/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1796 - accuracy: 0.9418\n","Epoch 17: val_accuracy did not improve from 0.91418\n","70/70 [==============================] - 1s 11ms/step - loss: 0.1791 - accuracy: 0.9417 - val_loss: 0.2440 - val_accuracy: 0.9128\n","Epoch 18/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1779 - accuracy: 0.9429\n","Epoch 18: val_accuracy did not improve from 0.91418\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1778 - accuracy: 0.9426 - val_loss: 0.2434 - val_accuracy: 0.9136\n","Epoch 19/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1756 - accuracy: 0.9438\n","Epoch 19: val_accuracy improved from 0.91418 to 0.91463, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1766 - accuracy: 0.9429 - val_loss: 0.2435 - val_accuracy: 0.9146\n","Epoch 20/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1742 - accuracy: 0.9441\n","Epoch 20: val_accuracy did not improve from 0.91463\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1755 - accuracy: 0.9436 - val_loss: 0.2422 - val_accuracy: 0.9143\n","Epoch 21/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1737 - accuracy: 0.9452\n","Epoch 21: val_accuracy did not improve from 0.91463\n","70/70 [==============================] - 1s 8ms/step - loss: 0.1746 - accuracy: 0.9441 - val_loss: 0.2441 - val_accuracy: 0.9139\n","Epoch 22/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1740 - accuracy: 0.9443\n","Epoch 22: val_accuracy improved from 0.91463 to 0.91523, saving model to best_model.h5\n","70/70 [==============================] - 1s 8ms/step - loss: 0.1758 - accuracy: 0.9436 - val_loss: 0.2421 - val_accuracy: 0.9152\n","Epoch 23/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1730 - accuracy: 0.9450\n","Epoch 23: val_accuracy improved from 0.91523 to 0.91533, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1730 - accuracy: 0.9450 - val_loss: 0.2410 - val_accuracy: 0.9153\n","Epoch 24/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1730 - accuracy: 0.9452\n","Epoch 24: val_accuracy did not improve from 0.91533\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1722 - accuracy: 0.9453 - val_loss: 0.2425 - val_accuracy: 0.9143\n","Epoch 25/150\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.1702 - accuracy: 0.9462\n","Epoch 25: val_accuracy did not improve from 0.91533\n","70/70 [==============================] - 1s 8ms/step - loss: 0.1715 - accuracy: 0.9452 - val_loss: 0.2422 - val_accuracy: 0.9140\n","Epoch 26/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1743 - accuracy: 0.9436\n","Epoch 26: val_accuracy improved from 0.91533 to 0.91661, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1741 - accuracy: 0.9437 - val_loss: 0.2375 - val_accuracy: 0.9166\n","Epoch 27/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1745 - accuracy: 0.9444\n","Epoch 27: val_accuracy did not improve from 0.91661\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1720 - accuracy: 0.9452 - val_loss: 0.2376 - val_accuracy: 0.9165\n","Epoch 28/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1681 - accuracy: 0.9467\n","Epoch 28: val_accuracy did not improve from 0.91661\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1694 - accuracy: 0.9459 - val_loss: 0.2388 - val_accuracy: 0.9153\n","Epoch 29/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9466\n","Epoch 29: val_accuracy improved from 0.91661 to 0.91675, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1687 - accuracy: 0.9466 - val_loss: 0.2377 - val_accuracy: 0.9168\n","Epoch 30/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1681 - accuracy: 0.9461\n","Epoch 30: val_accuracy improved from 0.91675 to 0.91714, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1681 - accuracy: 0.9461 - val_loss: 0.2371 - val_accuracy: 0.9171\n","Epoch 31/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1667 - accuracy: 0.9477\n","Epoch 31: val_accuracy improved from 0.91714 to 0.91786, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1674 - accuracy: 0.9471 - val_loss: 0.2352 - val_accuracy: 0.9179\n","Epoch 32/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1657 - accuracy: 0.9475\n","Epoch 32: val_accuracy did not improve from 0.91786\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1665 - accuracy: 0.9469 - val_loss: 0.2342 - val_accuracy: 0.9174\n","Epoch 33/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.9472\n","Epoch 33: val_accuracy did not improve from 0.91786\n","70/70 [==============================] - 1s 11ms/step - loss: 0.1660 - accuracy: 0.9472 - val_loss: 0.2395 - val_accuracy: 0.9154\n","Epoch 34/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1657 - accuracy: 0.9473\n","Epoch 34: val_accuracy did not improve from 0.91786\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1660 - accuracy: 0.9471 - val_loss: 0.2319 - val_accuracy: 0.9175\n","Epoch 35/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1666 - accuracy: 0.9468\n","Epoch 35: val_accuracy did not improve from 0.91786\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1680 - accuracy: 0.9462 - val_loss: 0.2425 - val_accuracy: 0.9121\n","Epoch 36/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9464\n","Epoch 36: val_accuracy did not improve from 0.91786\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1671 - accuracy: 0.9464 - val_loss: 0.2324 - val_accuracy: 0.9173\n","Epoch 37/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.9478\n","Epoch 37: val_accuracy did not improve from 0.91786\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1634 - accuracy: 0.9478 - val_loss: 0.2332 - val_accuracy: 0.9171\n","Epoch 38/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1629 - accuracy: 0.9479\n","Epoch 38: val_accuracy improved from 0.91786 to 0.91899, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1631 - accuracy: 0.9474 - val_loss: 0.2306 - val_accuracy: 0.9190\n","Epoch 39/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1611 - accuracy: 0.9493\n","Epoch 39: val_accuracy did not improve from 0.91899\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1614 - accuracy: 0.9490 - val_loss: 0.2316 - val_accuracy: 0.9177\n","Epoch 40/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1594 - accuracy: 0.9497\n","Epoch 40: val_accuracy improved from 0.91899 to 0.91930, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1609 - accuracy: 0.9490 - val_loss: 0.2296 - val_accuracy: 0.9193\n","Epoch 41/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1601 - accuracy: 0.9492\n","Epoch 41: val_accuracy did not improve from 0.91930\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1604 - accuracy: 0.9489 - val_loss: 0.2281 - val_accuracy: 0.9193\n","Epoch 42/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1600 - accuracy: 0.9490\n","Epoch 42: val_accuracy improved from 0.91930 to 0.91958, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1600 - accuracy: 0.9490 - val_loss: 0.2275 - val_accuracy: 0.9196\n","Epoch 43/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9491\n","Epoch 43: val_accuracy did not improve from 0.91958\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1596 - accuracy: 0.9491 - val_loss: 0.2292 - val_accuracy: 0.9184\n","Epoch 44/150\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.1584 - accuracy: 0.9497\n","Epoch 44: val_accuracy improved from 0.91958 to 0.92039, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1599 - accuracy: 0.9486 - val_loss: 0.2244 - val_accuracy: 0.9204\n","Epoch 45/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1591 - accuracy: 0.9501\n","Epoch 45: val_accuracy improved from 0.92039 to 0.92049, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1589 - accuracy: 0.9496 - val_loss: 0.2251 - val_accuracy: 0.9205\n","Epoch 46/150\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.1568 - accuracy: 0.9495\n","Epoch 46: val_accuracy improved from 0.92049 to 0.92130, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1580 - accuracy: 0.9494 - val_loss: 0.2252 - val_accuracy: 0.9213\n","Epoch 47/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1564 - accuracy: 0.9505\n","Epoch 47: val_accuracy did not improve from 0.92130\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1564 - accuracy: 0.9505 - val_loss: 0.2241 - val_accuracy: 0.9209\n","Epoch 48/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1574 - accuracy: 0.9499\n","Epoch 48: val_accuracy did not improve from 0.92130\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1568 - accuracy: 0.9507 - val_loss: 0.2269 - val_accuracy: 0.9198\n","Epoch 49/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1525 - accuracy: 0.9517\n","Epoch 49: val_accuracy did not improve from 0.92130\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1548 - accuracy: 0.9512 - val_loss: 0.2245 - val_accuracy: 0.9206\n","Epoch 50/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1563 - accuracy: 0.9502\n","Epoch 50: val_accuracy did not improve from 0.92130\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1551 - accuracy: 0.9507 - val_loss: 0.2253 - val_accuracy: 0.9208\n","Epoch 51/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9512\n","Epoch 51: val_accuracy did not improve from 0.92130\n","70/70 [==============================] - 1s 8ms/step - loss: 0.1542 - accuracy: 0.9512 - val_loss: 0.2257 - val_accuracy: 0.9195\n","Epoch 52/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1542 - accuracy: 0.9502\n","Epoch 52: val_accuracy did not improve from 0.92130\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1542 - accuracy: 0.9506 - val_loss: 0.2236 - val_accuracy: 0.9206\n","Epoch 53/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1536 - accuracy: 0.9507\n","Epoch 53: val_accuracy did not improve from 0.92130\n","70/70 [==============================] - 1s 11ms/step - loss: 0.1546 - accuracy: 0.9504 - val_loss: 0.2255 - val_accuracy: 0.9188\n","Epoch 54/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9509\n","Epoch 54: val_accuracy did not improve from 0.92130\n","70/70 [==============================] - 1s 14ms/step - loss: 0.1538 - accuracy: 0.9509 - val_loss: 0.2228 - val_accuracy: 0.9198\n","Epoch 55/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1525 - accuracy: 0.9514\n","Epoch 55: val_accuracy did not improve from 0.92130\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1520 - accuracy: 0.9518 - val_loss: 0.2221 - val_accuracy: 0.9202\n","Epoch 56/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1515 - accuracy: 0.9515\n","Epoch 56: val_accuracy did not improve from 0.92130\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1513 - accuracy: 0.9519 - val_loss: 0.2234 - val_accuracy: 0.9203\n","Epoch 57/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1505 - accuracy: 0.9522\n","Epoch 57: val_accuracy improved from 0.92130 to 0.92211, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1502 - accuracy: 0.9524 - val_loss: 0.2201 - val_accuracy: 0.9221\n","Epoch 58/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1496 - accuracy: 0.9527\n","Epoch 58: val_accuracy improved from 0.92211 to 0.92239, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1512 - accuracy: 0.9517 - val_loss: 0.2208 - val_accuracy: 0.9224\n","Epoch 59/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1511 - accuracy: 0.9519\n","Epoch 59: val_accuracy did not improve from 0.92239\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1507 - accuracy: 0.9521 - val_loss: 0.2204 - val_accuracy: 0.9222\n","Epoch 60/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1505 - accuracy: 0.9524\n","Epoch 60: val_accuracy did not improve from 0.92239\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1493 - accuracy: 0.9528 - val_loss: 0.2206 - val_accuracy: 0.9218\n","Epoch 61/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1480 - accuracy: 0.9527\n","Epoch 61: val_accuracy did not improve from 0.92239\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1488 - accuracy: 0.9526 - val_loss: 0.2220 - val_accuracy: 0.9200\n","Epoch 62/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1501 - accuracy: 0.9524\n","Epoch 62: val_accuracy did not improve from 0.92239\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1492 - accuracy: 0.9527 - val_loss: 0.2194 - val_accuracy: 0.9221\n","Epoch 63/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1506 - accuracy: 0.9522\n","Epoch 63: val_accuracy improved from 0.92239 to 0.92308, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1470 - accuracy: 0.9537 - val_loss: 0.2189 - val_accuracy: 0.9231\n","Epoch 64/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1476 - accuracy: 0.9533\n","Epoch 64: val_accuracy did not improve from 0.92308\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1468 - accuracy: 0.9533 - val_loss: 0.2191 - val_accuracy: 0.9230\n","Epoch 65/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1456 - accuracy: 0.9538\n","Epoch 65: val_accuracy did not improve from 0.92308\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1459 - accuracy: 0.9538 - val_loss: 0.2191 - val_accuracy: 0.9225\n","Epoch 66/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1460 - accuracy: 0.9537\n","Epoch 66: val_accuracy did not improve from 0.92308\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1478 - accuracy: 0.9528 - val_loss: 0.2303 - val_accuracy: 0.9209\n","Epoch 67/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1508 - accuracy: 0.9512\n","Epoch 67: val_accuracy did not improve from 0.92308\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1505 - accuracy: 0.9515 - val_loss: 0.2199 - val_accuracy: 0.9218\n","Epoch 68/150\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.1477 - accuracy: 0.9527\n","Epoch 68: val_accuracy did not improve from 0.92308\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1454 - accuracy: 0.9534 - val_loss: 0.2194 - val_accuracy: 0.9222\n","Epoch 69/150\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.1431 - accuracy: 0.9545\n","Epoch 69: val_accuracy improved from 0.92308 to 0.92395, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1437 - accuracy: 0.9544 - val_loss: 0.2163 - val_accuracy: 0.9240\n","Epoch 70/150\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.1395 - accuracy: 0.9567\n","Epoch 70: val_accuracy improved from 0.92395 to 0.92402, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1427 - accuracy: 0.9548 - val_loss: 0.2169 - val_accuracy: 0.9240\n","Epoch 71/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1415 - accuracy: 0.9556\n","Epoch 71: val_accuracy improved from 0.92402 to 0.92429, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1420 - accuracy: 0.9551 - val_loss: 0.2156 - val_accuracy: 0.9243\n","Epoch 72/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1418 - accuracy: 0.9547\n","Epoch 72: val_accuracy improved from 0.92429 to 0.92611, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1416 - accuracy: 0.9548 - val_loss: 0.2098 - val_accuracy: 0.9261\n","Epoch 73/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1433 - accuracy: 0.9540\n","Epoch 73: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1436 - accuracy: 0.9538 - val_loss: 0.2163 - val_accuracy: 0.9237\n","Epoch 74/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9551\n","Epoch 74: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1413 - accuracy: 0.9549 - val_loss: 0.2124 - val_accuracy: 0.9256\n","Epoch 75/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1409 - accuracy: 0.9555\n","Epoch 75: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1403 - accuracy: 0.9555 - val_loss: 0.2149 - val_accuracy: 0.9235\n","Epoch 76/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1382 - accuracy: 0.9563\n","Epoch 76: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1396 - accuracy: 0.9559 - val_loss: 0.2147 - val_accuracy: 0.9235\n","Epoch 77/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1401 - accuracy: 0.9552\n","Epoch 77: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1409 - accuracy: 0.9547 - val_loss: 0.2124 - val_accuracy: 0.9251\n","Epoch 78/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1396 - accuracy: 0.9561\n","Epoch 78: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1391 - accuracy: 0.9562 - val_loss: 0.2136 - val_accuracy: 0.9243\n","Epoch 79/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9562\n","Epoch 79: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1379 - accuracy: 0.9566 - val_loss: 0.2154 - val_accuracy: 0.9241\n","Epoch 80/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1372 - accuracy: 0.9569\n","Epoch 80: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1376 - accuracy: 0.9565 - val_loss: 0.2136 - val_accuracy: 0.9238\n","Epoch 81/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1373 - accuracy: 0.9570\n","Epoch 81: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1367 - accuracy: 0.9572 - val_loss: 0.2115 - val_accuracy: 0.9252\n","Epoch 82/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1367 - accuracy: 0.9560\n","Epoch 82: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1379 - accuracy: 0.9559 - val_loss: 0.2124 - val_accuracy: 0.9254\n","Epoch 83/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1354 - accuracy: 0.9575\n","Epoch 83: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1363 - accuracy: 0.9568 - val_loss: 0.2132 - val_accuracy: 0.9251\n","Epoch 84/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1343 - accuracy: 0.9576\n","Epoch 84: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1358 - accuracy: 0.9570 - val_loss: 0.2111 - val_accuracy: 0.9249\n","Epoch 85/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1375 - accuracy: 0.9558\n","Epoch 85: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1371 - accuracy: 0.9560 - val_loss: 0.2257 - val_accuracy: 0.9204\n","Epoch 86/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1438 - accuracy: 0.9538\n","Epoch 86: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1424 - accuracy: 0.9543 - val_loss: 0.2117 - val_accuracy: 0.9246\n","Epoch 87/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1358 - accuracy: 0.9571\n","Epoch 87: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1352 - accuracy: 0.9574 - val_loss: 0.2111 - val_accuracy: 0.9246\n","Epoch 88/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1374 - accuracy: 0.9564\n","Epoch 88: val_accuracy did not improve from 0.92611\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1362 - accuracy: 0.9571 - val_loss: 0.2088 - val_accuracy: 0.9260\n","Epoch 89/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1339 - accuracy: 0.9575\n","Epoch 89: val_accuracy improved from 0.92611 to 0.92633, saving model to best_model.h5\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1338 - accuracy: 0.9575 - val_loss: 0.2079 - val_accuracy: 0.9263\n","Epoch 90/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1317 - accuracy: 0.9590\n","Epoch 90: val_accuracy improved from 0.92633 to 0.92665, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1324 - accuracy: 0.9583 - val_loss: 0.2069 - val_accuracy: 0.9267\n","Epoch 91/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1331 - accuracy: 0.9578\n","Epoch 91: val_accuracy did not improve from 0.92665\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1326 - accuracy: 0.9580 - val_loss: 0.2088 - val_accuracy: 0.9259\n","Epoch 92/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1314 - accuracy: 0.9584\n","Epoch 92: val_accuracy did not improve from 0.92665\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1310 - accuracy: 0.9586 - val_loss: 0.2095 - val_accuracy: 0.9262\n","Epoch 93/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1304 - accuracy: 0.9590\n","Epoch 93: val_accuracy improved from 0.92665 to 0.92674, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1307 - accuracy: 0.9586 - val_loss: 0.2063 - val_accuracy: 0.9267\n","Epoch 94/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1296 - accuracy: 0.9592\n","Epoch 94: val_accuracy did not improve from 0.92674\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1308 - accuracy: 0.9585 - val_loss: 0.2101 - val_accuracy: 0.9265\n","Epoch 95/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1314 - accuracy: 0.9583\n","Epoch 95: val_accuracy improved from 0.92674 to 0.92799, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1308 - accuracy: 0.9585 - val_loss: 0.2048 - val_accuracy: 0.9280\n","Epoch 96/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1298 - accuracy: 0.9586\n","Epoch 96: val_accuracy improved from 0.92799 to 0.92800, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1296 - accuracy: 0.9588 - val_loss: 0.2046 - val_accuracy: 0.9280\n","Epoch 97/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1285 - accuracy: 0.9591\n","Epoch 97: val_accuracy did not improve from 0.92800\n","70/70 [==============================] - 1s 11ms/step - loss: 0.1285 - accuracy: 0.9596 - val_loss: 0.2050 - val_accuracy: 0.9279\n","Epoch 98/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9596\n","Epoch 98: val_accuracy did not improve from 0.92800\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1283 - accuracy: 0.9596 - val_loss: 0.2084 - val_accuracy: 0.9266\n","Epoch 99/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9574\n","Epoch 99: val_accuracy did not improve from 0.92800\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1311 - accuracy: 0.9577 - val_loss: 0.2115 - val_accuracy: 0.9237\n","Epoch 100/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1310 - accuracy: 0.9577\n","Epoch 100: val_accuracy did not improve from 0.92800\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1310 - accuracy: 0.9577 - val_loss: 0.2107 - val_accuracy: 0.9256\n","Epoch 101/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1362 - accuracy: 0.9558\n","Epoch 101: val_accuracy did not improve from 0.92800\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1367 - accuracy: 0.9558 - val_loss: 0.2061 - val_accuracy: 0.9280\n","Epoch 102/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1289 - accuracy: 0.9596\n","Epoch 102: val_accuracy did not improve from 0.92800\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1300 - accuracy: 0.9585 - val_loss: 0.2086 - val_accuracy: 0.9274\n","Epoch 103/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1290 - accuracy: 0.9587\n","Epoch 103: val_accuracy did not improve from 0.92800\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1293 - accuracy: 0.9583 - val_loss: 0.2084 - val_accuracy: 0.9265\n","Epoch 104/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9593\n","Epoch 104: val_accuracy improved from 0.92800 to 0.92873, saving model to best_model.h5\n","70/70 [==============================] - 1s 11ms/step - loss: 0.1279 - accuracy: 0.9591 - val_loss: 0.2049 - val_accuracy: 0.9287\n","Epoch 105/150\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.1237 - accuracy: 0.9604\n","Epoch 105: val_accuracy did not improve from 0.92873\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1258 - accuracy: 0.9601 - val_loss: 0.2066 - val_accuracy: 0.9285\n","Epoch 106/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9604\n","Epoch 106: val_accuracy did not improve from 0.92873\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1252 - accuracy: 0.9603 - val_loss: 0.2070 - val_accuracy: 0.9281\n","Epoch 107/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9601\n","Epoch 107: val_accuracy improved from 0.92873 to 0.92939, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1245 - accuracy: 0.9602 - val_loss: 0.2034 - val_accuracy: 0.9294\n","Epoch 108/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1243 - accuracy: 0.9603\n","Epoch 108: val_accuracy improved from 0.92939 to 0.92977, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1238 - accuracy: 0.9606 - val_loss: 0.2008 - val_accuracy: 0.9298\n","Epoch 109/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9616\n","Epoch 109: val_accuracy improved from 0.92977 to 0.93026, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1233 - accuracy: 0.9609 - val_loss: 0.2006 - val_accuracy: 0.9303\n","Epoch 110/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9607\n","Epoch 110: val_accuracy did not improve from 0.93026\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1233 - accuracy: 0.9607 - val_loss: 0.2024 - val_accuracy: 0.9287\n","Epoch 111/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9616\n","Epoch 111: val_accuracy did not improve from 0.93026\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1233 - accuracy: 0.9609 - val_loss: 0.2022 - val_accuracy: 0.9290\n","Epoch 112/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9609\n","Epoch 112: val_accuracy did not improve from 0.93026\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1227 - accuracy: 0.9609 - val_loss: 0.2022 - val_accuracy: 0.9293\n","Epoch 113/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1201 - accuracy: 0.9615\n","Epoch 113: val_accuracy did not improve from 0.93026\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1218 - accuracy: 0.9611 - val_loss: 0.2026 - val_accuracy: 0.9293\n","Epoch 114/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9619\n","Epoch 114: val_accuracy did not improve from 0.93026\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1214 - accuracy: 0.9613 - val_loss: 0.2005 - val_accuracy: 0.9299\n","Epoch 115/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9623\n","Epoch 115: val_accuracy improved from 0.93026 to 0.93056, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1205 - accuracy: 0.9620 - val_loss: 0.1983 - val_accuracy: 0.9306\n","Epoch 116/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1159 - accuracy: 0.9636\n","Epoch 116: val_accuracy improved from 0.93056 to 0.93113, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1199 - accuracy: 0.9621 - val_loss: 0.1980 - val_accuracy: 0.9311\n","Epoch 117/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1205 - accuracy: 0.9617\n","Epoch 117: val_accuracy improved from 0.93113 to 0.93163, saving model to best_model.h5\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1198 - accuracy: 0.9620 - val_loss: 0.1964 - val_accuracy: 0.9316\n","Epoch 118/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9618\n","Epoch 118: val_accuracy improved from 0.93163 to 0.93204, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1197 - accuracy: 0.9617 - val_loss: 0.1954 - val_accuracy: 0.9320\n","Epoch 119/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9617\n","Epoch 119: val_accuracy did not improve from 0.93204\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1198 - accuracy: 0.9617 - val_loss: 0.2010 - val_accuracy: 0.9300\n","Epoch 120/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1199 - accuracy: 0.9617\n","Epoch 120: val_accuracy did not improve from 0.93204\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1199 - accuracy: 0.9617 - val_loss: 0.1990 - val_accuracy: 0.9307\n","Epoch 121/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9625\n","Epoch 121: val_accuracy did not improve from 0.93204\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1185 - accuracy: 0.9626 - val_loss: 0.1963 - val_accuracy: 0.9315\n","Epoch 122/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9627\n","Epoch 122: val_accuracy did not improve from 0.93204\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1179 - accuracy: 0.9628 - val_loss: 0.1981 - val_accuracy: 0.9297\n","Epoch 123/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1198 - accuracy: 0.9614\n","Epoch 123: val_accuracy did not improve from 0.93204\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1177 - accuracy: 0.9625 - val_loss: 0.1965 - val_accuracy: 0.9315\n","Epoch 124/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9631\n","Epoch 124: val_accuracy did not improve from 0.93204\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1171 - accuracy: 0.9629 - val_loss: 0.1926 - val_accuracy: 0.9317\n","Epoch 125/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9615\n","Epoch 125: val_accuracy did not improve from 0.93204\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1216 - accuracy: 0.9615 - val_loss: 0.1940 - val_accuracy: 0.9319\n","Epoch 126/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9600\n","Epoch 126: val_accuracy did not improve from 0.93204\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1278 - accuracy: 0.9599 - val_loss: 0.1949 - val_accuracy: 0.9316\n","Epoch 127/150\n","65/70 [==========================\u003e...] - ETA: 0s - loss: 0.1199 - accuracy: 0.9618\n","Epoch 127: val_accuracy did not improve from 0.93204\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1203 - accuracy: 0.9619 - val_loss: 0.1940 - val_accuracy: 0.9317\n","Epoch 128/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1167 - accuracy: 0.9630\n","Epoch 128: val_accuracy did not improve from 0.93204\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1195 - accuracy: 0.9622 - val_loss: 0.1932 - val_accuracy: 0.9300\n","Epoch 129/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1197 - accuracy: 0.9622\n","Epoch 129: val_accuracy did not improve from 0.93204\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1198 - accuracy: 0.9619 - val_loss: 0.1910 - val_accuracy: 0.9317\n","Epoch 130/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1177 - accuracy: 0.9619\n","Epoch 130: val_accuracy improved from 0.93204 to 0.93343, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1187 - accuracy: 0.9618 - val_loss: 0.1890 - val_accuracy: 0.9334\n","Epoch 131/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1199 - accuracy: 0.9615\n","Epoch 131: val_accuracy did not improve from 0.93343\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1185 - accuracy: 0.9622 - val_loss: 0.1927 - val_accuracy: 0.9315\n","Epoch 132/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1174 - accuracy: 0.9625\n","Epoch 132: val_accuracy improved from 0.93343 to 0.93430, saving model to best_model.h5\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1166 - accuracy: 0.9628 - val_loss: 0.1871 - val_accuracy: 0.9343\n","Epoch 133/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1168 - accuracy: 0.9626\n","Epoch 133: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 13ms/step - loss: 0.1160 - accuracy: 0.9631 - val_loss: 0.1909 - val_accuracy: 0.9328\n","Epoch 134/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9629\n","Epoch 134: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1156 - accuracy: 0.9632 - val_loss: 0.1908 - val_accuracy: 0.9331\n","Epoch 135/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1213 - accuracy: 0.9605\n","Epoch 135: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1226 - accuracy: 0.9597 - val_loss: 0.2004 - val_accuracy: 0.9291\n","Epoch 136/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1189 - accuracy: 0.9614\n","Epoch 136: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1183 - accuracy: 0.9620 - val_loss: 0.1943 - val_accuracy: 0.9325\n","Epoch 137/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9632\n","Epoch 137: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 12ms/step - loss: 0.1156 - accuracy: 0.9632 - val_loss: 0.1943 - val_accuracy: 0.9319\n","Epoch 138/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1163 - accuracy: 0.9629\n","Epoch 138: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1164 - accuracy: 0.9628 - val_loss: 0.1946 - val_accuracy: 0.9324\n","Epoch 139/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9636\n","Epoch 139: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1139 - accuracy: 0.9639 - val_loss: 0.1921 - val_accuracy: 0.9329\n","Epoch 140/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1145 - accuracy: 0.9633\n","Epoch 140: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1137 - accuracy: 0.9635 - val_loss: 0.1948 - val_accuracy: 0.9316\n","Epoch 141/150\n","63/70 [==========================\u003e...] - ETA: 0s - loss: 0.1114 - accuracy: 0.9646\n","Epoch 141: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1139 - accuracy: 0.9634 - val_loss: 0.1963 - val_accuracy: 0.9310\n","Epoch 142/150\n","70/70 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9643\n","Epoch 142: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1123 - accuracy: 0.9643 - val_loss: 0.1944 - val_accuracy: 0.9320\n","Epoch 143/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1125 - accuracy: 0.9642\n","Epoch 143: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1121 - accuracy: 0.9643 - val_loss: 0.1924 - val_accuracy: 0.9323\n","Epoch 144/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1099 - accuracy: 0.9654\n","Epoch 144: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1112 - accuracy: 0.9649 - val_loss: 0.1941 - val_accuracy: 0.9319\n","Epoch 145/150\n","66/70 [===========================\u003e..] - ETA: 0s - loss: 0.1077 - accuracy: 0.9656\n","Epoch 145: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1109 - accuracy: 0.9647 - val_loss: 0.1938 - val_accuracy: 0.9322\n","Epoch 146/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1110 - accuracy: 0.9654\n","Epoch 146: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1106 - accuracy: 0.9650 - val_loss: 0.1924 - val_accuracy: 0.9328\n","Epoch 147/150\n","64/70 [==========================\u003e...] - ETA: 0s - loss: 0.1097 - accuracy: 0.9656\n","Epoch 147: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1104 - accuracy: 0.9651 - val_loss: 0.1924 - val_accuracy: 0.9329\n","Epoch 148/150\n","69/70 [============================\u003e.] - ETA: 0s - loss: 0.1163 - accuracy: 0.9630\n","Epoch 148: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 10ms/step - loss: 0.1172 - accuracy: 0.9623 - val_loss: 0.1936 - val_accuracy: 0.9298\n","Epoch 149/150\n","67/70 [===========================\u003e..] - ETA: 0s - loss: 0.1176 - accuracy: 0.9622\n","Epoch 149: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1179 - accuracy: 0.9620 - val_loss: 0.1891 - val_accuracy: 0.9331\n","Epoch 150/150\n","68/70 [============================\u003e.] - ETA: 0s - loss: 0.1162 - accuracy: 0.9625\n","Epoch 150: val_accuracy did not improve from 0.93430\n","70/70 [==============================] - 1s 9ms/step - loss: 0.1161 - accuracy: 0.9625 - val_loss: 0.1933 - val_accuracy: 0.9313\n"]}],"source":["history = model.fit(X_train, y_train, validation_split=0.2, epochs=150, callbacks=[checkpoint_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2ntq1x55AOe6"},"outputs":[{"name":"stdout","output_type":"stream","text":["22/22 [==============================] - 0s 5ms/step - loss: 0.1799 - accuracy: 0.9392\n","22/22 [==============================] - 0s 4ms/step\n"]}],"source":["results = model.evaluate(X_test, y_test)\n","y_pred = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SV1hRuEiAOe9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 2  2  2 ...  2  2  2]\n"," [ 2 11 11 ... 11 11 11]\n"," [ 7  7  7 ...  7  7  7]\n"," ...\n"," [ 9  9  9 ...  9  9  9]\n"," [ 7 13 13 ... 13 13 13]\n"," [ 3  3  0 ...  0  0  0]]\n","Predictions    : [1 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1]\n","Appeared times : [79 73 69 65 64 63 63 63 63 63 63 64 63 64 63 64 64 64 64 64 63 63 64 64\n"," 64 63 63 63 63 64 64 63 63 64 63 63 64 63 63 63 62 62 63 62 61 63 62 62\n"," 63 63 62 62 63 63 62 62 63 63 63 63 62 62 61 61 62 61 61 61 61 61 61 61\n"," 60 59 59 59 59 60 60 60 60 60 60 60 60 60 60 60 60 61 60 61 60 60 60 60\n"," 60 60 60 60 60 60 60 60 60 60 61 61 61 60 60 60 60 60 60 60 60 60 61 61\n"," 61 60 60 60 60 60 60 60 60 60 60 60 60 60 61 60 60 60 61 60 60 60 61 61\n"," 60 60 60 60 60 60]\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-11-f892e764b43a\u003e:3: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n","  major = np.squeeze(mode(predictions)[0])\n","\u003cipython-input-11-f892e764b43a\u003e:4: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n","  count = np.squeeze(mode(predictions)[1])\n"]}],"source":["predictions = np.argmax(y_pred, axis=2)\n","print(predictions)\n","major = np.squeeze(mode(predictions)[0])\n","count = np.squeeze(mode(predictions)[1])\n","print(f\"Predictions    : {major}\")\n","print(f\"Appeared times : {count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Sfuw3hzLAOe9"},"outputs":[{"name":"stdout","output_type":"stream","text":["['deaf', 'good', 'good', 'good', 'good', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf', 'deaf']\n"]}],"source":["def get_key(dictionary, val):\n","    for key, value in dictionary.items():\n","        if val == value:\n","            return key\n","\n","predictions_list = []\n","for word in major:\n","    predictions_list.append(get_key(label_dict, word))\n","print(predictions_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iXxxwediAOe-"},"outputs":[{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 1000x1000 with 2 Axes\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy: 93.92%\n"]}],"source":["cm = confusion_matrix(y_test.argmax(axis=2).flatten(), y_pred.argmax(axis=2).flatten())\n","plt.subplots(figsize=(10, 10))\n","sns.heatmap(cm, cmap='Blues', xticklabels=label_dict.keys(), yticklabels=label_dict.keys())\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n","print(f\"Accuracy: {accuracy*100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xRp01q6kAOe-"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.90      0.78      0.84      8550\n","           1       0.97      0.99      0.98      9000\n","           2       0.94      0.99      0.96      6300\n","           3       0.72      0.80      0.76      6300\n","           4       0.99      0.95      0.97      7650\n","           5       0.98      0.97      0.97      7800\n","           6       0.98      0.99      0.99      7800\n","           7       0.94      0.91      0.92      7800\n","           8       1.00      0.99      0.99      7650\n","           9       0.99      0.99      0.99      7050\n","          10       0.99      0.99      0.99      8400\n","          11       0.84      0.87      0.85      6900\n","          12       0.97      0.99      0.98      6150\n","          13       0.93      0.93      0.93      7650\n","\n","    accuracy                           0.94    105000\n","   macro avg       0.94      0.94      0.94    105000\n","weighted avg       0.94      0.94      0.94    105000\n","\n"]}],"source":["print(classification_report(y_test.argmax(axis=2).flatten(), y_pred.argmax(axis=2).flatten()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uEXwg0wSAOe5"},"outputs":[],"source":["# model.save('/content/drive/MyDrive/University/MemoireAbdelmalek/models/words.h5')\n","model.save('words.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yKyYWutaSyuz"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"]}],"source":["run_model = tf.function(lambda x: model(x))\n","BATCH_SIZE = 1\n","INPUT_SIZE = 150\n","STEPS = 11\n","concrete_func = run_model.get_concrete_function(\n","    tf.TensorSpec([BATCH_SIZE, INPUT_SIZE, STEPS], model.inputs[0].dtype))\n","\n","MODEL_DIR = \"./\"\n","model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n","\n","converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","converter._experimental_lower_tensor_list_ops = False\n","converter.allow_custom_ops = False\n","tflite_model = converter.convert()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DDJk_WzN8evO"},"outputs":[],"source":["# with open('/content/drive/MyDrive/University/MemoireAbdelmalek/models/words.tflite', 'wb') as f:\n","#     f.write(tflite_model)\n","with open('words.tflite', 'wb') as f:\n","    f.write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ExD_R1p1C-mF"},"outputs":[{"name":"stdout","output_type":"stream","text":["characters.cc  characters.tflite  words.h5\n","characters.h5  words.cc\t\t  words.tflite\n"]}],"source":["!xxd -i words.tflite \u003e words.cc\n","# !xxd -i words.tflite drive/MyDrive/University/MemoireAbdelmalek/models/words.cc\n","!ls drive/MyDrive/University/MemoireAbdelmalek/models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YuWvie7atYqQ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.11.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}}},"nbformat":4,"nbformat_minor":0}