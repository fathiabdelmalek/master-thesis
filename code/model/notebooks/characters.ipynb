{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22416,"status":"ok","timestamp":1686347449771,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"KzwHZnuoParA","outputId":"3c48ff2a-d07a-4fca-984f-28f329ddeebc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5024,"status":"ok","timestamp":1686347504594,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"IjZnYmv-PaBr"},"outputs":[],"source":["from scipy.stats import mode\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","from tensorflow import keras\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","from keras.utils import to_categorical\n","from keras.optimizers import Adam\n","from keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3877,"status":"ok","timestamp":1686347508443,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"pVHMJyn7PaBy","outputId":"3c51a2b4-3843-4f1b-fa9a-f2cb633da67d"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n","(6500, 150, 11) (6500, 150, 26)\n"]}],"source":["df = pd.read_csv('/content/drive/MyDrive/University/MemoireAbdelmalek/data/all/characters.csv')\n","X = df[['flex_1', 'flex_2', 'flex_3', 'flex_4', 'flex_5', 'GYRx', 'GYRy', 'GYRz', 'ACCx', 'ACCy', 'ACCz']].values\n","labels = df.iloc[:, -1]\n","label_dict = {label: i for i, label in enumerate(sorted(set(labels)))}\n","y = np.array([label_dict[label] for label in labels])\n","y = to_categorical(y, num_classes=len(label_dict))\n","\n","print(label_dict)\n","\n","X = np.reshape(X, (X.shape[0]//150, 150, X.shape[1]))\n","y = np.reshape(y, (y.shape[0]//150, 150, y.shape[1]))\n","print(X.shape, y.shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1686347508444,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"jEhtZu0WPaBz","outputId":"d169da7c-f8bb-4588-da20-25d353e3dcbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["(5200, 150, 11) (1300, 150, 11)\n"]}],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n","print(X_train.shape, X_test.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1686347508445,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"tWrs0DaioyVl"},"outputs":[],"source":["checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","    filepath='best_model.h5',\n","    monitor='val_accuracy',\n","    save_best_only=True,\n","    save_weights_only=False,\n","    mode='max',\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4600,"status":"ok","timestamp":1686347513039,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"PgeqnFxePaB0"},"outputs":[],"source":["input_layer = Input(shape=(150, 11), name='input_layer')\n","lstm_layer = LSTM(units=38, return_sequences=True, name='lstm_layer')(input_layer)\n","output_layer = Dense(units=len(label_dict), activation='softmax', name='output_layer')(lstm_layer)\n","model = Model(inputs=input_layer, outputs=output_layer)\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss=CategoricalCrossentropy(), metrics=['accuracy'])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":369025,"status":"ok","timestamp":1686347882039,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"JAEh-TL6QBXK","outputId":"c7bfcfed-c9b3-42d1-e4b7-b7bb05c6c5bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 1\n","Epoch 1/50\n","125/130 [===========================>..] - ETA: 0s - loss: 3.3596 - accuracy: 0.0663\n","Epoch 1: val_accuracy improved from -inf to 0.07467, saving model to best_model.h5\n","130/130 [==============================] - 8s 16ms/step - loss: 3.3541 - accuracy: 0.0681 - val_loss: 3.2759 - val_accuracy: 0.0747\n","Epoch 2/50\n","124/130 [===========================>..] - ETA: 0s - loss: 3.2064 - accuracy: 0.0991\n","Epoch 2: val_accuracy improved from 0.07467 to 0.10971, saving model to best_model.h5\n","130/130 [==============================] - 2s 12ms/step - loss: 3.2076 - accuracy: 0.0971 - val_loss: 3.1499 - val_accuracy: 0.1097\n","Epoch 3/50\n","125/130 [===========================>..] - ETA: 0s - loss: 3.0843 - accuracy: 0.1316\n","Epoch 3: val_accuracy improved from 0.10971 to 0.14564, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 3.0850 - accuracy: 0.1321 - val_loss: 3.0388 - val_accuracy: 0.1456\n","Epoch 4/50\n","130/130 [==============================] - ETA: 0s - loss: 2.9756 - accuracy: 0.1724\n","Epoch 4: val_accuracy improved from 0.14564 to 0.19078, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 2.9756 - accuracy: 0.1724 - val_loss: 2.9409 - val_accuracy: 0.1908\n","Epoch 5/50\n","124/130 [===========================>..] - ETA: 0s - loss: 2.8767 - accuracy: 0.2196\n","Epoch 5: val_accuracy improved from 0.19078 to 0.23329, saving model to best_model.h5\n","130/130 [==============================] - 1s 7ms/step - loss: 2.8751 - accuracy: 0.2200 - val_loss: 2.8457 - val_accuracy: 0.2333\n","Epoch 6/50\n","123/130 [===========================>..] - ETA: 0s - loss: 2.7808 - accuracy: 0.2705\n","Epoch 6: val_accuracy improved from 0.23329 to 0.28015, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 2.7810 - accuracy: 0.2703 - val_loss: 2.7540 - val_accuracy: 0.2801\n","Epoch 7/50\n","123/130 [===========================>..] - ETA: 0s - loss: 2.6940 - accuracy: 0.3074\n","Epoch 7: val_accuracy improved from 0.28015 to 0.31498, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 2.6912 - accuracy: 0.3101 - val_loss: 2.6672 - val_accuracy: 0.3150\n","Epoch 8/50\n","130/130 [==============================] - ETA: 0s - loss: 2.6040 - accuracy: 0.3430\n","Epoch 8: val_accuracy improved from 0.31498 to 0.34840, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 2.6040 - accuracy: 0.3430 - val_loss: 2.5815 - val_accuracy: 0.3484\n","Epoch 9/50\n","122/130 [===========================>..] - ETA: 0s - loss: 2.5212 - accuracy: 0.3738\n","Epoch 9: val_accuracy improved from 0.34840 to 0.38216, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 2.5184 - accuracy: 0.3748 - val_loss: 2.5004 - val_accuracy: 0.3822\n","Epoch 10/50\n","122/130 [===========================>..] - ETA: 0s - loss: 2.4323 - accuracy: 0.4037\n","Epoch 10: val_accuracy improved from 0.38216 to 0.40112, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 2.4307 - accuracy: 0.4035 - val_loss: 2.4129 - val_accuracy: 0.4011\n","Epoch 11/50\n","128/130 [============================>.] - ETA: 0s - loss: 2.3457 - accuracy: 0.4327\n","Epoch 11: val_accuracy improved from 0.40112 to 0.42951, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 2.3453 - accuracy: 0.4326 - val_loss: 2.3282 - val_accuracy: 0.4295\n","Epoch 12/50\n","122/130 [===========================>..] - ETA: 0s - loss: 2.2664 - accuracy: 0.4643\n","Epoch 12: val_accuracy improved from 0.42951 to 0.45481, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 2.2652 - accuracy: 0.4655 - val_loss: 2.2520 - val_accuracy: 0.4548\n","Epoch 13/50\n","129/130 [============================>.] - ETA: 0s - loss: 2.1888 - accuracy: 0.4890\n","Epoch 13: val_accuracy improved from 0.45481 to 0.47092, saving model to best_model.h5\n","130/130 [==============================] - 1s 10ms/step - loss: 2.1898 - accuracy: 0.4880 - val_loss: 2.1784 - val_accuracy: 0.4709\n","Epoch 14/50\n","124/130 [===========================>..] - ETA: 0s - loss: 2.1199 - accuracy: 0.5020\n","Epoch 14: val_accuracy improved from 0.47092 to 0.48706, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 2.1152 - accuracy: 0.5053 - val_loss: 2.1042 - val_accuracy: 0.4871\n","Epoch 15/50\n","129/130 [============================>.] - ETA: 0s - loss: 2.0461 - accuracy: 0.5195\n","Epoch 15: val_accuracy improved from 0.48706 to 0.49955, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 2.0454 - accuracy: 0.5195 - val_loss: 2.0372 - val_accuracy: 0.4996\n","Epoch 16/50\n","122/130 [===========================>..] - ETA: 0s - loss: 1.9857 - accuracy: 0.5306\n","Epoch 16: val_accuracy improved from 0.49955 to 0.50728, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.9817 - accuracy: 0.5320 - val_loss: 1.9768 - val_accuracy: 0.5073\n","Epoch 17/50\n","129/130 [============================>.] - ETA: 0s - loss: 1.9204 - accuracy: 0.5426\n","Epoch 17: val_accuracy improved from 0.50728 to 0.51180, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.9203 - accuracy: 0.5429 - val_loss: 1.9207 - val_accuracy: 0.5118\n","Epoch 18/50\n","125/130 [===========================>..] - ETA: 0s - loss: 1.8621 - accuracy: 0.5499\n","Epoch 18: val_accuracy improved from 0.51180 to 0.52294, saving model to best_model.h5\n","130/130 [==============================] - 1s 7ms/step - loss: 1.8607 - accuracy: 0.5521 - val_loss: 1.8633 - val_accuracy: 0.5229\n","Epoch 19/50\n","124/130 [===========================>..] - ETA: 0s - loss: 1.8052 - accuracy: 0.5638\n","Epoch 19: val_accuracy improved from 0.52294 to 0.52968, saving model to best_model.h5\n","130/130 [==============================] - 1s 7ms/step - loss: 1.8041 - accuracy: 0.5626 - val_loss: 1.8100 - val_accuracy: 0.5297\n","Epoch 20/50\n","125/130 [===========================>..] - ETA: 0s - loss: 1.7475 - accuracy: 0.5762\n","Epoch 20: val_accuracy improved from 0.52968 to 0.53743, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 1.7517 - accuracy: 0.5737 - val_loss: 1.7635 - val_accuracy: 0.5374\n","Epoch 21/50\n","123/130 [===========================>..] - ETA: 0s - loss: 1.7059 - accuracy: 0.5802\n","Epoch 21: val_accuracy improved from 0.53743 to 0.54815, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.7049 - accuracy: 0.5829 - val_loss: 1.7164 - val_accuracy: 0.5481\n","Epoch 22/50\n","130/130 [==============================] - ETA: 0s - loss: 1.6607 - accuracy: 0.5921\n","Epoch 22: val_accuracy improved from 0.54815 to 0.56150, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.6607 - accuracy: 0.5921 - val_loss: 1.6732 - val_accuracy: 0.5615\n","Epoch 23/50\n","123/130 [===========================>..] - ETA: 0s - loss: 1.6188 - accuracy: 0.6018\n","Epoch 23: val_accuracy improved from 0.56150 to 0.57426, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.6165 - accuracy: 0.6028 - val_loss: 1.6310 - val_accuracy: 0.5743\n","Epoch 24/50\n","127/130 [============================>.] - ETA: 0s - loss: 1.5764 - accuracy: 0.6062\n","Epoch 24: val_accuracy improved from 0.57426 to 0.58167, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 1.5755 - accuracy: 0.6060 - val_loss: 1.5922 - val_accuracy: 0.5817\n","Epoch 25/50\n","129/130 [============================>.] - ETA: 0s - loss: 1.5343 - accuracy: 0.6171\n","Epoch 25: val_accuracy improved from 0.58167 to 0.59372, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.5355 - accuracy: 0.6166 - val_loss: 1.5497 - val_accuracy: 0.5937\n","Epoch 26/50\n","127/130 [============================>.] - ETA: 0s - loss: 1.4948 - accuracy: 0.6261\n","Epoch 26: val_accuracy improved from 0.59372 to 0.60281, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 1.4937 - accuracy: 0.6258 - val_loss: 1.5106 - val_accuracy: 0.6028\n","Epoch 27/50\n","125/130 [===========================>..] - ETA: 0s - loss: 1.4554 - accuracy: 0.6355\n","Epoch 27: val_accuracy improved from 0.60281 to 0.60613, saving model to best_model.h5\n","130/130 [==============================] - 2s 12ms/step - loss: 1.4584 - accuracy: 0.6338 - val_loss: 1.4852 - val_accuracy: 0.6061\n","Epoch 28/50\n","127/130 [============================>.] - ETA: 0s - loss: 1.4237 - accuracy: 0.6424\n","Epoch 28: val_accuracy improved from 0.60613 to 0.61831, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 1.4239 - accuracy: 0.6420 - val_loss: 1.4475 - val_accuracy: 0.6183\n","Epoch 29/50\n","124/130 [===========================>..] - ETA: 0s - loss: 1.3891 - accuracy: 0.6546\n","Epoch 29: val_accuracy improved from 0.61831 to 0.62714, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.3890 - accuracy: 0.6544 - val_loss: 1.4155 - val_accuracy: 0.6271\n","Epoch 30/50\n","124/130 [===========================>..] - ETA: 0s - loss: 1.3572 - accuracy: 0.6674\n","Epoch 30: val_accuracy improved from 0.62714 to 0.64637, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.3562 - accuracy: 0.6673 - val_loss: 1.3841 - val_accuracy: 0.6464\n","Epoch 31/50\n","124/130 [===========================>..] - ETA: 0s - loss: 1.3204 - accuracy: 0.6808\n","Epoch 31: val_accuracy improved from 0.64637 to 0.65452, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.3257 - accuracy: 0.6774 - val_loss: 1.3569 - val_accuracy: 0.6545\n","Epoch 32/50\n","123/130 [===========================>..] - ETA: 0s - loss: 1.2961 - accuracy: 0.6838\n","Epoch 32: val_accuracy improved from 0.65452 to 0.66293, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.2966 - accuracy: 0.6845 - val_loss: 1.3250 - val_accuracy: 0.6629\n","Epoch 33/50\n","123/130 [===========================>..] - ETA: 0s - loss: 1.2717 - accuracy: 0.6940\n","Epoch 33: val_accuracy improved from 0.66293 to 0.66734, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.2693 - accuracy: 0.6940 - val_loss: 1.3019 - val_accuracy: 0.6673\n","Epoch 34/50\n","130/130 [==============================] - ETA: 0s - loss: 1.2423 - accuracy: 0.6970\n","Epoch 34: val_accuracy improved from 0.66734 to 0.67858, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.2423 - accuracy: 0.6970 - val_loss: 1.2742 - val_accuracy: 0.6786\n","Epoch 35/50\n","130/130 [==============================] - ETA: 0s - loss: 1.2173 - accuracy: 0.7027\n","Epoch 35: val_accuracy improved from 0.67858 to 0.68764, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 1.2173 - accuracy: 0.7027 - val_loss: 1.2465 - val_accuracy: 0.6876\n","Epoch 36/50\n","130/130 [==============================] - ETA: 0s - loss: 1.1935 - accuracy: 0.7082\n","Epoch 36: val_accuracy did not improve from 0.68764\n","130/130 [==============================] - 1s 8ms/step - loss: 1.1935 - accuracy: 0.7082 - val_loss: 1.2262 - val_accuracy: 0.6874\n","Epoch 37/50\n","127/130 [============================>.] - ETA: 0s - loss: 1.1715 - accuracy: 0.7139\n","Epoch 37: val_accuracy improved from 0.68764 to 0.69454, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.1703 - accuracy: 0.7149 - val_loss: 1.2056 - val_accuracy: 0.6945\n","Epoch 38/50\n","128/130 [============================>.] - ETA: 0s - loss: 1.1442 - accuracy: 0.7250\n","Epoch 38: val_accuracy improved from 0.69454 to 0.70353, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 1.1466 - accuracy: 0.7241 - val_loss: 1.1822 - val_accuracy: 0.7035\n","Epoch 39/50\n","127/130 [============================>.] - ETA: 0s - loss: 1.1222 - accuracy: 0.7262\n","Epoch 39: val_accuracy did not improve from 0.70353\n","130/130 [==============================] - 1s 11ms/step - loss: 1.1227 - accuracy: 0.7263 - val_loss: 1.1602 - val_accuracy: 0.7033\n","Epoch 40/50\n","127/130 [============================>.] - ETA: 0s - loss: 1.1038 - accuracy: 0.7300\n","Epoch 40: val_accuracy improved from 0.70353 to 0.70831, saving model to best_model.h5\n","130/130 [==============================] - 1s 10ms/step - loss: 1.1022 - accuracy: 0.7301 - val_loss: 1.1410 - val_accuracy: 0.7083\n","Epoch 41/50\n","129/130 [============================>.] - ETA: 0s - loss: 1.0854 - accuracy: 0.7349\n","Epoch 41: val_accuracy did not improve from 0.70831\n","130/130 [==============================] - 1s 8ms/step - loss: 1.0855 - accuracy: 0.7339 - val_loss: 1.1254 - val_accuracy: 0.7065\n","Epoch 42/50\n","123/130 [===========================>..] - ETA: 0s - loss: 1.0686 - accuracy: 0.7382\n","Epoch 42: val_accuracy improved from 0.70831 to 0.71613, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.0657 - accuracy: 0.7384 - val_loss: 1.1031 - val_accuracy: 0.7161\n","Epoch 43/50\n","129/130 [============================>.] - ETA: 0s - loss: 1.0461 - accuracy: 0.7432\n","Epoch 43: val_accuracy improved from 0.71613 to 0.71714, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 1.0453 - accuracy: 0.7435 - val_loss: 1.0845 - val_accuracy: 0.7171\n","Epoch 44/50\n","125/130 [===========================>..] - ETA: 0s - loss: 1.0340 - accuracy: 0.7422\n","Epoch 44: val_accuracy did not improve from 0.71714\n","130/130 [==============================] - 1s 8ms/step - loss: 1.0307 - accuracy: 0.7434 - val_loss: 1.0797 - val_accuracy: 0.7147\n","Epoch 45/50\n","126/130 [============================>.] - ETA: 0s - loss: 1.0228 - accuracy: 0.7415\n","Epoch 45: val_accuracy improved from 0.71714 to 0.72147, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 1.0229 - accuracy: 0.7413 - val_loss: 1.0576 - val_accuracy: 0.7215\n","Epoch 46/50\n","126/130 [============================>.] - ETA: 0s - loss: 1.0000 - accuracy: 0.7472\n","Epoch 46: val_accuracy did not improve from 0.72147\n","130/130 [==============================] - 1s 8ms/step - loss: 1.0002 - accuracy: 0.7473 - val_loss: 1.0364 - val_accuracy: 0.7213\n","Epoch 47/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.9812 - accuracy: 0.7520\n","Epoch 47: val_accuracy improved from 0.72147 to 0.72746, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.9822 - accuracy: 0.7508 - val_loss: 1.0184 - val_accuracy: 0.7275\n","Epoch 48/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.9625 - accuracy: 0.7562\n","Epoch 48: val_accuracy improved from 0.72746 to 0.72971, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.9637 - accuracy: 0.7553 - val_loss: 1.0016 - val_accuracy: 0.7297\n","Epoch 49/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.9490 - accuracy: 0.7582\n","Epoch 49: val_accuracy improved from 0.72971 to 0.73128, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.9495 - accuracy: 0.7576 - val_loss: 0.9883 - val_accuracy: 0.7313\n","Epoch 50/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.9346 - accuracy: 0.7600\n","Epoch 50: val_accuracy improved from 0.73128 to 0.73646, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 0.9343 - accuracy: 0.7600 - val_loss: 0.9729 - val_accuracy: 0.7365\n","Fold 2\n","Epoch 1/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.9222 - accuracy: 0.7640\n","Epoch 1: val_accuracy improved from 0.73646 to 0.73995, saving model to best_model.h5\n","130/130 [==============================] - 2s 12ms/step - loss: 0.9206 - accuracy: 0.7638 - val_loss: 0.9595 - val_accuracy: 0.7399\n","Epoch 2/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.9033 - accuracy: 0.7672\n","Epoch 2: val_accuracy improved from 0.73995 to 0.74199, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.9065 - accuracy: 0.7653 - val_loss: 0.9460 - val_accuracy: 0.7420\n","Epoch 3/50\n","130/130 [==============================] - ETA: 0s - loss: 0.8938 - accuracy: 0.7677\n","Epoch 3: val_accuracy did not improve from 0.74199\n","130/130 [==============================] - 1s 8ms/step - loss: 0.8938 - accuracy: 0.7677 - val_loss: 0.9357 - val_accuracy: 0.7413\n","Epoch 4/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.8830 - accuracy: 0.7670\n","Epoch 4: val_accuracy did not improve from 0.74199\n","130/130 [==============================] - 1s 8ms/step - loss: 0.8809 - accuracy: 0.7679 - val_loss: 0.9218 - val_accuracy: 0.7419\n","Epoch 5/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.8726 - accuracy: 0.7649\n","Epoch 5: val_accuracy improved from 0.74199 to 0.74887, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.8721 - accuracy: 0.7644 - val_loss: 0.9061 - val_accuracy: 0.7489\n","Epoch 6/50\n","122/130 [===========================>..] - ETA: 0s - loss: 0.8519 - accuracy: 0.7739\n","Epoch 6: val_accuracy did not improve from 0.74887\n","130/130 [==============================] - 1s 9ms/step - loss: 0.8575 - accuracy: 0.7719 - val_loss: 0.8981 - val_accuracy: 0.7483\n","Epoch 7/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.8473 - accuracy: 0.7743\n","Epoch 7: val_accuracy improved from 0.74887 to 0.75311, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.8482 - accuracy: 0.7724 - val_loss: 0.8862 - val_accuracy: 0.7531\n","Epoch 8/50\n","130/130 [==============================] - ETA: 0s - loss: 0.8386 - accuracy: 0.7709\n","Epoch 8: val_accuracy did not improve from 0.75311\n","130/130 [==============================] - 1s 8ms/step - loss: 0.8386 - accuracy: 0.7709 - val_loss: 0.8739 - val_accuracy: 0.7504\n","Epoch 9/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.8242 - accuracy: 0.7765\n","Epoch 9: val_accuracy improved from 0.75311 to 0.75671, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.8249 - accuracy: 0.7744 - val_loss: 0.8629 - val_accuracy: 0.7567\n","Epoch 10/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.8178 - accuracy: 0.7736\n","Epoch 10: val_accuracy improved from 0.75671 to 0.75714, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.8140 - accuracy: 0.7768 - val_loss: 0.8523 - val_accuracy: 0.7571\n","Epoch 11/50\n","130/130 [==============================] - ETA: 0s - loss: 0.8040 - accuracy: 0.7786\n","Epoch 11: val_accuracy improved from 0.75714 to 0.75931, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.8040 - accuracy: 0.7786 - val_loss: 0.8409 - val_accuracy: 0.7593\n","Epoch 12/50\n","130/130 [==============================] - ETA: 0s - loss: 0.7944 - accuracy: 0.7803\n","Epoch 12: val_accuracy did not improve from 0.75931\n","130/130 [==============================] - 1s 11ms/step - loss: 0.7944 - accuracy: 0.7803 - val_loss: 0.8308 - val_accuracy: 0.7588\n","Epoch 13/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.7852 - accuracy: 0.7805\n","Epoch 13: val_accuracy improved from 0.75931 to 0.76528, saving model to best_model.h5\n","130/130 [==============================] - 2s 12ms/step - loss: 0.7841 - accuracy: 0.7823 - val_loss: 0.8232 - val_accuracy: 0.7653\n","Epoch 14/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.7740 - accuracy: 0.7854\n","Epoch 14: val_accuracy improved from 0.76528 to 0.76628, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.7739 - accuracy: 0.7865 - val_loss: 0.8135 - val_accuracy: 0.7663\n","Epoch 15/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.7688 - accuracy: 0.7844\n","Epoch 15: val_accuracy improved from 0.76628 to 0.76779, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.7655 - accuracy: 0.7864 - val_loss: 0.8053 - val_accuracy: 0.7678\n","Epoch 16/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.7577 - accuracy: 0.7899\n","Epoch 16: val_accuracy improved from 0.76779 to 0.77019, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.7561 - accuracy: 0.7898 - val_loss: 0.7950 - val_accuracy: 0.7702\n","Epoch 17/50\n","122/130 [===========================>..] - ETA: 0s - loss: 0.7546 - accuracy: 0.7900\n","Epoch 17: val_accuracy did not improve from 0.77019\n","130/130 [==============================] - 1s 7ms/step - loss: 0.7487 - accuracy: 0.7922 - val_loss: 0.7865 - val_accuracy: 0.7698\n","Epoch 18/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.7396 - accuracy: 0.7930\n","Epoch 18: val_accuracy improved from 0.77019 to 0.77419, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.7398 - accuracy: 0.7933 - val_loss: 0.7784 - val_accuracy: 0.7742\n","Epoch 19/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.7294 - accuracy: 0.7948\n","Epoch 19: val_accuracy did not improve from 0.77419\n","130/130 [==============================] - 1s 8ms/step - loss: 0.7320 - accuracy: 0.7937 - val_loss: 0.7700 - val_accuracy: 0.7735\n","Epoch 20/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.7245 - accuracy: 0.7953\n","Epoch 20: val_accuracy improved from 0.77419 to 0.77631, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.7255 - accuracy: 0.7946 - val_loss: 0.7626 - val_accuracy: 0.7763\n","Epoch 21/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.7163 - accuracy: 0.7958\n","Epoch 21: val_accuracy improved from 0.77631 to 0.77774, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.7151 - accuracy: 0.7971 - val_loss: 0.7518 - val_accuracy: 0.7777\n","Epoch 22/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.7085 - accuracy: 0.7981\n","Epoch 22: val_accuracy did not improve from 0.77774\n","130/130 [==============================] - 1s 8ms/step - loss: 0.7074 - accuracy: 0.7987 - val_loss: 0.7513 - val_accuracy: 0.7727\n","Epoch 23/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.7035 - accuracy: 0.7957\n","Epoch 23: val_accuracy improved from 0.77774 to 0.77871, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.7067 - accuracy: 0.7945 - val_loss: 0.7395 - val_accuracy: 0.7787\n","Epoch 24/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.6939 - accuracy: 0.8019\n","Epoch 24: val_accuracy improved from 0.77871 to 0.77983, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.6956 - accuracy: 0.8014 - val_loss: 0.7364 - val_accuracy: 0.7798\n","Epoch 25/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.6876 - accuracy: 0.8037\n","Epoch 25: val_accuracy improved from 0.77983 to 0.78338, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 0.6871 - accuracy: 0.8037 - val_loss: 0.7263 - val_accuracy: 0.7834\n","Epoch 26/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.6766 - accuracy: 0.8084\n","Epoch 26: val_accuracy improved from 0.78338 to 0.78528, saving model to best_model.h5\n","130/130 [==============================] - 2s 12ms/step - loss: 0.6788 - accuracy: 0.8059 - val_loss: 0.7162 - val_accuracy: 0.7853\n","Epoch 27/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.6717 - accuracy: 0.8055\n","Epoch 27: val_accuracy improved from 0.78528 to 0.78607, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.6717 - accuracy: 0.8067 - val_loss: 0.7098 - val_accuracy: 0.7861\n","Epoch 28/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.6622 - accuracy: 0.8086\n","Epoch 28: val_accuracy improved from 0.78607 to 0.78854, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.6628 - accuracy: 0.8086 - val_loss: 0.7001 - val_accuracy: 0.7885\n","Epoch 29/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.6576 - accuracy: 0.8099\n","Epoch 29: val_accuracy improved from 0.78854 to 0.79003, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.6563 - accuracy: 0.8101 - val_loss: 0.6929 - val_accuracy: 0.7900\n","Epoch 30/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.6480 - accuracy: 0.8114\n","Epoch 30: val_accuracy improved from 0.79003 to 0.79166, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.6484 - accuracy: 0.8110 - val_loss: 0.6854 - val_accuracy: 0.7917\n","Epoch 31/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.6404 - accuracy: 0.8135\n","Epoch 31: val_accuracy did not improve from 0.79166\n","130/130 [==============================] - 1s 8ms/step - loss: 0.6416 - accuracy: 0.8116 - val_loss: 0.6797 - val_accuracy: 0.7891\n","Epoch 32/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.6355 - accuracy: 0.8144\n","Epoch 32: val_accuracy improved from 0.79166 to 0.79273, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.6345 - accuracy: 0.8143 - val_loss: 0.6728 - val_accuracy: 0.7927\n","Epoch 33/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.6270 - accuracy: 0.8157\n","Epoch 33: val_accuracy did not improve from 0.79273\n","130/130 [==============================] - 1s 8ms/step - loss: 0.6272 - accuracy: 0.8150 - val_loss: 0.6687 - val_accuracy: 0.7921\n","Epoch 34/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.6214 - accuracy: 0.8159\n","Epoch 34: val_accuracy improved from 0.79273 to 0.79440, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.6222 - accuracy: 0.8153 - val_loss: 0.6613 - val_accuracy: 0.7944\n","Epoch 35/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.6154 - accuracy: 0.8179\n","Epoch 35: val_accuracy improved from 0.79440 to 0.79510, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.6159 - accuracy: 0.8179 - val_loss: 0.6535 - val_accuracy: 0.7951\n","Epoch 36/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.6092 - accuracy: 0.8189\n","Epoch 36: val_accuracy improved from 0.79510 to 0.79604, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.6088 - accuracy: 0.8186 - val_loss: 0.6484 - val_accuracy: 0.7960\n","Epoch 37/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.8185\n","Epoch 37: val_accuracy improved from 0.79604 to 0.79688, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 0.6020 - accuracy: 0.8191 - val_loss: 0.6484 - val_accuracy: 0.7969\n","Epoch 38/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.5972 - accuracy: 0.8208\n","Epoch 38: val_accuracy did not improve from 0.79688\n","130/130 [==============================] - 1s 11ms/step - loss: 0.5973 - accuracy: 0.8207 - val_loss: 0.6362 - val_accuracy: 0.7960\n","Epoch 39/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.5911 - accuracy: 0.8213\n","Epoch 39: val_accuracy improved from 0.79688 to 0.80157, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.5918 - accuracy: 0.8214 - val_loss: 0.6276 - val_accuracy: 0.8016\n","Epoch 40/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.5865 - accuracy: 0.8232\n","Epoch 40: val_accuracy improved from 0.80157 to 0.80746, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.5855 - accuracy: 0.8241 - val_loss: 0.6235 - val_accuracy: 0.8075\n","Epoch 41/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.5826 - accuracy: 0.8265\n","Epoch 41: val_accuracy did not improve from 0.80746\n","130/130 [==============================] - 1s 7ms/step - loss: 0.5797 - accuracy: 0.8271 - val_loss: 0.6205 - val_accuracy: 0.8068\n","Epoch 42/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.5734 - accuracy: 0.8285\n","Epoch 42: val_accuracy improved from 0.80746 to 0.80985, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.5726 - accuracy: 0.8289 - val_loss: 0.6147 - val_accuracy: 0.8098\n","Epoch 43/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.5705 - accuracy: 0.8263\n","Epoch 43: val_accuracy improved from 0.80985 to 0.81116, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.5701 - accuracy: 0.8271 - val_loss: 0.6068 - val_accuracy: 0.8112\n","Epoch 44/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.5623 - accuracy: 0.8303\n","Epoch 44: val_accuracy improved from 0.81116 to 0.81262, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.5658 - accuracy: 0.8282 - val_loss: 0.6035 - val_accuracy: 0.8126\n","Epoch 45/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.5599 - accuracy: 0.8328\n","Epoch 45: val_accuracy did not improve from 0.81262\n","130/130 [==============================] - 1s 8ms/step - loss: 0.5596 - accuracy: 0.8326 - val_loss: 0.6011 - val_accuracy: 0.8118\n","Epoch 46/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.5546 - accuracy: 0.8334\n","Epoch 46: val_accuracy did not improve from 0.81262\n","130/130 [==============================] - 1s 7ms/step - loss: 0.5538 - accuracy: 0.8339 - val_loss: 0.5955 - val_accuracy: 0.8125\n","Epoch 47/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.5477 - accuracy: 0.8361\n","Epoch 47: val_accuracy did not improve from 0.81262\n","130/130 [==============================] - 1s 8ms/step - loss: 0.5474 - accuracy: 0.8362 - val_loss: 0.5934 - val_accuracy: 0.8107\n","Epoch 48/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.5498 - accuracy: 0.8331\n","Epoch 48: val_accuracy improved from 0.81262 to 0.81677, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.5507 - accuracy: 0.8324 - val_loss: 0.5828 - val_accuracy: 0.8168\n","Epoch 49/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.5417 - accuracy: 0.8387\n","Epoch 49: val_accuracy improved from 0.81677 to 0.82178, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 0.5404 - accuracy: 0.8390 - val_loss: 0.5774 - val_accuracy: 0.8218\n","Epoch 50/50\n","130/130 [==============================] - ETA: 0s - loss: 0.5345 - accuracy: 0.8407\n","Epoch 50: val_accuracy improved from 0.82178 to 0.82263, saving model to best_model.h5\n","130/130 [==============================] - 1s 10ms/step - loss: 0.5345 - accuracy: 0.8407 - val_loss: 0.5717 - val_accuracy: 0.8226\n","Fold 3\n","Epoch 1/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.5277 - accuracy: 0.8433\n","Epoch 1: val_accuracy improved from 0.82263 to 0.82377, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.5283 - accuracy: 0.8429 - val_loss: 0.5666 - val_accuracy: 0.8238\n","Epoch 2/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.5210 - accuracy: 0.8430\n","Epoch 2: val_accuracy did not improve from 0.82377\n","130/130 [==============================] - 1s 7ms/step - loss: 0.5239 - accuracy: 0.8425 - val_loss: 0.5637 - val_accuracy: 0.8232\n","Epoch 3/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.5228 - accuracy: 0.8442\n","Epoch 3: val_accuracy improved from 0.82377 to 0.82388, saving model to best_model.h5\n","130/130 [==============================] - 1s 7ms/step - loss: 0.5203 - accuracy: 0.8450 - val_loss: 0.5625 - val_accuracy: 0.8239\n","Epoch 4/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.5140 - accuracy: 0.8465\n","Epoch 4: val_accuracy improved from 0.82388 to 0.82635, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.5153 - accuracy: 0.8463 - val_loss: 0.5545 - val_accuracy: 0.8264\n","Epoch 5/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.5078 - accuracy: 0.8484\n","Epoch 5: val_accuracy did not improve from 0.82635\n","130/130 [==============================] - 1s 7ms/step - loss: 0.5101 - accuracy: 0.8464 - val_loss: 0.5551 - val_accuracy: 0.8222\n","Epoch 6/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.5046 - accuracy: 0.8482\n","Epoch 6: val_accuracy improved from 0.82635 to 0.82779, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.5045 - accuracy: 0.8484 - val_loss: 0.5475 - val_accuracy: 0.8278\n","Epoch 7/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.5034 - accuracy: 0.8485\n","Epoch 7: val_accuracy did not improve from 0.82779\n","130/130 [==============================] - 1s 8ms/step - loss: 0.5009 - accuracy: 0.8496 - val_loss: 0.5440 - val_accuracy: 0.8274\n","Epoch 8/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.4984 - accuracy: 0.8503\n","Epoch 8: val_accuracy improved from 0.82779 to 0.82926, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4966 - accuracy: 0.8515 - val_loss: 0.5401 - val_accuracy: 0.8293\n","Epoch 9/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.4913 - accuracy: 0.8529\n","Epoch 9: val_accuracy did not improve from 0.82926\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4926 - accuracy: 0.8526 - val_loss: 0.5359 - val_accuracy: 0.8250\n","Epoch 10/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.4855 - accuracy: 0.8534\n","Epoch 10: val_accuracy did not improve from 0.82926\n","130/130 [==============================] - 1s 9ms/step - loss: 0.4875 - accuracy: 0.8527 - val_loss: 0.5297 - val_accuracy: 0.8280\n","Epoch 11/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.4844 - accuracy: 0.8536\n","Epoch 11: val_accuracy improved from 0.82926 to 0.83010, saving model to best_model.h5\n","130/130 [==============================] - 1s 12ms/step - loss: 0.4831 - accuracy: 0.8542 - val_loss: 0.5275 - val_accuracy: 0.8301\n","Epoch 12/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.4787 - accuracy: 0.8575\n","Epoch 12: val_accuracy improved from 0.83010 to 0.83038, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 0.4790 - accuracy: 0.8569 - val_loss: 0.5224 - val_accuracy: 0.8304\n","Epoch 13/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.4752 - accuracy: 0.8582\n","Epoch 13: val_accuracy improved from 0.83038 to 0.83127, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.4743 - accuracy: 0.8585 - val_loss: 0.5179 - val_accuracy: 0.8313\n","Epoch 14/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.4655 - accuracy: 0.8630\n","Epoch 14: val_accuracy improved from 0.83127 to 0.83341, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4716 - accuracy: 0.8588 - val_loss: 0.5146 - val_accuracy: 0.8334\n","Epoch 15/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.4671 - accuracy: 0.8594\n","Epoch 15: val_accuracy improved from 0.83341 to 0.83434, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4678 - accuracy: 0.8593 - val_loss: 0.5087 - val_accuracy: 0.8343\n","Epoch 16/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.4629 - accuracy: 0.8606\n","Epoch 16: val_accuracy improved from 0.83434 to 0.83652, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4641 - accuracy: 0.8601 - val_loss: 0.5027 - val_accuracy: 0.8365\n","Epoch 17/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.4605 - accuracy: 0.8605\n","Epoch 17: val_accuracy did not improve from 0.83652\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4597 - accuracy: 0.8610 - val_loss: 0.5031 - val_accuracy: 0.8331\n","Epoch 18/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.4575 - accuracy: 0.8608\n","Epoch 18: val_accuracy did not improve from 0.83652\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4572 - accuracy: 0.8611 - val_loss: 0.4990 - val_accuracy: 0.8353\n","Epoch 19/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.4526 - accuracy: 0.8625\n","Epoch 19: val_accuracy did not improve from 0.83652\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4539 - accuracy: 0.8619 - val_loss: 0.4958 - val_accuracy: 0.8351\n","Epoch 20/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.4531 - accuracy: 0.8625\n","Epoch 20: val_accuracy did not improve from 0.83652\n","130/130 [==============================] - 1s 7ms/step - loss: 0.4515 - accuracy: 0.8638 - val_loss: 0.4980 - val_accuracy: 0.8333\n","Epoch 21/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.4434 - accuracy: 0.8685\n","Epoch 21: val_accuracy improved from 0.83652 to 0.83761, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4474 - accuracy: 0.8653 - val_loss: 0.4942 - val_accuracy: 0.8376\n","Epoch 22/50\n","130/130 [==============================] - ETA: 0s - loss: 0.4428 - accuracy: 0.8656\n","Epoch 22: val_accuracy did not improve from 0.83761\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4428 - accuracy: 0.8656 - val_loss: 0.4889 - val_accuracy: 0.8345\n","Epoch 23/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.4397 - accuracy: 0.8670\n","Epoch 23: val_accuracy improved from 0.83761 to 0.83789, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 0.4396 - accuracy: 0.8674 - val_loss: 0.4871 - val_accuracy: 0.8379\n","Epoch 24/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.4355 - accuracy: 0.8677\n","Epoch 24: val_accuracy improved from 0.83789 to 0.83898, saving model to best_model.h5\n","130/130 [==============================] - 1s 10ms/step - loss: 0.4371 - accuracy: 0.8670 - val_loss: 0.4847 - val_accuracy: 0.8390\n","Epoch 25/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.4361 - accuracy: 0.8680\n","Epoch 25: val_accuracy did not improve from 0.83898\n","130/130 [==============================] - 1s 11ms/step - loss: 0.4342 - accuracy: 0.8690 - val_loss: 0.4787 - val_accuracy: 0.8384\n","Epoch 26/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.4295 - accuracy: 0.8689\n","Epoch 26: val_accuracy improved from 0.83898 to 0.83958, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4317 - accuracy: 0.8680 - val_loss: 0.4739 - val_accuracy: 0.8396\n","Epoch 27/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.4283 - accuracy: 0.8692\n","Epoch 27: val_accuracy improved from 0.83958 to 0.84217, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4288 - accuracy: 0.8690 - val_loss: 0.4715 - val_accuracy: 0.8422\n","Epoch 28/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.4264 - accuracy: 0.8694\n","Epoch 28: val_accuracy did not improve from 0.84217\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4264 - accuracy: 0.8699 - val_loss: 0.4704 - val_accuracy: 0.8406\n","Epoch 29/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.4236 - accuracy: 0.8720\n","Epoch 29: val_accuracy improved from 0.84217 to 0.84222, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4225 - accuracy: 0.8720 - val_loss: 0.4695 - val_accuracy: 0.8422\n","Epoch 30/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.4233 - accuracy: 0.8726\n","Epoch 30: val_accuracy improved from 0.84222 to 0.84374, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4221 - accuracy: 0.8729 - val_loss: 0.4663 - val_accuracy: 0.8437\n","Epoch 31/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.4154 - accuracy: 0.8740\n","Epoch 31: val_accuracy improved from 0.84374 to 0.84434, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4174 - accuracy: 0.8737 - val_loss: 0.4646 - val_accuracy: 0.8443\n","Epoch 32/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.4168 - accuracy: 0.8733\n","Epoch 32: val_accuracy improved from 0.84434 to 0.84739, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4133 - accuracy: 0.8749 - val_loss: 0.4597 - val_accuracy: 0.8474\n","Epoch 33/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.4142 - accuracy: 0.8751\n","Epoch 33: val_accuracy improved from 0.84739 to 0.84859, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4127 - accuracy: 0.8753 - val_loss: 0.4577 - val_accuracy: 0.8486\n","Epoch 34/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.4118 - accuracy: 0.8747\n","Epoch 34: val_accuracy did not improve from 0.84859\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4123 - accuracy: 0.8738 - val_loss: 0.4577 - val_accuracy: 0.8451\n","Epoch 35/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8736\n","Epoch 35: val_accuracy did not improve from 0.84859\n","130/130 [==============================] - 1s 9ms/step - loss: 0.4109 - accuracy: 0.8734 - val_loss: 0.4608 - val_accuracy: 0.8452\n","Epoch 36/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.4065 - accuracy: 0.8763\n","Epoch 36: val_accuracy improved from 0.84859 to 0.84946, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 0.4060 - accuracy: 0.8765 - val_loss: 0.4488 - val_accuracy: 0.8495\n","Epoch 37/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.4003 - accuracy: 0.8768\n","Epoch 37: val_accuracy improved from 0.84946 to 0.85028, saving model to best_model.h5\n","130/130 [==============================] - 1s 12ms/step - loss: 0.3999 - accuracy: 0.8773 - val_loss: 0.4456 - val_accuracy: 0.8503\n","Epoch 38/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.4004 - accuracy: 0.8782\n","Epoch 38: val_accuracy improved from 0.85028 to 0.85098, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.4002 - accuracy: 0.8790 - val_loss: 0.4532 - val_accuracy: 0.8510\n","Epoch 39/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.4000 - accuracy: 0.8776\n","Epoch 39: val_accuracy did not improve from 0.85098\n","130/130 [==============================] - 1s 8ms/step - loss: 0.4001 - accuracy: 0.8775 - val_loss: 0.4466 - val_accuracy: 0.8482\n","Epoch 40/50\n","130/130 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.8811\n","Epoch 40: val_accuracy improved from 0.85098 to 0.85355, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3941 - accuracy: 0.8811 - val_loss: 0.4424 - val_accuracy: 0.8536\n","Epoch 41/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.3908 - accuracy: 0.8817\n","Epoch 41: val_accuracy improved from 0.85355 to 0.85469, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3906 - accuracy: 0.8826 - val_loss: 0.4379 - val_accuracy: 0.8547\n","Epoch 42/50\n","130/130 [==============================] - ETA: 0s - loss: 0.3874 - accuracy: 0.8840\n","Epoch 42: val_accuracy improved from 0.85469 to 0.85504, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3874 - accuracy: 0.8840 - val_loss: 0.4359 - val_accuracy: 0.8550\n","Epoch 43/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.3839 - accuracy: 0.8831\n","Epoch 43: val_accuracy improved from 0.85504 to 0.85719, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3840 - accuracy: 0.8826 - val_loss: 0.4308 - val_accuracy: 0.8572\n","Epoch 44/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.3792 - accuracy: 0.8862\n","Epoch 44: val_accuracy did not improve from 0.85719\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3810 - accuracy: 0.8855 - val_loss: 0.4304 - val_accuracy: 0.8569\n","Epoch 45/50\n","130/130 [==============================] - ETA: 0s - loss: 0.3794 - accuracy: 0.8844\n","Epoch 45: val_accuracy did not improve from 0.85719\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3794 - accuracy: 0.8844 - val_loss: 0.4297 - val_accuracy: 0.8564\n","Epoch 46/50\n","130/130 [==============================] - ETA: 0s - loss: 0.3785 - accuracy: 0.8842\n","Epoch 46: val_accuracy improved from 0.85719 to 0.85762, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3785 - accuracy: 0.8842 - val_loss: 0.4260 - val_accuracy: 0.8576\n","Epoch 47/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.3740 - accuracy: 0.8874\n","Epoch 47: val_accuracy improved from 0.85762 to 0.86006, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3743 - accuracy: 0.8866 - val_loss: 0.4223 - val_accuracy: 0.8601\n","Epoch 48/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.3732 - accuracy: 0.8858\n","Epoch 48: val_accuracy did not improve from 0.86006\n","130/130 [==============================] - 1s 11ms/step - loss: 0.3728 - accuracy: 0.8862 - val_loss: 0.4222 - val_accuracy: 0.8585\n","Epoch 49/50\n","130/130 [==============================] - ETA: 0s - loss: 0.3723 - accuracy: 0.8851\n","Epoch 49: val_accuracy improved from 0.86006 to 0.86013, saving model to best_model.h5\n","130/130 [==============================] - 2s 12ms/step - loss: 0.3723 - accuracy: 0.8851 - val_loss: 0.4189 - val_accuracy: 0.8601\n","Epoch 50/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.3672 - accuracy: 0.8885\n","Epoch 50: val_accuracy did not improve from 0.86013\n","130/130 [==============================] - 1s 9ms/step - loss: 0.3673 - accuracy: 0.8885 - val_loss: 0.4182 - val_accuracy: 0.8591\n","Fold 4\n","Epoch 1/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.3672 - accuracy: 0.8897\n","Epoch 1: val_accuracy improved from 0.86013 to 0.86179, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.3669 - accuracy: 0.8888 - val_loss: 0.4146 - val_accuracy: 0.8618\n","Epoch 2/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.3630 - accuracy: 0.8893\n","Epoch 2: val_accuracy improved from 0.86179 to 0.86256, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3625 - accuracy: 0.8898 - val_loss: 0.4134 - val_accuracy: 0.8626\n","Epoch 3/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.3611 - accuracy: 0.8907\n","Epoch 3: val_accuracy improved from 0.86256 to 0.86287, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3622 - accuracy: 0.8900 - val_loss: 0.4123 - val_accuracy: 0.8629\n","Epoch 4/50\n","130/130 [==============================] - ETA: 0s - loss: 0.3607 - accuracy: 0.8899\n","Epoch 4: val_accuracy improved from 0.86287 to 0.86381, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3607 - accuracy: 0.8899 - val_loss: 0.4081 - val_accuracy: 0.8638\n","Epoch 5/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.3531 - accuracy: 0.8941\n","Epoch 5: val_accuracy improved from 0.86381 to 0.86476, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3565 - accuracy: 0.8924 - val_loss: 0.4058 - val_accuracy: 0.8648\n","Epoch 6/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.3550 - accuracy: 0.8933\n","Epoch 6: val_accuracy improved from 0.86476 to 0.86642, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3544 - accuracy: 0.8930 - val_loss: 0.4037 - val_accuracy: 0.8664\n","Epoch 7/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.3499 - accuracy: 0.8949\n","Epoch 7: val_accuracy improved from 0.86642 to 0.86755, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3512 - accuracy: 0.8948 - val_loss: 0.4007 - val_accuracy: 0.8676\n","Epoch 8/50\n","130/130 [==============================] - ETA: 0s - loss: 0.3497 - accuracy: 0.8947\n","Epoch 8: val_accuracy improved from 0.86755 to 0.86789, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 0.3497 - accuracy: 0.8947 - val_loss: 0.3985 - val_accuracy: 0.8679\n","Epoch 9/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.3485 - accuracy: 0.8952\n","Epoch 9: val_accuracy improved from 0.86789 to 0.86852, saving model to best_model.h5\n","130/130 [==============================] - 2s 12ms/step - loss: 0.3488 - accuracy: 0.8948 - val_loss: 0.3966 - val_accuracy: 0.8685\n","Epoch 10/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.3506 - accuracy: 0.8932\n","Epoch 10: val_accuracy did not improve from 0.86852\n","130/130 [==============================] - 1s 9ms/step - loss: 0.3541 - accuracy: 0.8913 - val_loss: 0.4072 - val_accuracy: 0.8574\n","Epoch 11/50\n","122/130 [===========================>..] - ETA: 0s - loss: 0.3548 - accuracy: 0.8877\n","Epoch 11: val_accuracy did not improve from 0.86852\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3523 - accuracy: 0.8893 - val_loss: 0.3952 - val_accuracy: 0.8649\n","Epoch 12/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.3487 - accuracy: 0.8901\n","Epoch 12: val_accuracy did not improve from 0.86852\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3479 - accuracy: 0.8907 - val_loss: 0.3916 - val_accuracy: 0.8674\n","Epoch 13/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.3448 - accuracy: 0.8928\n","Epoch 13: val_accuracy did not improve from 0.86852\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3442 - accuracy: 0.8930 - val_loss: 0.3925 - val_accuracy: 0.8683\n","Epoch 14/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8950\n","Epoch 14: val_accuracy improved from 0.86852 to 0.87224, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3406 - accuracy: 0.8955 - val_loss: 0.3888 - val_accuracy: 0.8722\n","Epoch 15/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.3409 - accuracy: 0.8965\n","Epoch 15: val_accuracy did not improve from 0.87224\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3413 - accuracy: 0.8964 - val_loss: 0.3874 - val_accuracy: 0.8696\n","Epoch 16/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.3385 - accuracy: 0.8968\n","Epoch 16: val_accuracy improved from 0.87224 to 0.87412, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3380 - accuracy: 0.8970 - val_loss: 0.3845 - val_accuracy: 0.8741\n","Epoch 17/50\n","130/130 [==============================] - ETA: 0s - loss: 0.3356 - accuracy: 0.8987\n","Epoch 17: val_accuracy improved from 0.87412 to 0.87615, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3356 - accuracy: 0.8987 - val_loss: 0.3835 - val_accuracy: 0.8761\n","Epoch 18/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.8998\n","Epoch 18: val_accuracy did not improve from 0.87615\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3308 - accuracy: 0.8999 - val_loss: 0.3818 - val_accuracy: 0.8753\n","Epoch 19/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.3287 - accuracy: 0.9012\n","Epoch 19: val_accuracy did not improve from 0.87615\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3286 - accuracy: 0.9011 - val_loss: 0.3777 - val_accuracy: 0.8751\n","Epoch 20/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.3265 - accuracy: 0.9015\n","Epoch 20: val_accuracy improved from 0.87615 to 0.87818, saving model to best_model.h5\n","130/130 [==============================] - 2s 12ms/step - loss: 0.3269 - accuracy: 0.9013 - val_loss: 0.3751 - val_accuracy: 0.8782\n","Epoch 21/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.9036\n","Epoch 21: val_accuracy improved from 0.87818 to 0.87862, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 0.3237 - accuracy: 0.9038 - val_loss: 0.3733 - val_accuracy: 0.8786\n","Epoch 22/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.3229 - accuracy: 0.9026\n","Epoch 22: val_accuracy did not improve from 0.87862\n","130/130 [==============================] - 1s 10ms/step - loss: 0.3222 - accuracy: 0.9036 - val_loss: 0.3722 - val_accuracy: 0.8780\n","Epoch 23/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.3256 - accuracy: 0.9014\n","Epoch 23: val_accuracy did not improve from 0.87862\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3261 - accuracy: 0.9019 - val_loss: 0.3725 - val_accuracy: 0.8760\n","Epoch 24/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.9034\n","Epoch 24: val_accuracy improved from 0.87862 to 0.88217, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3193 - accuracy: 0.9035 - val_loss: 0.3671 - val_accuracy: 0.8822\n","Epoch 25/50\n","130/130 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.9007\n","Epoch 25: val_accuracy did not improve from 0.88217\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3266 - accuracy: 0.9007 - val_loss: 0.3729 - val_accuracy: 0.8774\n","Epoch 26/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.9002\n","Epoch 26: val_accuracy did not improve from 0.88217\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3227 - accuracy: 0.9009 - val_loss: 0.3662 - val_accuracy: 0.8803\n","Epoch 27/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.9038\n","Epoch 27: val_accuracy improved from 0.88217 to 0.88336, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3178 - accuracy: 0.9040 - val_loss: 0.3633 - val_accuracy: 0.8834\n","Epoch 28/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.9064\n","Epoch 28: val_accuracy improved from 0.88336 to 0.88352, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3139 - accuracy: 0.9057 - val_loss: 0.3617 - val_accuracy: 0.8835\n","Epoch 29/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.3102 - accuracy: 0.9086\n","Epoch 29: val_accuracy improved from 0.88352 to 0.88430, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3107 - accuracy: 0.9079 - val_loss: 0.3614 - val_accuracy: 0.8843\n","Epoch 30/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.9096\n","Epoch 30: val_accuracy improved from 0.88430 to 0.88533, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3080 - accuracy: 0.9090 - val_loss: 0.3589 - val_accuracy: 0.8853\n","Epoch 31/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.9091\n","Epoch 31: val_accuracy did not improve from 0.88533\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3067 - accuracy: 0.9093 - val_loss: 0.3588 - val_accuracy: 0.8839\n","Epoch 32/50\n","130/130 [==============================] - ETA: 0s - loss: 0.3051 - accuracy: 0.9099\n","Epoch 32: val_accuracy improved from 0.88533 to 0.88656, saving model to best_model.h5\n","130/130 [==============================] - 1s 11ms/step - loss: 0.3051 - accuracy: 0.9099 - val_loss: 0.3555 - val_accuracy: 0.8866\n","Epoch 33/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.3024 - accuracy: 0.9109\n","Epoch 33: val_accuracy did not improve from 0.88656\n","130/130 [==============================] - 1s 11ms/step - loss: 0.3036 - accuracy: 0.9098 - val_loss: 0.3540 - val_accuracy: 0.8860\n","Epoch 34/50\n","130/130 [==============================] - ETA: 0s - loss: 0.3001 - accuracy: 0.9114\n","Epoch 34: val_accuracy did not improve from 0.88656\n","130/130 [==============================] - 1s 11ms/step - loss: 0.3001 - accuracy: 0.9114 - val_loss: 0.3529 - val_accuracy: 0.8864\n","Epoch 35/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.3028 - accuracy: 0.9099\n","Epoch 35: val_accuracy did not improve from 0.88656\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3025 - accuracy: 0.9096 - val_loss: 0.3580 - val_accuracy: 0.8783\n","Epoch 36/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.9090\n","Epoch 36: val_accuracy did not improve from 0.88656\n","130/130 [==============================] - 1s 8ms/step - loss: 0.3016 - accuracy: 0.9092 - val_loss: 0.3517 - val_accuracy: 0.8858\n","Epoch 37/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.2966 - accuracy: 0.9130\n","Epoch 37: val_accuracy improved from 0.88656 to 0.89084, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2969 - accuracy: 0.9127 - val_loss: 0.3438 - val_accuracy: 0.8908\n","Epoch 38/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.9130\n","Epoch 38: val_accuracy did not improve from 0.89084\n","130/130 [==============================] - 1s 9ms/step - loss: 0.2946 - accuracy: 0.9129 - val_loss: 0.3447 - val_accuracy: 0.8886\n","Epoch 39/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.9124\n","Epoch 39: val_accuracy did not improve from 0.89084\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2936 - accuracy: 0.9130 - val_loss: 0.3426 - val_accuracy: 0.8891\n","Epoch 40/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9133\n","Epoch 40: val_accuracy did not improve from 0.89084\n","130/130 [==============================] - 1s 9ms/step - loss: 0.2924 - accuracy: 0.9133 - val_loss: 0.3456 - val_accuracy: 0.8857\n","Epoch 41/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.9131\n","Epoch 41: val_accuracy did not improve from 0.89084\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2905 - accuracy: 0.9133 - val_loss: 0.3399 - val_accuracy: 0.8884\n","Epoch 42/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9148\n","Epoch 42: val_accuracy did not improve from 0.89084\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2886 - accuracy: 0.9138 - val_loss: 0.3380 - val_accuracy: 0.8893\n","Epoch 43/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.9155\n","Epoch 43: val_accuracy did not improve from 0.89084\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2871 - accuracy: 0.9155 - val_loss: 0.3356 - val_accuracy: 0.8896\n","Epoch 44/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2848 - accuracy: 0.9165\n","Epoch 44: val_accuracy did not improve from 0.89084\n","130/130 [==============================] - 2s 12ms/step - loss: 0.2840 - accuracy: 0.9170 - val_loss: 0.3360 - val_accuracy: 0.8892\n","Epoch 45/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9176\n","Epoch 45: val_accuracy did not improve from 0.89084\n","130/130 [==============================] - 1s 11ms/step - loss: 0.2849 - accuracy: 0.9174 - val_loss: 0.3369 - val_accuracy: 0.8908\n","Epoch 46/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9153\n","Epoch 46: val_accuracy improved from 0.89084 to 0.89272, saving model to best_model.h5\n","130/130 [==============================] - 1s 10ms/step - loss: 0.2880 - accuracy: 0.9155 - val_loss: 0.3314 - val_accuracy: 0.8927\n","Epoch 47/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.2841 - accuracy: 0.9185\n","Epoch 47: val_accuracy improved from 0.89272 to 0.89513, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2852 - accuracy: 0.9176 - val_loss: 0.3295 - val_accuracy: 0.8951\n","Epoch 48/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.2834 - accuracy: 0.9182\n","Epoch 48: val_accuracy improved from 0.89513 to 0.89615, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2821 - accuracy: 0.9184 - val_loss: 0.3282 - val_accuracy: 0.8962\n","Epoch 49/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.2818 - accuracy: 0.9186\n","Epoch 49: val_accuracy improved from 0.89615 to 0.89803, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2827 - accuracy: 0.9176 - val_loss: 0.3237 - val_accuracy: 0.8980\n","Epoch 50/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.2768 - accuracy: 0.9219\n","Epoch 50: val_accuracy improved from 0.89803 to 0.89809, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.2780 - accuracy: 0.9213 - val_loss: 0.3219 - val_accuracy: 0.8981\n","Fold 5\n","Epoch 1/50\n","130/130 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 0.9192\n","Epoch 1: val_accuracy did not improve from 0.89809\n","130/130 [==============================] - 1s 10ms/step - loss: 0.2796 - accuracy: 0.9192 - val_loss: 0.3250 - val_accuracy: 0.8931\n","Epoch 2/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.2780 - accuracy: 0.9184\n","Epoch 2: val_accuracy did not improve from 0.89809\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2771 - accuracy: 0.9191 - val_loss: 0.3241 - val_accuracy: 0.8943\n","Epoch 3/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.2719 - accuracy: 0.9234\n","Epoch 3: val_accuracy did not improve from 0.89809\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2729 - accuracy: 0.9225 - val_loss: 0.3196 - val_accuracy: 0.8971\n","Epoch 4/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.9232\n","Epoch 4: val_accuracy did not improve from 0.89809\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2696 - accuracy: 0.9231 - val_loss: 0.3187 - val_accuracy: 0.8956\n","Epoch 5/50\n","130/130 [==============================] - ETA: 0s - loss: 0.2688 - accuracy: 0.9231\n","Epoch 5: val_accuracy did not improve from 0.89809\n","130/130 [==============================] - 1s 10ms/step - loss: 0.2688 - accuracy: 0.9231 - val_loss: 0.3163 - val_accuracy: 0.8973\n","Epoch 6/50\n","130/130 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9242\n","Epoch 6: val_accuracy did not improve from 0.89809\n","130/130 [==============================] - 1s 12ms/step - loss: 0.2682 - accuracy: 0.9242 - val_loss: 0.3164 - val_accuracy: 0.8975\n","Epoch 7/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.2662 - accuracy: 0.9248\n","Epoch 7: val_accuracy improved from 0.89809 to 0.89940, saving model to best_model.h5\n","130/130 [==============================] - 2s 12ms/step - loss: 0.2652 - accuracy: 0.9253 - val_loss: 0.3162 - val_accuracy: 0.8994\n","Epoch 8/50\n","130/130 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.9258\n","Epoch 8: val_accuracy improved from 0.89940 to 0.90146, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.2656 - accuracy: 0.9258 - val_loss: 0.3159 - val_accuracy: 0.9015\n","Epoch 9/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.2634 - accuracy: 0.9271\n","Epoch 9: val_accuracy did not improve from 0.90146\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2635 - accuracy: 0.9273 - val_loss: 0.3126 - val_accuracy: 0.8997\n","Epoch 10/50\n","130/130 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.9275\n","Epoch 10: val_accuracy did not improve from 0.90146\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2614 - accuracy: 0.9275 - val_loss: 0.3118 - val_accuracy: 0.9008\n","Epoch 11/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.2610 - accuracy: 0.9273\n","Epoch 11: val_accuracy did not improve from 0.90146\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2600 - accuracy: 0.9282 - val_loss: 0.3117 - val_accuracy: 0.8995\n","Epoch 12/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.2576 - accuracy: 0.9280\n","Epoch 12: val_accuracy did not improve from 0.90146\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2592 - accuracy: 0.9266 - val_loss: 0.3092 - val_accuracy: 0.9009\n","Epoch 13/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.2558 - accuracy: 0.9299\n","Epoch 13: val_accuracy did not improve from 0.90146\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2568 - accuracy: 0.9295 - val_loss: 0.3068 - val_accuracy: 0.9006\n","Epoch 14/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.2563 - accuracy: 0.9293\n","Epoch 14: val_accuracy improved from 0.90146 to 0.90267, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2567 - accuracy: 0.9288 - val_loss: 0.3060 - val_accuracy: 0.9027\n","Epoch 15/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.9312\n","Epoch 15: val_accuracy improved from 0.90267 to 0.90481, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.2536 - accuracy: 0.9312 - val_loss: 0.3064 - val_accuracy: 0.9048\n","Epoch 16/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2528 - accuracy: 0.9312\n","Epoch 16: val_accuracy improved from 0.90481 to 0.90585, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.2520 - accuracy: 0.9314 - val_loss: 0.3013 - val_accuracy: 0.9058\n","Epoch 17/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9330\n","Epoch 17: val_accuracy improved from 0.90585 to 0.90662, saving model to best_model.h5\n","130/130 [==============================] - 1s 10ms/step - loss: 0.2506 - accuracy: 0.9323 - val_loss: 0.3006 - val_accuracy: 0.9066\n","Epoch 18/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2513 - accuracy: 0.9322\n","Epoch 18: val_accuracy did not improve from 0.90662\n","130/130 [==============================] - 1s 11ms/step - loss: 0.2506 - accuracy: 0.9325 - val_loss: 0.3041 - val_accuracy: 0.9045\n","Epoch 19/50\n","130/130 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.9328\n","Epoch 19: val_accuracy improved from 0.90662 to 0.90675, saving model to best_model.h5\n","130/130 [==============================] - 2s 12ms/step - loss: 0.2485 - accuracy: 0.9328 - val_loss: 0.2989 - val_accuracy: 0.9068\n","Epoch 20/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9344\n","Epoch 20: val_accuracy improved from 0.90675 to 0.90831, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.2466 - accuracy: 0.9335 - val_loss: 0.2961 - val_accuracy: 0.9083\n","Epoch 21/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.9295\n","Epoch 21: val_accuracy did not improve from 0.90831\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2519 - accuracy: 0.9291 - val_loss: 0.3026 - val_accuracy: 0.9057\n","Epoch 22/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.2496 - accuracy: 0.9317\n","Epoch 22: val_accuracy did not improve from 0.90831\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2493 - accuracy: 0.9316 - val_loss: 0.3025 - val_accuracy: 0.9054\n","Epoch 23/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.2502 - accuracy: 0.9304\n","Epoch 23: val_accuracy improved from 0.90831 to 0.90863, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2503 - accuracy: 0.9307 - val_loss: 0.2914 - val_accuracy: 0.9086\n","Epoch 24/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.2417 - accuracy: 0.9357\n","Epoch 24: val_accuracy did not improve from 0.90863\n","130/130 [==============================] - 1s 10ms/step - loss: 0.2414 - accuracy: 0.9355 - val_loss: 0.2928 - val_accuracy: 0.9080\n","Epoch 25/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.2420 - accuracy: 0.9343\n","Epoch 25: val_accuracy improved from 0.90863 to 0.91122, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.2409 - accuracy: 0.9349 - val_loss: 0.2905 - val_accuracy: 0.9112\n","Epoch 26/50\n","130/130 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.9372\n","Epoch 26: val_accuracy did not improve from 0.91122\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2386 - accuracy: 0.9372 - val_loss: 0.2905 - val_accuracy: 0.9110\n","Epoch 27/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.2367 - accuracy: 0.9373\n","Epoch 27: val_accuracy improved from 0.91122 to 0.91315, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2367 - accuracy: 0.9372 - val_loss: 0.2866 - val_accuracy: 0.9131\n","Epoch 28/50\n","130/130 [==============================] - ETA: 0s - loss: 0.2377 - accuracy: 0.9361\n","Epoch 28: val_accuracy did not improve from 0.91315\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2377 - accuracy: 0.9361 - val_loss: 0.2932 - val_accuracy: 0.9102\n","Epoch 29/50\n","127/130 [============================>.] - ETA: 0s - loss: 0.2335 - accuracy: 0.9385\n","Epoch 29: val_accuracy improved from 0.91315 to 0.91545, saving model to best_model.h5\n","130/130 [==============================] - 2s 12ms/step - loss: 0.2359 - accuracy: 0.9380 - val_loss: 0.2841 - val_accuracy: 0.9154\n","Epoch 30/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.2346 - accuracy: 0.9388\n","Epoch 30: val_accuracy did not improve from 0.91545\n","130/130 [==============================] - 1s 11ms/step - loss: 0.2345 - accuracy: 0.9386 - val_loss: 0.2843 - val_accuracy: 0.9127\n","Epoch 31/50\n","130/130 [==============================] - ETA: 0s - loss: 0.2323 - accuracy: 0.9389\n","Epoch 31: val_accuracy did not improve from 0.91545\n","130/130 [==============================] - 2s 12ms/step - loss: 0.2323 - accuracy: 0.9389 - val_loss: 0.2829 - val_accuracy: 0.9129\n","Epoch 32/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.2299 - accuracy: 0.9413\n","Epoch 32: val_accuracy did not improve from 0.91545\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2310 - accuracy: 0.9393 - val_loss: 0.2849 - val_accuracy: 0.9096\n","Epoch 33/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.9340\n","Epoch 33: val_accuracy improved from 0.91545 to 0.91665, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2411 - accuracy: 0.9340 - val_loss: 0.2805 - val_accuracy: 0.9166\n","Epoch 34/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.2297 - accuracy: 0.9408\n","Epoch 34: val_accuracy did not improve from 0.91665\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2291 - accuracy: 0.9413 - val_loss: 0.2818 - val_accuracy: 0.9160\n","Epoch 35/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.2283 - accuracy: 0.9410\n","Epoch 35: val_accuracy did not improve from 0.91665\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2276 - accuracy: 0.9411 - val_loss: 0.2778 - val_accuracy: 0.9160\n","Epoch 36/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9395\n","Epoch 36: val_accuracy did not improve from 0.91665\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2274 - accuracy: 0.9399 - val_loss: 0.2863 - val_accuracy: 0.9131\n","Epoch 37/50\n","125/130 [===========================>..] - ETA: 0s - loss: 0.2269 - accuracy: 0.9407\n","Epoch 37: val_accuracy did not improve from 0.91665\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2269 - accuracy: 0.9401 - val_loss: 0.2761 - val_accuracy: 0.9162\n","Epoch 38/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.2241 - accuracy: 0.9412\n","Epoch 38: val_accuracy improved from 0.91665 to 0.91694, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2234 - accuracy: 0.9418 - val_loss: 0.2756 - val_accuracy: 0.9169\n","Epoch 39/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.2210 - accuracy: 0.9433\n","Epoch 39: val_accuracy improved from 0.91694 to 0.91837, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2209 - accuracy: 0.9431 - val_loss: 0.2730 - val_accuracy: 0.9184\n","Epoch 40/50\n","124/130 [===========================>..] - ETA: 0s - loss: 0.2219 - accuracy: 0.9427\n","Epoch 40: val_accuracy did not improve from 0.91837\n","130/130 [==============================] - 1s 9ms/step - loss: 0.2197 - accuracy: 0.9438 - val_loss: 0.2745 - val_accuracy: 0.9180\n","Epoch 41/50\n","129/130 [============================>.] - ETA: 0s - loss: 0.2188 - accuracy: 0.9443\n","Epoch 41: val_accuracy improved from 0.91837 to 0.92053, saving model to best_model.h5\n","130/130 [==============================] - 1s 12ms/step - loss: 0.2186 - accuracy: 0.9443 - val_loss: 0.2695 - val_accuracy: 0.9205\n","Epoch 42/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.2187 - accuracy: 0.9434\n","Epoch 42: val_accuracy did not improve from 0.92053\n","130/130 [==============================] - 1s 11ms/step - loss: 0.2182 - accuracy: 0.9437 - val_loss: 0.2718 - val_accuracy: 0.9192\n","Epoch 43/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.9438\n","Epoch 43: val_accuracy did not improve from 0.92053\n","130/130 [==============================] - 1s 10ms/step - loss: 0.2175 - accuracy: 0.9440 - val_loss: 0.2707 - val_accuracy: 0.9197\n","Epoch 44/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.2160 - accuracy: 0.9452\n","Epoch 44: val_accuracy improved from 0.92053 to 0.92101, saving model to best_model.h5\n","130/130 [==============================] - 1s 9ms/step - loss: 0.2155 - accuracy: 0.9456 - val_loss: 0.2687 - val_accuracy: 0.9210\n","Epoch 45/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.2138 - accuracy: 0.9456\n","Epoch 45: val_accuracy did not improve from 0.92101\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2137 - accuracy: 0.9457 - val_loss: 0.2680 - val_accuracy: 0.9205\n","Epoch 46/50\n","126/130 [============================>.] - ETA: 0s - loss: 0.2154 - accuracy: 0.9454\n","Epoch 46: val_accuracy improved from 0.92101 to 0.92191, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2143 - accuracy: 0.9456 - val_loss: 0.2655 - val_accuracy: 0.9219\n","Epoch 47/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.2133 - accuracy: 0.9454\n","Epoch 47: val_accuracy improved from 0.92191 to 0.92304, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2125 - accuracy: 0.9460 - val_loss: 0.2659 - val_accuracy: 0.9230\n","Epoch 48/50\n","128/130 [============================>.] - ETA: 0s - loss: 0.2133 - accuracy: 0.9457\n","Epoch 48: val_accuracy did not improve from 0.92304\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2134 - accuracy: 0.9454 - val_loss: 0.2646 - val_accuracy: 0.9226\n","Epoch 49/50\n","123/130 [===========================>..] - ETA: 0s - loss: 0.2088 - accuracy: 0.9480\n","Epoch 49: val_accuracy improved from 0.92304 to 0.92436, saving model to best_model.h5\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2096 - accuracy: 0.9480 - val_loss: 0.2611 - val_accuracy: 0.9244\n","Epoch 50/50\n","130/130 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9463\n","Epoch 50: val_accuracy did not improve from 0.92436\n","130/130 [==============================] - 1s 8ms/step - loss: 0.2115 - accuracy: 0.9463 - val_loss: 0.2696 - val_accuracy: 0.9168\n"]}],"source":["kfold = KFold(n_splits=5, shuffle=True)\n","\n","for fold, (train_indices, val_indices) in enumerate(kfold.split(X_train)):\n","    print(f\"Fold {fold+1}\")\n","    model.fit(X_train, y_train, validation_split=0.2, epochs=50, callbacks=[checkpoint_callback])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1686347882040,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"},"user_tz":-60},"id":"NB4CfklPPaB1","outputId":"69e07fad-3afb-4225-a347-d19ee8cc0881"},"outputs":[{"output_type":"stream","name":"stdout","text":["(5200, 150, 11) (1300, 150, 11)\n"]}],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n","print(X_train.shape, X_test.shape)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2K_Q744oQXgD","outputId":"94307d7b-7132-4f18-b4fb-eba303f5d816","executionInfo":{"status":"ok","timestamp":1686348079755,"user_tz":-60,"elapsed":197724,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.2171 - accuracy: 0.9429\n","Epoch 1: val_accuracy did not improve from 0.92436\n","147/147 [==============================] - 2s 8ms/step - loss: 0.2176 - accuracy: 0.9427 - val_loss: 0.2602 - val_accuracy: 0.9243\n","Epoch 2/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.2125 - accuracy: 0.9449\n","Epoch 2: val_accuracy did not improve from 0.92436\n","147/147 [==============================] - 2s 11ms/step - loss: 0.2141 - accuracy: 0.9441 - val_loss: 0.2622 - val_accuracy: 0.9240\n","Epoch 3/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.2137 - accuracy: 0.9448\n","Epoch 3: val_accuracy improved from 0.92436 to 0.92799, saving model to best_model.h5\n","147/147 [==============================] - 2s 12ms/step - loss: 0.2132 - accuracy: 0.9448 - val_loss: 0.2562 - val_accuracy: 0.9280\n","Epoch 4/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.2077 - accuracy: 0.9473\n","Epoch 4: val_accuracy did not improve from 0.92799\n","147/147 [==============================] - 1s 8ms/step - loss: 0.2087 - accuracy: 0.9468 - val_loss: 0.2561 - val_accuracy: 0.9247\n","Epoch 5/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.2132 - accuracy: 0.9440\n","Epoch 5: val_accuracy did not improve from 0.92799\n","147/147 [==============================] - 1s 10ms/step - loss: 0.2125 - accuracy: 0.9444 - val_loss: 0.2565 - val_accuracy: 0.9243\n","Epoch 6/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.2094 - accuracy: 0.9466\n","Epoch 6: val_accuracy did not improve from 0.92799\n","147/147 [==============================] - 2s 11ms/step - loss: 0.2083 - accuracy: 0.9468 - val_loss: 0.2570 - val_accuracy: 0.9238\n","Epoch 7/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.2054 - accuracy: 0.9478\n","Epoch 7: val_accuracy did not improve from 0.92799\n","147/147 [==============================] - 1s 9ms/step - loss: 0.2053 - accuracy: 0.9479 - val_loss: 0.2512 - val_accuracy: 0.9274\n","Epoch 8/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.2030 - accuracy: 0.9481\n","Epoch 8: val_accuracy did not improve from 0.92799\n","147/147 [==============================] - 1s 8ms/step - loss: 0.2037 - accuracy: 0.9480 - val_loss: 0.2513 - val_accuracy: 0.9271\n","Epoch 9/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.2057 - accuracy: 0.9471\n","Epoch 9: val_accuracy did not improve from 0.92799\n","147/147 [==============================] - 1s 8ms/step - loss: 0.2073 - accuracy: 0.9468 - val_loss: 0.2512 - val_accuracy: 0.9254\n","Epoch 10/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.2024 - accuracy: 0.9487\n","Epoch 10: val_accuracy did not improve from 0.92799\n","147/147 [==============================] - 1s 8ms/step - loss: 0.2027 - accuracy: 0.9488 - val_loss: 0.2533 - val_accuracy: 0.9253\n","Epoch 11/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9489\n","Epoch 11: val_accuracy improved from 0.92799 to 0.92878, saving model to best_model.h5\n","147/147 [==============================] - 1s 9ms/step - loss: 0.2008 - accuracy: 0.9496 - val_loss: 0.2469 - val_accuracy: 0.9288\n","Epoch 12/150\n","147/147 [==============================] - ETA: 0s - loss: 0.2002 - accuracy: 0.9494\n","Epoch 12: val_accuracy did not improve from 0.92878\n","147/147 [==============================] - 2s 11ms/step - loss: 0.2002 - accuracy: 0.9494 - val_loss: 0.2470 - val_accuracy: 0.9279\n","Epoch 13/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9515\n","Epoch 13: val_accuracy did not improve from 0.92878\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1958 - accuracy: 0.9513 - val_loss: 0.2458 - val_accuracy: 0.9278\n","Epoch 14/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.2038 - accuracy: 0.9470\n","Epoch 14: val_accuracy did not improve from 0.92878\n","147/147 [==============================] - 1s 8ms/step - loss: 0.2041 - accuracy: 0.9471 - val_loss: 0.2498 - val_accuracy: 0.9259\n","Epoch 15/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1977 - accuracy: 0.9499\n","Epoch 15: val_accuracy did not improve from 0.92878\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1976 - accuracy: 0.9501 - val_loss: 0.2450 - val_accuracy: 0.9275\n","Epoch 16/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.9489\n","Epoch 16: val_accuracy did not improve from 0.92878\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1976 - accuracy: 0.9489 - val_loss: 0.2468 - val_accuracy: 0.9275\n","Epoch 17/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9506\n","Epoch 17: val_accuracy improved from 0.92878 to 0.93182, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1939 - accuracy: 0.9505 - val_loss: 0.2422 - val_accuracy: 0.9318\n","Epoch 18/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1919 - accuracy: 0.9520\n","Epoch 18: val_accuracy did not improve from 0.93182\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1919 - accuracy: 0.9520 - val_loss: 0.2415 - val_accuracy: 0.9301\n","Epoch 19/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1918 - accuracy: 0.9520\n","Epoch 19: val_accuracy improved from 0.93182 to 0.93313, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1925 - accuracy: 0.9514 - val_loss: 0.2422 - val_accuracy: 0.9331\n","Epoch 20/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1916 - accuracy: 0.9507\n","Epoch 20: val_accuracy did not improve from 0.93313\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1914 - accuracy: 0.9508 - val_loss: 0.2448 - val_accuracy: 0.9285\n","Epoch 21/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.1900 - accuracy: 0.9508\n","Epoch 21: val_accuracy did not improve from 0.93313\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1898 - accuracy: 0.9512 - val_loss: 0.2461 - val_accuracy: 0.9278\n","Epoch 22/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1913 - accuracy: 0.9513\n","Epoch 22: val_accuracy did not improve from 0.93313\n","147/147 [==============================] - 1s 9ms/step - loss: 0.1917 - accuracy: 0.9511 - val_loss: 0.2397 - val_accuracy: 0.9291\n","Epoch 23/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.9527\n","Epoch 23: val_accuracy did not improve from 0.93313\n","147/147 [==============================] - 2s 12ms/step - loss: 0.1872 - accuracy: 0.9527 - val_loss: 0.2404 - val_accuracy: 0.9276\n","Epoch 24/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9516\n","Epoch 24: val_accuracy did not improve from 0.93313\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1899 - accuracy: 0.9511 - val_loss: 0.2393 - val_accuracy: 0.9284\n","Epoch 25/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1891 - accuracy: 0.9518\n","Epoch 25: val_accuracy did not improve from 0.93313\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1890 - accuracy: 0.9519 - val_loss: 0.2472 - val_accuracy: 0.9250\n","Epoch 26/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1870 - accuracy: 0.9519\n","Epoch 26: val_accuracy did not improve from 0.93313\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1875 - accuracy: 0.9515 - val_loss: 0.2319 - val_accuracy: 0.9327\n","Epoch 27/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1837 - accuracy: 0.9531\n","Epoch 27: val_accuracy improved from 0.93313 to 0.93317, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1844 - accuracy: 0.9527 - val_loss: 0.2335 - val_accuracy: 0.9332\n","Epoch 28/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1914 - accuracy: 0.9479\n","Epoch 28: val_accuracy did not improve from 0.93317\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1924 - accuracy: 0.9477 - val_loss: 0.2368 - val_accuracy: 0.9303\n","Epoch 29/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9507\n","Epoch 29: val_accuracy did not improve from 0.93317\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1856 - accuracy: 0.9507 - val_loss: 0.2309 - val_accuracy: 0.9320\n","Epoch 30/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9539\n","Epoch 30: val_accuracy improved from 0.93317 to 0.93333, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1815 - accuracy: 0.9539 - val_loss: 0.2293 - val_accuracy: 0.9333\n","Epoch 31/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1812 - accuracy: 0.9534\n","Epoch 31: val_accuracy improved from 0.93333 to 0.93574, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1802 - accuracy: 0.9538 - val_loss: 0.2258 - val_accuracy: 0.9357\n","Epoch 32/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.1813 - accuracy: 0.9533\n","Epoch 32: val_accuracy did not improve from 0.93574\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1798 - accuracy: 0.9541 - val_loss: 0.2241 - val_accuracy: 0.9348\n","Epoch 33/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1752 - accuracy: 0.9567\n","Epoch 33: val_accuracy did not improve from 0.93574\n","147/147 [==============================] - 1s 10ms/step - loss: 0.1760 - accuracy: 0.9565 - val_loss: 0.2240 - val_accuracy: 0.9349\n","Epoch 34/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1768 - accuracy: 0.9551\n","Epoch 34: val_accuracy improved from 0.93574 to 0.93592, saving model to best_model.h5\n","147/147 [==============================] - 2s 12ms/step - loss: 0.1768 - accuracy: 0.9551 - val_loss: 0.2218 - val_accuracy: 0.9359\n","Epoch 35/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.9539\n","Epoch 35: val_accuracy improved from 0.93592 to 0.93635, saving model to best_model.h5\n","147/147 [==============================] - 1s 10ms/step - loss: 0.1780 - accuracy: 0.9540 - val_loss: 0.2194 - val_accuracy: 0.9363\n","Epoch 36/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1798 - accuracy: 0.9527\n","Epoch 36: val_accuracy did not improve from 0.93635\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1798 - accuracy: 0.9527 - val_loss: 0.2357 - val_accuracy: 0.9225\n","Epoch 37/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.1785 - accuracy: 0.9520\n","Epoch 37: val_accuracy did not improve from 0.93635\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1800 - accuracy: 0.9517 - val_loss: 0.2381 - val_accuracy: 0.9196\n","Epoch 38/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1799 - accuracy: 0.9522\n","Epoch 38: val_accuracy did not improve from 0.93635\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1789 - accuracy: 0.9524 - val_loss: 0.2217 - val_accuracy: 0.9344\n","Epoch 39/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9562\n","Epoch 39: val_accuracy did not improve from 0.93635\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1733 - accuracy: 0.9559 - val_loss: 0.2214 - val_accuracy: 0.9346\n","Epoch 40/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.1723 - accuracy: 0.9563\n","Epoch 40: val_accuracy did not improve from 0.93635\n","147/147 [==============================] - 1s 7ms/step - loss: 0.1720 - accuracy: 0.9562 - val_loss: 0.2213 - val_accuracy: 0.9362\n","Epoch 41/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1697 - accuracy: 0.9564\n","Epoch 41: val_accuracy improved from 0.93635 to 0.93792, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1713 - accuracy: 0.9559 - val_loss: 0.2189 - val_accuracy: 0.9379\n","Epoch 42/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9512\n","Epoch 42: val_accuracy did not improve from 0.93792\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1822 - accuracy: 0.9513 - val_loss: 0.2223 - val_accuracy: 0.9346\n","Epoch 43/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1704 - accuracy: 0.9569\n","Epoch 43: val_accuracy improved from 0.93792 to 0.93878, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1703 - accuracy: 0.9569 - val_loss: 0.2162 - val_accuracy: 0.9388\n","Epoch 44/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1675 - accuracy: 0.9581\n","Epoch 44: val_accuracy improved from 0.93878 to 0.93946, saving model to best_model.h5\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1674 - accuracy: 0.9582 - val_loss: 0.2143 - val_accuracy: 0.9395\n","Epoch 45/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1673 - accuracy: 0.9584\n","Epoch 45: val_accuracy improved from 0.93946 to 0.94038, saving model to best_model.h5\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1672 - accuracy: 0.9584 - val_loss: 0.2116 - val_accuracy: 0.9404\n","Epoch 46/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1659 - accuracy: 0.9585\n","Epoch 46: val_accuracy did not improve from 0.94038\n","147/147 [==============================] - 1s 9ms/step - loss: 0.1654 - accuracy: 0.9588 - val_loss: 0.2153 - val_accuracy: 0.9375\n","Epoch 47/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1712 - accuracy: 0.9551\n","Epoch 47: val_accuracy did not improve from 0.94038\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1711 - accuracy: 0.9551 - val_loss: 0.2148 - val_accuracy: 0.9403\n","Epoch 48/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9580\n","Epoch 48: val_accuracy did not improve from 0.94038\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1665 - accuracy: 0.9580 - val_loss: 0.2123 - val_accuracy: 0.9399\n","Epoch 49/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.9586\n","Epoch 49: val_accuracy did not improve from 0.94038\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1649 - accuracy: 0.9587 - val_loss: 0.2161 - val_accuracy: 0.9356\n","Epoch 50/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1636 - accuracy: 0.9585\n","Epoch 50: val_accuracy did not improve from 0.94038\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1644 - accuracy: 0.9577 - val_loss: 0.2078 - val_accuracy: 0.9381\n","Epoch 51/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1619 - accuracy: 0.9590\n","Epoch 51: val_accuracy did not improve from 0.94038\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1619 - accuracy: 0.9590 - val_loss: 0.2062 - val_accuracy: 0.9394\n","Epoch 52/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1622 - accuracy: 0.9579\n","Epoch 52: val_accuracy improved from 0.94038 to 0.94086, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1621 - accuracy: 0.9581 - val_loss: 0.2078 - val_accuracy: 0.9409\n","Epoch 53/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1600 - accuracy: 0.9597\n","Epoch 53: val_accuracy improved from 0.94086 to 0.94181, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1600 - accuracy: 0.9597 - val_loss: 0.2067 - val_accuracy: 0.9418\n","Epoch 54/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.9601\n","Epoch 54: val_accuracy did not improve from 0.94181\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1576 - accuracy: 0.9602 - val_loss: 0.2091 - val_accuracy: 0.9392\n","Epoch 55/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1589 - accuracy: 0.9590\n","Epoch 55: val_accuracy improved from 0.94181 to 0.94186, saving model to best_model.h5\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1590 - accuracy: 0.9587 - val_loss: 0.2052 - val_accuracy: 0.9419\n","Epoch 56/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1564 - accuracy: 0.9612\n","Epoch 56: val_accuracy did not improve from 0.94186\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1566 - accuracy: 0.9611 - val_loss: 0.2032 - val_accuracy: 0.9405\n","Epoch 57/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1570 - accuracy: 0.9594\n","Epoch 57: val_accuracy did not improve from 0.94186\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1572 - accuracy: 0.9596 - val_loss: 0.2047 - val_accuracy: 0.9416\n","Epoch 58/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1557 - accuracy: 0.9611\n","Epoch 58: val_accuracy improved from 0.94186 to 0.94295, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1547 - accuracy: 0.9612 - val_loss: 0.2019 - val_accuracy: 0.9429\n","Epoch 59/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1561 - accuracy: 0.9599\n","Epoch 59: val_accuracy did not improve from 0.94295\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1557 - accuracy: 0.9600 - val_loss: 0.2060 - val_accuracy: 0.9413\n","Epoch 60/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1541 - accuracy: 0.9606\n","Epoch 60: val_accuracy did not improve from 0.94295\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1536 - accuracy: 0.9608 - val_loss: 0.2032 - val_accuracy: 0.9425\n","Epoch 61/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9599\n","Epoch 61: val_accuracy did not improve from 0.94295\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1542 - accuracy: 0.9599 - val_loss: 0.2177 - val_accuracy: 0.9349\n","Epoch 62/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.9598\n","Epoch 62: val_accuracy did not improve from 0.94295\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1574 - accuracy: 0.9598 - val_loss: 0.2066 - val_accuracy: 0.9395\n","Epoch 63/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1539 - accuracy: 0.9601\n","Epoch 63: val_accuracy did not improve from 0.94295\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1532 - accuracy: 0.9604 - val_loss: 0.2174 - val_accuracy: 0.9356\n","Epoch 64/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1573 - accuracy: 0.9594\n","Epoch 64: val_accuracy did not improve from 0.94295\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1573 - accuracy: 0.9595 - val_loss: 0.2067 - val_accuracy: 0.9395\n","Epoch 65/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1525 - accuracy: 0.9610\n","Epoch 65: val_accuracy did not improve from 0.94295\n","147/147 [==============================] - 1s 9ms/step - loss: 0.1524 - accuracy: 0.9611 - val_loss: 0.1998 - val_accuracy: 0.9427\n","Epoch 66/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1497 - accuracy: 0.9623\n","Epoch 66: val_accuracy did not improve from 0.94295\n","147/147 [==============================] - 2s 12ms/step - loss: 0.1499 - accuracy: 0.9623 - val_loss: 0.1969 - val_accuracy: 0.9417\n","Epoch 67/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1493 - accuracy: 0.9625\n","Epoch 67: val_accuracy did not improve from 0.94295\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1495 - accuracy: 0.9625 - val_loss: 0.2089 - val_accuracy: 0.9395\n","Epoch 68/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1487 - accuracy: 0.9634\n","Epoch 68: val_accuracy improved from 0.94295 to 0.94549, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1487 - accuracy: 0.9634 - val_loss: 0.2001 - val_accuracy: 0.9455\n","Epoch 69/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.9634\n","Epoch 69: val_accuracy did not improve from 0.94549\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1464 - accuracy: 0.9637 - val_loss: 0.2015 - val_accuracy: 0.9433\n","Epoch 70/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.1438 - accuracy: 0.9646\n","Epoch 70: val_accuracy did not improve from 0.94549\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1455 - accuracy: 0.9638 - val_loss: 0.1987 - val_accuracy: 0.9437\n","Epoch 71/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1498 - accuracy: 0.9625\n","Epoch 71: val_accuracy did not improve from 0.94549\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1496 - accuracy: 0.9625 - val_loss: 0.2054 - val_accuracy: 0.9440\n","Epoch 72/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1486 - accuracy: 0.9625\n","Epoch 72: val_accuracy did not improve from 0.94549\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1486 - accuracy: 0.9625 - val_loss: 0.2038 - val_accuracy: 0.9438\n","Epoch 73/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1457 - accuracy: 0.9635\n","Epoch 73: val_accuracy did not improve from 0.94549\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1457 - accuracy: 0.9635 - val_loss: 0.1982 - val_accuracy: 0.9454\n","Epoch 74/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1470 - accuracy: 0.9627\n","Epoch 74: val_accuracy did not improve from 0.94549\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1479 - accuracy: 0.9628 - val_loss: 0.1987 - val_accuracy: 0.9426\n","Epoch 75/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1509 - accuracy: 0.9607\n","Epoch 75: val_accuracy did not improve from 0.94549\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1503 - accuracy: 0.9611 - val_loss: 0.1959 - val_accuracy: 0.9448\n","Epoch 76/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1458 - accuracy: 0.9626\n","Epoch 76: val_accuracy did not improve from 0.94549\n","147/147 [==============================] - 2s 10ms/step - loss: 0.1461 - accuracy: 0.9625 - val_loss: 0.2072 - val_accuracy: 0.9393\n","Epoch 77/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1464 - accuracy: 0.9622\n","Epoch 77: val_accuracy did not improve from 0.94549\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1458 - accuracy: 0.9624 - val_loss: 0.1963 - val_accuracy: 0.9444\n","Epoch 78/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1456 - accuracy: 0.9627\n","Epoch 78: val_accuracy improved from 0.94549 to 0.94582, saving model to best_model.h5\n","147/147 [==============================] - 1s 10ms/step - loss: 0.1456 - accuracy: 0.9627 - val_loss: 0.1976 - val_accuracy: 0.9458\n","Epoch 79/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1443 - accuracy: 0.9635\n","Epoch 79: val_accuracy did not improve from 0.94582\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1440 - accuracy: 0.9637 - val_loss: 0.2033 - val_accuracy: 0.9413\n","Epoch 80/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.1426 - accuracy: 0.9646\n","Epoch 80: val_accuracy did not improve from 0.94582\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1435 - accuracy: 0.9640 - val_loss: 0.1924 - val_accuracy: 0.9439\n","Epoch 81/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1420 - accuracy: 0.9643\n","Epoch 81: val_accuracy did not improve from 0.94582\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1428 - accuracy: 0.9636 - val_loss: 0.1934 - val_accuracy: 0.9452\n","Epoch 82/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.9652\n","Epoch 82: val_accuracy improved from 0.94582 to 0.94835, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1393 - accuracy: 0.9654 - val_loss: 0.1899 - val_accuracy: 0.9483\n","Epoch 83/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.9657\n","Epoch 83: val_accuracy improved from 0.94835 to 0.94855, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1370 - accuracy: 0.9657 - val_loss: 0.1888 - val_accuracy: 0.9486\n","Epoch 84/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1351 - accuracy: 0.9665\n","Epoch 84: val_accuracy improved from 0.94855 to 0.94874, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1354 - accuracy: 0.9664 - val_loss: 0.1876 - val_accuracy: 0.9487\n","Epoch 85/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.9669\n","Epoch 85: val_accuracy did not improve from 0.94874\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1350 - accuracy: 0.9668 - val_loss: 0.1858 - val_accuracy: 0.9485\n","Epoch 86/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1341 - accuracy: 0.9666\n","Epoch 86: val_accuracy improved from 0.94874 to 0.94958, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1341 - accuracy: 0.9666 - val_loss: 0.1851 - val_accuracy: 0.9496\n","Epoch 87/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1336 - accuracy: 0.9670\n","Epoch 87: val_accuracy improved from 0.94958 to 0.94978, saving model to best_model.h5\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1336 - accuracy: 0.9671 - val_loss: 0.1851 - val_accuracy: 0.9498\n","Epoch 88/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1347 - accuracy: 0.9664\n","Epoch 88: val_accuracy did not improve from 0.94978\n","147/147 [==============================] - 2s 12ms/step - loss: 0.1347 - accuracy: 0.9664 - val_loss: 0.1840 - val_accuracy: 0.9487\n","Epoch 89/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1324 - accuracy: 0.9673\n","Epoch 89: val_accuracy improved from 0.94978 to 0.94987, saving model to best_model.h5\n","147/147 [==============================] - 1s 9ms/step - loss: 0.1315 - accuracy: 0.9678 - val_loss: 0.1848 - val_accuracy: 0.9499\n","Epoch 90/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1322 - accuracy: 0.9681\n","Epoch 90: val_accuracy did not improve from 0.94987\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1318 - accuracy: 0.9679 - val_loss: 0.1874 - val_accuracy: 0.9486\n","Epoch 91/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9674\n","Epoch 91: val_accuracy improved from 0.94987 to 0.95045, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1309 - accuracy: 0.9675 - val_loss: 0.1823 - val_accuracy: 0.9504\n","Epoch 92/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.1315 - accuracy: 0.9670\n","Epoch 92: val_accuracy did not improve from 0.95045\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1318 - accuracy: 0.9670 - val_loss: 0.1820 - val_accuracy: 0.9501\n","Epoch 93/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1314 - accuracy: 0.9670\n","Epoch 93: val_accuracy did not improve from 0.95045\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1316 - accuracy: 0.9671 - val_loss: 0.1817 - val_accuracy: 0.9486\n","Epoch 94/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1299 - accuracy: 0.9683\n","Epoch 94: val_accuracy improved from 0.95045 to 0.95046, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1298 - accuracy: 0.9684 - val_loss: 0.1808 - val_accuracy: 0.9505\n","Epoch 95/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1341 - accuracy: 0.9672\n","Epoch 95: val_accuracy improved from 0.95046 to 0.95137, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1341 - accuracy: 0.9669 - val_loss: 0.1830 - val_accuracy: 0.9514\n","Epoch 96/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1288 - accuracy: 0.9680\n","Epoch 96: val_accuracy improved from 0.95137 to 0.95149, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1288 - accuracy: 0.9680 - val_loss: 0.1785 - val_accuracy: 0.9515\n","Epoch 97/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9679\n","Epoch 97: val_accuracy did not improve from 0.95149\n","147/147 [==============================] - 1s 10ms/step - loss: 0.1284 - accuracy: 0.9682 - val_loss: 0.1777 - val_accuracy: 0.9507\n","Epoch 98/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9691\n","Epoch 98: val_accuracy did not improve from 0.95149\n","147/147 [==============================] - 2s 12ms/step - loss: 0.1277 - accuracy: 0.9691 - val_loss: 0.1854 - val_accuracy: 0.9493\n","Epoch 99/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1276 - accuracy: 0.9689\n","Epoch 99: val_accuracy did not improve from 0.95149\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1277 - accuracy: 0.9689 - val_loss: 0.1807 - val_accuracy: 0.9511\n","Epoch 100/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1283 - accuracy: 0.9682\n","Epoch 100: val_accuracy did not improve from 0.95149\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1291 - accuracy: 0.9674 - val_loss: 0.1801 - val_accuracy: 0.9505\n","Epoch 101/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9684\n","Epoch 101: val_accuracy improved from 0.95149 to 0.95215, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1273 - accuracy: 0.9682 - val_loss: 0.1778 - val_accuracy: 0.9522\n","Epoch 102/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1259 - accuracy: 0.9685\n","Epoch 102: val_accuracy improved from 0.95215 to 0.95224, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1246 - accuracy: 0.9691 - val_loss: 0.1735 - val_accuracy: 0.9522\n","Epoch 103/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9700\n","Epoch 103: val_accuracy improved from 0.95224 to 0.95401, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1221 - accuracy: 0.9702 - val_loss: 0.1689 - val_accuracy: 0.9540\n","Epoch 104/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9703\n","Epoch 104: val_accuracy did not improve from 0.95401\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1220 - accuracy: 0.9700 - val_loss: 0.1725 - val_accuracy: 0.9494\n","Epoch 105/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9698\n","Epoch 105: val_accuracy did not improve from 0.95401\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1234 - accuracy: 0.9696 - val_loss: 0.1720 - val_accuracy: 0.9506\n","Epoch 106/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9694\n","Epoch 106: val_accuracy did not improve from 0.95401\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1223 - accuracy: 0.9694 - val_loss: 0.1763 - val_accuracy: 0.9490\n","Epoch 107/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1308 - accuracy: 0.9673\n","Epoch 107: val_accuracy did not improve from 0.95401\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1299 - accuracy: 0.9676 - val_loss: 0.1693 - val_accuracy: 0.9517\n","Epoch 108/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9693\n","Epoch 108: val_accuracy did not improve from 0.95401\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1239 - accuracy: 0.9692 - val_loss: 0.1667 - val_accuracy: 0.9536\n","Epoch 109/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9698\n","Epoch 109: val_accuracy did not improve from 0.95401\n","147/147 [==============================] - 2s 12ms/step - loss: 0.1213 - accuracy: 0.9697 - val_loss: 0.1709 - val_accuracy: 0.9525\n","Epoch 110/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1202 - accuracy: 0.9697\n","Epoch 110: val_accuracy did not improve from 0.95401\n","147/147 [==============================] - 1s 9ms/step - loss: 0.1200 - accuracy: 0.9699 - val_loss: 0.1692 - val_accuracy: 0.9522\n","Epoch 111/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1234 - accuracy: 0.9679\n","Epoch 111: val_accuracy did not improve from 0.95401\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1231 - accuracy: 0.9680 - val_loss: 0.1660 - val_accuracy: 0.9528\n","Epoch 112/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9705\n","Epoch 112: val_accuracy did not improve from 0.95401\n","147/147 [==============================] - 1s 9ms/step - loss: 0.1184 - accuracy: 0.9705 - val_loss: 0.1696 - val_accuracy: 0.9535\n","Epoch 113/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9702\n","Epoch 113: val_accuracy improved from 0.95401 to 0.95423, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1205 - accuracy: 0.9698 - val_loss: 0.1630 - val_accuracy: 0.9542\n","Epoch 114/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9715\n","Epoch 114: val_accuracy improved from 0.95423 to 0.95526, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1177 - accuracy: 0.9714 - val_loss: 0.1588 - val_accuracy: 0.9553\n","Epoch 115/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.1210 - accuracy: 0.9697\n","Epoch 115: val_accuracy did not improve from 0.95526\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1201 - accuracy: 0.9701 - val_loss: 0.1696 - val_accuracy: 0.9508\n","Epoch 116/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1203 - accuracy: 0.9696\n","Epoch 116: val_accuracy improved from 0.95526 to 0.95547, saving model to best_model.h5\n","147/147 [==============================] - 1s 9ms/step - loss: 0.1197 - accuracy: 0.9698 - val_loss: 0.1659 - val_accuracy: 0.9555\n","Epoch 117/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1158 - accuracy: 0.9719\n","Epoch 117: val_accuracy did not improve from 0.95547\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1168 - accuracy: 0.9715 - val_loss: 0.1669 - val_accuracy: 0.9520\n","Epoch 118/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9666\n","Epoch 118: val_accuracy did not improve from 0.95547\n","147/147 [==============================] - 1s 10ms/step - loss: 0.1271 - accuracy: 0.9667 - val_loss: 0.1807 - val_accuracy: 0.9433\n","Epoch 119/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9680\n","Epoch 119: val_accuracy did not improve from 0.95547\n","147/147 [==============================] - 2s 12ms/step - loss: 0.1194 - accuracy: 0.9680 - val_loss: 0.1591 - val_accuracy: 0.9552\n","Epoch 120/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9702\n","Epoch 120: val_accuracy did not improve from 0.95547\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1165 - accuracy: 0.9703 - val_loss: 0.1742 - val_accuracy: 0.9494\n","Epoch 121/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1135 - accuracy: 0.9716\n","Epoch 121: val_accuracy did not improve from 0.95547\n","147/147 [==============================] - 1s 9ms/step - loss: 0.1134 - accuracy: 0.9716 - val_loss: 0.1590 - val_accuracy: 0.9548\n","Epoch 122/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9719\n","Epoch 122: val_accuracy improved from 0.95547 to 0.95703, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1120 - accuracy: 0.9720 - val_loss: 0.1568 - val_accuracy: 0.9570\n","Epoch 123/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1119 - accuracy: 0.9725\n","Epoch 123: val_accuracy did not improve from 0.95703\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1126 - accuracy: 0.9721 - val_loss: 0.1573 - val_accuracy: 0.9561\n","Epoch 124/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9724\n","Epoch 124: val_accuracy did not improve from 0.95703\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1116 - accuracy: 0.9726 - val_loss: 0.1545 - val_accuracy: 0.9569\n","Epoch 125/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.1100 - accuracy: 0.9722\n","Epoch 125: val_accuracy improved from 0.95703 to 0.95722, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1102 - accuracy: 0.9725 - val_loss: 0.1552 - val_accuracy: 0.9572\n","Epoch 126/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1131 - accuracy: 0.9716\n","Epoch 126: val_accuracy improved from 0.95722 to 0.95754, saving model to best_model.h5\n","147/147 [==============================] - 1s 9ms/step - loss: 0.1133 - accuracy: 0.9714 - val_loss: 0.1655 - val_accuracy: 0.9575\n","Epoch 127/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.1121 - accuracy: 0.9724\n","Epoch 127: val_accuracy did not improve from 0.95754\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1129 - accuracy: 0.9719 - val_loss: 0.1608 - val_accuracy: 0.9551\n","Epoch 128/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9724\n","Epoch 128: val_accuracy did not improve from 0.95754\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1094 - accuracy: 0.9723 - val_loss: 0.1596 - val_accuracy: 0.9548\n","Epoch 129/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9731\n","Epoch 129: val_accuracy did not improve from 0.95754\n","147/147 [==============================] - 2s 12ms/step - loss: 0.1084 - accuracy: 0.9731 - val_loss: 0.1538 - val_accuracy: 0.9561\n","Epoch 130/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1067 - accuracy: 0.9737\n","Epoch 130: val_accuracy did not improve from 0.95754\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1069 - accuracy: 0.9737 - val_loss: 0.1563 - val_accuracy: 0.9564\n","Epoch 131/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9732\n","Epoch 131: val_accuracy did not improve from 0.95754\n","147/147 [==============================] - 1s 9ms/step - loss: 0.1074 - accuracy: 0.9733 - val_loss: 0.1564 - val_accuracy: 0.9569\n","Epoch 132/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9738\n","Epoch 132: val_accuracy did not improve from 0.95754\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1065 - accuracy: 0.9738 - val_loss: 0.1534 - val_accuracy: 0.9575\n","Epoch 133/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1054 - accuracy: 0.9741\n","Epoch 133: val_accuracy did not improve from 0.95754\n","147/147 [==============================] - 2s 12ms/step - loss: 0.1053 - accuracy: 0.9742 - val_loss: 0.1535 - val_accuracy: 0.9572\n","Epoch 134/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9729\n","Epoch 134: val_accuracy improved from 0.95754 to 0.95768, saving model to best_model.h5\n","147/147 [==============================] - 2s 12ms/step - loss: 0.1071 - accuracy: 0.9729 - val_loss: 0.1533 - val_accuracy: 0.9577\n","Epoch 135/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9733\n","Epoch 135: val_accuracy improved from 0.95768 to 0.95972, saving model to best_model.h5\n","147/147 [==============================] - 1s 10ms/step - loss: 0.1085 - accuracy: 0.9733 - val_loss: 0.1504 - val_accuracy: 0.9597\n","Epoch 136/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9730\n","Epoch 136: val_accuracy did not improve from 0.95972\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1075 - accuracy: 0.9730 - val_loss: 0.1582 - val_accuracy: 0.9557\n","Epoch 137/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9732\n","Epoch 137: val_accuracy did not improve from 0.95972\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1068 - accuracy: 0.9732 - val_loss: 0.1516 - val_accuracy: 0.9579\n","Epoch 138/150\n","143/147 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9745\n","Epoch 138: val_accuracy improved from 0.95972 to 0.96018, saving model to best_model.h5\n","147/147 [==============================] - 1s 10ms/step - loss: 0.1046 - accuracy: 0.9743 - val_loss: 0.1490 - val_accuracy: 0.9602\n","Epoch 139/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9747\n","Epoch 139: val_accuracy did not improve from 0.96018\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1033 - accuracy: 0.9747 - val_loss: 0.1525 - val_accuracy: 0.9584\n","Epoch 140/150\n","145/147 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9729\n","Epoch 140: val_accuracy did not improve from 0.96018\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1079 - accuracy: 0.9730 - val_loss: 0.1568 - val_accuracy: 0.9571\n","Epoch 141/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1052 - accuracy: 0.9740\n","Epoch 141: val_accuracy did not improve from 0.96018\n","147/147 [==============================] - 1s 9ms/step - loss: 0.1052 - accuracy: 0.9740 - val_loss: 0.1543 - val_accuracy: 0.9594\n","Epoch 142/150\n","140/147 [===========================>..] - ETA: 0s - loss: 0.1038 - accuracy: 0.9743\n","Epoch 142: val_accuracy did not improve from 0.96018\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1035 - accuracy: 0.9747 - val_loss: 0.1497 - val_accuracy: 0.9601\n","Epoch 143/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9731\n","Epoch 143: val_accuracy did not improve from 0.96018\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1074 - accuracy: 0.9731 - val_loss: 0.1472 - val_accuracy: 0.9595\n","Epoch 144/150\n","142/147 [===========================>..] - ETA: 0s - loss: 0.1014 - accuracy: 0.9746\n","Epoch 144: val_accuracy did not improve from 0.96018\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1020 - accuracy: 0.9746 - val_loss: 0.1507 - val_accuracy: 0.9598\n","Epoch 145/150\n","141/147 [===========================>..] - ETA: 0s - loss: 0.0990 - accuracy: 0.9760\n","Epoch 145: val_accuracy improved from 0.96018 to 0.96201, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1014 - accuracy: 0.9754 - val_loss: 0.1457 - val_accuracy: 0.9620\n","Epoch 146/150\n","147/147 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9690\n","Epoch 146: val_accuracy improved from 0.96201 to 0.96209, saving model to best_model.h5\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1189 - accuracy: 0.9690 - val_loss: 0.1538 - val_accuracy: 0.9621\n","Epoch 147/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9719\n","Epoch 147: val_accuracy did not improve from 0.96209\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1130 - accuracy: 0.9717 - val_loss: 0.1634 - val_accuracy: 0.9535\n","Epoch 148/150\n","146/147 [============================>.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9691\n","Epoch 148: val_accuracy did not improve from 0.96209\n","147/147 [==============================] - 1s 8ms/step - loss: 0.1188 - accuracy: 0.9691 - val_loss: 0.1682 - val_accuracy: 0.9532\n","Epoch 149/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9731\n","Epoch 149: val_accuracy improved from 0.96209 to 0.96259, saving model to best_model.h5\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1086 - accuracy: 0.9728 - val_loss: 0.1453 - val_accuracy: 0.9626\n","Epoch 150/150\n","144/147 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9751\n","Epoch 150: val_accuracy did not improve from 0.96259\n","147/147 [==============================] - 2s 11ms/step - loss: 0.1010 - accuracy: 0.9750 - val_loss: 0.1452 - val_accuracy: 0.9620\n"]}],"source":["history = model.fit(X_train, y_train, validation_split=0.1, epochs=150, callbacks=[checkpoint_callback])"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fcJ_xR8HPeDv","executionInfo":{"status":"ok","timestamp":1686348080706,"user_tz":-60,"elapsed":979,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"}},"outputId":"cc0c9ec6-4ea5-419f-87c2-b84f9c278869"},"outputs":[{"output_type":"stream","name":"stdout","text":["41/41 [==============================] - 0s 7ms/step - loss: 0.1618 - accuracy: 0.9503\n","41/41 [==============================] - 1s 5ms/step\n"]}],"source":["results = model.evaluate(X_test, y_test)\n","y_pred = model.predict(X_test)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYqzKSuoPaB4","executionInfo":{"status":"ok","timestamp":1686348080707,"user_tz":-60,"elapsed":14,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"}},"outputId":"bcba85c4-69a3-4a10-f530-e04b9221486a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[17 17 17 ... 13 13 13]\n"," [19 19 19 ...  3  3  3]\n"," [ 8  8  8 ...  8  8  8]\n"," ...\n"," [ 6  6  6 ...  6  6  6]\n"," [25 25 25 ... 25 25 25]\n"," [ 3  3  3 ...  3  3  3]]\n","Predictions    : [20 20 20  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n","  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n","  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  6  8  8  8  8  8  8  8  8\n","  8  8  8  8  8  2  8  8  2  8  8  8  8  2  8  8  2  8  8  8  8  8  8  2\n","  6  2  2  2  6  6  6  2  2  2  2  6  2  2  8  8  8  8  8  8  2  8  8  2\n","  8  8  8  8  6  8  2  2  2  2  6  2  2  2  2  2  2  2  2  2  2  2  2  2\n","  2  2  2  2  2  2]\n","Appeared times : [101  79  69  68  68  68  66  67  68  67  65  63  64  66  68  66  67  65\n","  64  66  66  65  68  67  68  67  66  68  67  68  67  67  65  64  67  66\n","  64  64  63  64  65  64  64  63  62  63  64  63  63  63  63  63  63  63\n","  63  63  63  63  61  60  61  61  61  59  60  61  61  61  60  61  61  61\n","  61  61  61  61  61  60  60  60  61  61  60  60  60  60  60  60  60  60\n","  60  60  60  60  60  59  59  59  59  59  59  59  59  59  59  59  60  59\n","  59  59  60  60  60  60  60  60  59  60  60  59  60  60  60  60  59  60\n","  62  62  62  60  59  59  59  61  59  61  60  61  61  60  60  59  61  60\n","  60  59  59  61  60  60]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-f892e764b43a>:3: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n","  major = np.squeeze(mode(predictions)[0])\n","<ipython-input-11-f892e764b43a>:4: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n","  count = np.squeeze(mode(predictions)[1])\n"]}],"source":["predictions = np.argmax(y_pred, axis=2)\n","print(predictions)\n","major = np.squeeze(mode(predictions)[0])\n","count = np.squeeze(mode(predictions)[1])\n","print(f\"Predictions    : {major}\")\n","print(f\"Appeared times : {count}\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCQzMuSLPaB5","executionInfo":{"status":"ok","timestamp":1686348080707,"user_tz":-60,"elapsed":12,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"}},"outputId":"9584fa89-f65a-4ab8-d649-1d95f4dc5b43"},"outputs":[{"output_type":"stream","name":"stdout","text":["['u', 'u', 'u', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'g', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'c', 'i', 'i', 'c', 'i', 'i', 'i', 'i', 'c', 'i', 'i', 'c', 'i', 'i', 'i', 'i', 'i', 'i', 'c', 'g', 'c', 'c', 'c', 'g', 'g', 'g', 'c', 'c', 'c', 'c', 'g', 'c', 'c', 'i', 'i', 'i', 'i', 'i', 'i', 'c', 'i', 'i', 'c', 'i', 'i', 'i', 'i', 'g', 'i', 'c', 'c', 'c', 'c', 'g', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c']\n"]}],"source":["def get_key(dictionary, val):\n","    for key, value in dictionary.items():\n","        if val == value:\n","            return key\n","\n","predictions_list = []\n","for word in major:\n","    predictions_list.append(get_key(label_dict, word))\n","print(predictions_list)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":867},"id":"_hQ4rHCgPaB5","executionInfo":{"status":"ok","timestamp":1686348081495,"user_tz":-60,"elapsed":797,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"}},"outputId":"b46fb113-0f04-4088-9a7e-9b03883ffc16"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x1000 with 2 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Accuracy: 95.03%\n"]}],"source":["cm = confusion_matrix(y_test.argmax(axis=2).flatten(), y_pred.argmax(axis=2).flatten())\n","plt.subplots(figsize=(10, 10))\n","sns.heatmap(cm, cmap='Blues', xticklabels=label_dict.keys(), yticklabels=label_dict.keys())\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n","print(f\"Accuracy: {accuracy*100:.2f}%\")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"or48xr4NPaB6","executionInfo":{"status":"ok","timestamp":1686348081883,"user_tz":-60,"elapsed":416,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"}},"outputId":"0f676e39-3741-4141-a5fb-38c1ab1f7637"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      0.96      0.98      8400\n","           1       1.00      0.98      0.99      6900\n","           2       0.95      0.89      0.92      9300\n","           3       0.91      0.89      0.90      7200\n","           4       0.83      0.98      0.90      6900\n","           5       0.97      1.00      0.99      6750\n","           6       1.00      1.00      1.00      8850\n","           7       1.00      1.00      1.00      6600\n","           8       0.94      0.96      0.95      9150\n","           9       0.95      0.93      0.94      7800\n","          10       0.96      0.99      0.97      7800\n","          11       0.95      1.00      0.97      6750\n","          12       1.00      0.99      0.99      8100\n","          13       0.99      0.93      0.96      7500\n","          14       0.93      0.96      0.94      7500\n","          15       1.00      1.00      1.00      5850\n","          16       1.00      1.00      1.00      7500\n","          17       0.85      0.86      0.85      7950\n","          18       0.91      0.82      0.86      7650\n","          19       0.93      0.91      0.92      7350\n","          20       0.86      0.91      0.88      8100\n","          21       0.87      0.83      0.85      6600\n","          22       1.00      1.00      1.00      6450\n","          23       0.97      0.99      0.98      7200\n","          24       0.99      0.99      0.99      7950\n","          25       0.98      0.97      0.97      6900\n","\n","    accuracy                           0.95    195000\n","   macro avg       0.95      0.95      0.95    195000\n","weighted avg       0.95      0.95      0.95    195000\n","\n"]}],"source":["print(classification_report(y_test.argmax(axis=2).flatten(), y_pred.argmax(axis=2).flatten()))"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"YRqJ4zYmQtQY","executionInfo":{"status":"ok","timestamp":1686348351296,"user_tz":-60,"elapsed":652,"user":{"displayName":"Fathi Abdelmalek","userId":"11850075352265060286"}}},"outputs":[],"source":["# model.save('/content/drive/MyDrive/University/MemoireAbdelmalek/models/characters.h5')\n","model.save('characters.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MB4ou0MLPkjf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a3dc203-1319-402c-a8db-fd51e59b696f"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"]}],"source":["run_model = tf.function(lambda x: model(x))\n","BATCH_SIZE = 1\n","INPUT_SIZE = 150\n","STEPS = 11\n","concrete_func = run_model.get_concrete_function(\n","    tf.TensorSpec([BATCH_SIZE, INPUT_SIZE, STEPS], model.inputs[0].dtype))\n","\n","MODEL_DIR = \"./\"\n","model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n","\n","converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","converter._experimental_lower_tensor_list_ops = False\n","converter.allow_custom_ops = False\n","tflite_model = converter.convert()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6M-vv6HaPkNv"},"outputs":[],"source":["# with open('/content/drive/MyDrive/University/MemoireAbdelmalek/models/characters.tflite', 'wb') as f:\n","#     f.write(tflite_model)\n","with open('characters.tflite', 'wb') as f:\n","    f.write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nRSnRvx7SP18"},"outputs":[],"source":["!xxd -i characters.tflite > characters.cc\n","# !xxd -i characters.tflite drive/MyDrive/University/MemoireAbdelmalek/models/characters.cc\n","!ls drive/MyDrive/University/MemoireAbdelmalek/models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IlxLCiPam_H9"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"}},"nbformat":4,"nbformat_minor":0}